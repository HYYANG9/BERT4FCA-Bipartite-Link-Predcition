{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the maximum of length of sequences\n",
    "max_len = 10 * 2 + 3\n",
    "# the number of tokens (objects or attributes)\n",
    "max_vocab = 166\n",
    "# the maximum number of masked tokens\n",
    "max_pred = 4\n",
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 32\n",
    "# dimension of embedding\n",
    "d_model = 224  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 7\n",
    "# number of encoders\n",
    "n_layers = 7\n",
    "# the number of input setences\n",
    "n_segs = 2\n",
    "\n",
    "p_dropout = .1\n",
    "\n",
    "#80% the chosen token is replaced by [mask], 10% is replaced by a random token, 10% do nothing\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in word2idx\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    # The size of pad_mask is [batch, seq_len, seq_len]\n",
    "    # The resulting tensor has True where padding tokens are located and False elsewhere.\n",
    "    \n",
    "    # print(f'the shape of pad_mask is {pad_mask.shape}')\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        word_enc = self.word_emb(x)\n",
    "        \n",
    "        '''\n",
    "        maybe positional embedding can be deleted\n",
    "        '''\n",
    "        # # positional embedding\n",
    "        # pos = torch.arange(x.shape[1], dtype=torch.long, device=device) # .long: round down\n",
    "        # pos = pos.unsqueeze(0).expand_as(x) # the shape is [1, seq_len]\n",
    "        # pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# pre-LN is easier to train than post-LN, but if fullly training, post_LN have better result than pre-LN. \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
    "\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The full connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embeddings()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "        \n",
    "        # next sentence prediction. output is 0 or 1.\n",
    "        self.next_cls = nn.Linear(d_model, 2)\n",
    "        self.gelu = gelu\n",
    "        \n",
    "        # Sharing weight between some fully connect layer, this will make training easier.\n",
    "        shared_weight = self.pooler.fc.weight\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.fc.weight = shared_weight\n",
    "\n",
    "        shared_weight = self.embedding.word_emb.weight\n",
    "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
    "        self.word_classifier.weight = shared_weight\n",
    "\n",
    "    def forward(self, tokens, segments, masked_pos):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "\n",
    "        # NSP Task\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        logits_cls = self.next_cls(hidden_pool)\n",
    "\n",
    "        # Masked Language Model Task\n",
    "        '''\n",
    "        extracting representations of masked positions, \n",
    "        passing them through a fully connected layer, \n",
    "        applying the GELU activation function, \n",
    "        and making predictions using the word classifier\n",
    "        '''\n",
    "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
    "\n",
    "        # h_masked: [batch, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
    "        h_masked = self.gelu(self.fc(h_masked))\n",
    "        logits_lm = self.word_classifier(h_masked)\n",
    "        # logits_lm: [batch, max_pred, max_vocab]\n",
    "        # logits_cls: [batch, 2]\n",
    "\n",
    "        return logits_cls, logits_lm, hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9177c2-dcb1-4500-bf67-e0f7ee0d286a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14012f5d-fc34-42ba-a496-1bd61287973d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "def process_concepts_from_file(filename) :\n",
    "    extents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) >= 2:\n",
    "                extent = parts[1].strip()\n",
    "                extents.append(extent)\n",
    "\n",
    "    object_list = list(set(\" \".join(extents).split()))\n",
    "    sorted_object_list = sorted(map(int, object_list))\n",
    "\n",
    "    print(len(object_list))\n",
    "    # Create the object2idx dictionary\n",
    "    object2idx = {str(obj): idx + 1  for idx, obj in enumerate(sorted_object_list)}\n",
    "    sorted_object_list = list(map(str, sorted_object_list ))\n",
    "\n",
    "    special_tokens = {'[PAD]': max_vocab - 4, '[CLS]': max_vocab - 3, '[SEP]': max_vocab - 2, '[MASK]': max_vocab - 1}\n",
    "\n",
    "    object2idx.update(special_tokens)\n",
    "    # print(len(object2idx))\n",
    "\n",
    "    idx2object = {idx: object for object, idx in object2idx.items()}\n",
    "    vocab_size = len(object2idx)\n",
    "    # assert len(object2idx) == len(idx2object)\n",
    "\n",
    "    extent_token_list = []\n",
    "    for extent in extents:\n",
    "        extent_token_list.append([\n",
    "            object2idx[s] for s in extent.split()\n",
    "        ])\n",
    "        \n",
    "    return extent_token_list, object2idx, idx2object\n",
    "\n",
    "extent_token_list, object2idx, idx2object = process_concepts_from_file('paper-keywords-before2015_concepts.txt')\n",
    "extent_token_list_new, object2idx2, idx2object2 = process_concepts_from_file('paper-keywords-all_concepts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5d1fe4-fffa-4d8f-838b-4ea8e3686507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for extent in extent_token_list :\n",
    "    maxlen = max(len(extent), maxlen)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f03f47b-2af5-4b2f-90fa-eb9939803580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# padding the token lists to have the same length.\n",
    "def padding(ids, n_pads, pad_symb=0):\n",
    "    return ids.extend([pad_symb for _ in range(n_pads)])\n",
    "\n",
    "def masking_procedure(cand_pos, input_ids, masked_symb='[MASK]'):\n",
    "    masked_pos = []\n",
    "    masked_tokens = []\n",
    "    for pos in cand_pos:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random.random() < p_mask:\n",
    "            input_ids[pos] = masked_symb\n",
    "        elif random.random() > (p_mask + p_replace):\n",
    "            rand_word_idx = random.randint(0, max_vocab - 4)\n",
    "            input_ids[pos] = rand_word_idx\n",
    "\n",
    "    return masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a28666-9619-4be9-b41d-287b585b9d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8153\n"
     ]
    }
   ],
   "source": [
    "def get_neighbor_samples(extents) :\n",
    "    n = len(extents)\n",
    "    samples = []\n",
    "\n",
    "    dep = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "    neighbor = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "\n",
    "    for i in range(n) :\n",
    "        for j in range(i + 1, n) :\n",
    "            if set(extents[i]).issubset(set(extents[j])) :\n",
    "                dep[i][j] = 1\n",
    "            if set(extents[j]).issubset(set(extents[i])) :\n",
    "                dep[j][i] = 1\n",
    "\n",
    "    for i in range(n) :\n",
    "        se = set([])\n",
    "        for j in range(n) :\n",
    "            if j != i :\n",
    "                if dep[j][i] == 1 :\n",
    "                    rep = False\n",
    "                    lst = list(se)\n",
    "                    for idk, k in enumerate(lst) :\n",
    "                        if dep[k][j] :\n",
    "                            se.remove(k)\n",
    "                            se.add(j)\n",
    "                            rep = True\n",
    "                        if dep[j][k] :\n",
    "                            rep = True\n",
    "                    if not rep :\n",
    "                        se.add(j)\n",
    "\n",
    "        for j in range(n) :\n",
    "            if j in se :\n",
    "                samples.append([i, j, True])\n",
    "            elif random.random() < 0.0018 :\n",
    "                samples.append([i, j, False])\n",
    "        \n",
    "    return samples\n",
    "\n",
    "all_samples = get_neighbor_samples(extent_token_list)\n",
    "print(len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5afe0534-d54a-4aa2-9aaa-384813468cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data(extents, all_samples, word2idx, n_data, num_per_sample = 120):\n",
    "    batch_data = []\n",
    "    positive = negative = 0\n",
    "    max_len = 0\n",
    "    len_sentences = len(extents)\n",
    "    for extent in extents :\n",
    "        max_len = max(max_len, len(extent))\n",
    "    max_len = max_len * 2 + 3\n",
    "        \n",
    "    for sample in all_samples :\n",
    "        \n",
    "        tokens_a_idx = sample[0]\n",
    "        tokens_b_idx = sample[1]\n",
    "        tokens_a = extent_token_list[tokens_a_idx]\n",
    "        tokens_b = extent_token_list[tokens_b_idx]\n",
    "             \n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0 for i in range(\n",
    "            1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))]\n",
    "\n",
    "        # Determines the number of positions to mask (n_pred) based on the input sequence length.\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * .15)))\n",
    "        cand_pos = [i for i, token in enumerate(input_ids)\n",
    "                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] #exclude special tokens.\n",
    "\n",
    "        # shuffle all candidate position index, to sampling maksed position from first n_pred\n",
    "        masked_pos, masked_tokens = masking_procedure(\n",
    "            cand_pos[:n_pred], input_ids, word2idx['[MASK]'])\n",
    "\n",
    "        # zero padding for tokens to ensure that the input sequences and segment IDs have the maximum sequence length\n",
    "        padding(input_ids, max_len - len(input_ids))\n",
    "        # print(\"the size of input_ids is \" ,len(input_ids))\n",
    "        padding(segment_ids, max_len - len(segment_ids))\n",
    "        # print(\"the size of segment_ids is \" ,len(segment_ids))\n",
    "\n",
    "        # zero padding for mask\n",
    "        if max_pred > n_pred:\n",
    "            n_pads = max_pred - n_pred\n",
    "            padding(masked_pos, n_pads)\n",
    "            padding(masked_tokens, n_pads)\n",
    "\n",
    "        # Creating Batch Data:\n",
    "        batch_data.append(\n",
    "            [input_ids, segment_ids, masked_tokens, masked_pos, sample[2]])\n",
    "\n",
    "    random.shuffle(batch_data)\n",
    "    print(len(batch_data))\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.is_next = is_next\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.is_next[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b5fd-3776-4641-baa3-d82a8c496007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-Train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7abc56-ac8f-449d-9709-e60d720dace3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DO_NSP_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3a030f9-caff-4a5e-8763-9a061fbabe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # 必须为偶数\n",
    "lr = 1.9e-5\n",
    "epochs = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40521dbb-258b-4a9e-a94f-859e4a6836a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7165\n",
      "Entering training process...\n",
      "Epoch:0\t loss: 1.861504\n",
      "Epoch:50\t loss: 0.451407\n",
      "Epoch:100\t loss: 0.153402\n",
      "Epoch:150\t loss: 0.160886\n",
      "Epoch:200\t loss: 0.059680\n",
      "Epoch:250\t loss: 0.099437\n",
      "Epoch:300\t loss: 0.082009\n",
      "Epoch:350\t loss: 0.072126\n",
      "Epoch:400\t loss: 0.141783\n",
      "Epoch:450\t loss: 0.071086\n",
      "Epoch:500\t loss: 0.091308\n",
      "Epoch:550\t loss: 0.045794\n",
      "Epoch:600\t loss: 0.111170\n",
      "Epoch:650\t loss: 0.091328\n",
      "Epoch:700\t loss: 0.117798\n",
      "Epoch:750\t loss: 0.044200\n",
      "Epoch:800\t loss: 0.112777\n",
      "Epoch:850\t loss: 0.219594\n",
      "Epoch:900\t loss: 0.071489\n",
      "Epoch:950\t loss: 0.064486\n",
      "Epoch:1000\t loss: 0.069704\n",
      "Epoch:1050\t loss: 0.067912\n",
      "Epoch:1100\t loss: 0.036912\n",
      "Epoch:1150\t loss: 0.121193\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "train_samples, test_samples = [], []\n",
    "\n",
    "if DO_NSP_TEST :\n",
    "    train_samples, test_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "else :\n",
    "    train_samples = all_samples\n",
    "\n",
    "batch_data = make_data(extent_token_list, train_samples, object2idx, n_data=len(all_samples))\n",
    "\n",
    "batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)]\n",
    "dataset = BERTDataset(*batch_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "model = BERT(n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "print('Entering training process...')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bat = 0\n",
    "    for one_batch in dataloader:\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch]\n",
    "\n",
    "        logits_cls, logits_lm, _ = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        loss_cls = criterion(logits_cls, is_next)\n",
    "        loss_lm = criterion(logits_lm.view(-1, max_vocab), masked_tokens.view(-1))\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n",
    "    \n",
    "    # 每30个epoch保存一次模型\n",
    "    if epoch % 50 == 0 :\n",
    "        torch.save(model.state_dict(), 'oo_no_pos_pretrained.dat')\n",
    "        print(f'Epoch:{epoch}\\t loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d56dd2-3618-450c-9225-04674cfc2eb7",
   "metadata": {},
   "source": [
    "# Neighboring Concept Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18dc99ff-c7e1-456e-a3f9-922e86485bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True: 0\n",
      "Number of False: 0\n"
     ]
    }
   ],
   "source": [
    "labels = [sample[2] for sample in test_samples]\n",
    "# print(labels)\n",
    "\n",
    "num_true = labels.count(True)\n",
    "num_false = labels.count(False)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of True:\", num_true)\n",
    "print(\"Number of False:\", num_false)\n",
    "\n",
    "labels_mapping = {\"True\": 1, \"False\": 0}\n",
    "labels_01 = [labels_mapping[str(sample[2])] for sample in test_samples]\n",
    "# print(labels_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "28abb838-fa9d-4e0f-915a-2c0db846b2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.861244019138756\n",
      "Precision: 0.7713004484304933\n",
      "Recall: 0.9608938547486033\n",
      "F1 Score: 0.8557213930348258\n",
      "AUC Score: 0.8737523667048457\n"
     ]
    }
   ],
   "source": [
    "if DO_NSP_TEST :\n",
    "    pretrained_model = BERT(n_layers)\n",
    "    pretrained_model.eval()\n",
    "    pretrained_model.load_state_dict(torch.load('oo_no_pos_pretrained.dat'))\n",
    "    pretrained_model.to(device)\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
    "    #     input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    #     segment_ids = torch.LongTensor(segment_ids).unsqueeze(0).to(device)\n",
    "    #     masked_pos = torch.LongTensor(masked_pos).unsqueeze(0).to(device)\n",
    "    #     masked_tokens = torch.LongTensor(masked_tokens).unsqueeze(0).to(device)\n",
    "    #     logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
    "    #     input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
    "\n",
    "    for sample in test_samples:\n",
    "        index_a = sample[0]\n",
    "        index_b = sample[1]\n",
    "        tokens_a = extent_token_list[index_a]\n",
    "        tokens_b = extent_token_list[index_b]\n",
    "\n",
    "        input_ids = torch.tensor([object2idx['[CLS]']] + tokens_a + [object2idx['[SEP]']] + tokens_b + [object2idx['[SEP]']])\n",
    "        segment_ids = torch.tensor([0 for i in range(\n",
    "                        1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))])\n",
    "        masked_pos = torch.tensor([0 for i in range(\n",
    "                        1 + len(tokens_a) + 1)] + [0 for i in range(1 + len(tokens_b))])\n",
    "        input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "        segment_ids = torch.LongTensor(segment_ids).unsqueeze(0).to(device)\n",
    "        masked_pos = torch.LongTensor(masked_pos).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "        logits_cls, _, _ = pretrained_model(input_ids, segment_ids, masked_pos)\n",
    "        cpu = torch.device('cpu')\n",
    "        pred_next = logits_cls.data.max(1)[1].data.to(cpu).numpy()[0]\n",
    "        predictions.append(pred_next) \n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels_01, predictions)\n",
    "    precision = precision_score(labels_01, predictions)\n",
    "    recall = recall_score(labels_01, predictions)\n",
    "    f1 = f1_score(labels_01, predictions)\n",
    "    roc_auc = roc_auc_score(labels_01, predictions)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"AUC Score:\", roc_auc)\n",
    "else :\n",
    "    print('NSP TEST is disabled since DO_NSP_TEST is set to False.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256c64-5c39-4e0d-8c3f-7e9e294f2bcc",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7ec4a-ee80-4db1-8e40-e57e1e780117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe06b24e-1cd3-4a63-879c-eec2a1fc44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[   0.    0. 4004. 2625. 1654.  905.]\n",
      "[   0.    0. 5568. 3793. 2338. 1238.]\n",
      "[0 1 2 3 4 5]\n",
      "[0.0, 0.0, 0.4171779141104294, 0.31154974659909307, 0.18244865297412644, 0.08882368631635103]\n",
      "1.0\n",
      "[0.         0.         0.41717791 0.31154975 0.18244865 0.08882369]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def get_true_permutes(extent_token_list, tup_len = 3) :\n",
    "    true_permutes = []\n",
    "    dist = [0 for i in range(tup_len + 1)]\n",
    "    \n",
    "    for extent in extent_token_list :\n",
    "        extent_len = len(extent)\n",
    "        \n",
    "        for now_len in range(2, tup_len + 1) :\n",
    "            if extent_len >= now_len :\n",
    "                now_pmt = [' '.join([str(ele) for ele in list(p)] + ['0' for _ in range(tup_len - now_len)]) for p in itertools.combinations(extent, now_len)]\n",
    " #               now_pmt = [' '.join([str(ele) for ele in list(p)] + ['0' for _ in range(tup_len - now_len)]) for p in itertools.permutations(extent, now_len)]\n",
    "            else :\n",
    "                now_pmt = []\n",
    "                \n",
    "            true_permutes.extend(now_pmt)\n",
    "            dist[now_len] += len(now_pmt)\n",
    "\n",
    "    true_permutes = set(true_permutes)\n",
    "    \n",
    "    return true_permutes, np.array(dist, dtype = np.float64)\n",
    "\n",
    "def pad_negative_samples(object2idx, true_permutes, length_distribution, number) :\n",
    "    lengths = np.arange(0, len(length_distribution))\n",
    "    tup_len = len(length_distribution) - 1 \n",
    "\n",
    "    print(lengths)\n",
    "    print(length_distribution)\n",
    "    print(np.sum(length_distribution))\n",
    "    \n",
    "    length_distribution[-1] = 1.0 - np.sum(length_distribution[0:-1])\n",
    "    length_distribution /= np.sum(length_distribution)\n",
    "    \n",
    "    print(length_distribution)\n",
    "    print(np.sum(length_distribution))\n",
    "    \n",
    "    object_list = []\n",
    "    for obj in object2idx :\n",
    "        if not '[' in obj :\n",
    "            object_list.append(object2idx[obj])\n",
    "    \n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < number :\n",
    "        length = np.random.choice(lengths, p=length_distribution)\n",
    "\n",
    "        tmp_list = random.sample(object_list, length)\n",
    "        if length < tup_len :\n",
    "            tmp_list.extend([0 for _ in range(tup_len - length)])\n",
    "        \n",
    "        tmp_str = ' '.join([str(x) for x in tmp_list])\n",
    "        if tmp_str in true_permutes :\n",
    "            continue\n",
    "\n",
    "        negative_samples.append((tmp_list, False))\n",
    "    return negative_samples\n",
    "\n",
    "def prepare_object_list_data(object2idx, extent_token_list, extent_token_list_new, tup_len = 3) :\n",
    "    old_true_permutes, old_distribution = get_true_permutes(extent_token_list, tup_len)\n",
    "    new_true_permutes, new_distribution = get_true_permutes(extent_token_list_new, tup_len)\n",
    "    added_true_permutes = new_true_permutes - old_true_permutes\n",
    "    added_distribution = new_distribution - old_distribution\n",
    "    added_distribution /= np.sum(added_distribution)\n",
    "    \n",
    "    print(old_distribution)\n",
    "    print(new_distribution)\n",
    "    \n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "    \n",
    "    for perm_str in old_true_permutes :\n",
    "        lst = [int(x) for x in perm_str.split(' ')]\n",
    "        train_samples.append((lst, True))\n",
    "    for perm_str in added_true_permutes :\n",
    "        lst = [int(x) for x in perm_str.split(' ')]\n",
    "        test_samples.append((lst, True))\n",
    "    \n",
    "    train_len = len(train_samples)\n",
    "    test_len = len(test_samples)\n",
    "    \n",
    "    negative_samples = pad_negative_samples(object2idx, new_true_permutes, list(added_distribution), train_len + test_len)\n",
    "    train_negative_samples, test_negative_samples = train_test_split(negative_samples, test_size=test_len / (train_len + test_len), random_state=42)\n",
    "\n",
    "    train_samples.extend(train_negative_samples)\n",
    "    test_samples.extend(test_negative_samples)\n",
    "    \n",
    "    random.shuffle(train_samples)\n",
    "    random.shuffle(test_samples)\n",
    "    \n",
    "    return train_samples, test_samples\n",
    "\n",
    "max_lenn = 0\n",
    "for extent in extent_token_list :\n",
    "    max_lenn = max(max_lenn, len(extent))\n",
    "print(max_lenn)\n",
    "\n",
    "train_labeled_lists, test_labeled_lists = prepare_object_list_data(object2idx, extent_token_list, extent_token_list_new, tup_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "444363be-08e7-43c5-9c40-b38c8e7a39b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in test set ratio of 0s: 0.500\n",
      "in test set ratio of 1s: 0.500\n",
      "in train set ratio of 0s: 0.500\n",
      "in train set ratio of 1s: 0.500\n",
      "train set size 13360\n",
      "test set size 3504\n"
     ]
    }
   ],
   "source": [
    "# check the ratio of 1 and 0\n",
    "df = pd.DataFrame(test_labeled_lists, columns=['Pair', 'Label'])\n",
    "\n",
    "# Calculate the ratios\n",
    "ratio_zeros = (df['Label'] == 0).mean()\n",
    "ratio_ones = (df['Label'] == 1).mean()\n",
    "\n",
    "print(f\"in test set ratio of 0s: {ratio_zeros:.3f}\")\n",
    "print(f\"in test set ratio of 1s: {ratio_ones:.3f}\")\n",
    "\n",
    "df2 = pd.DataFrame(train_labeled_lists, columns=['Pair', 'Label'])\n",
    "\n",
    "# Calculate the ratios\n",
    "ratio_zero = (df2['Label'] == 0).mean()\n",
    "ratio_one = (df2['Label'] == 1).mean()\n",
    "\n",
    "print(f\"in train set ratio of 0s: {ratio_zero:.3f}\")\n",
    "print(f\"in train set ratio of 1s: {ratio_one:.3f}\")\n",
    "\n",
    "print('train set size ' + str(len(train_labeled_lists)))\n",
    "print('test set size ' + str(len(test_labeled_lists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44006f7b-b58f-43b5-ba7b-32dc2ce5e780",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef9943-2c2e-4e63-9d0e-4d98973caddb",
   "metadata": {},
   "source": [
    "##  MLP for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f9da990-44e9-426a-aa85-dd26c9c29aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design a MLP for classification task\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, bert_model, embedding_size, hidden_size, output_size, dropout_rate = .1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.bert = bert_model\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, segments, masked_poses):\n",
    "        _, __, x = self.bert(inputs, segments, masked_poses)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "def prepare_data(pair_set):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for lst, label in pair_set:\n",
    "        inputs.append(lst)\n",
    "        labels.append(label)\n",
    "    return torch.tensor(inputs), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d50d109-a6e5-41c6-a451-43391b326a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# input_size = 2 * d_model\n",
    "hidden_size = 324\n",
    "output_size = 1\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "pretrained_model = BERT(n_layers)\n",
    "#pretrained_model.load_state_dict(torch.load('oo_no_pos_pretrained.dat'))\n",
    "pretrained_model.train()\n",
    "# pretrained_model.eval()\n",
    "pretrained_model.to(device)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MLP_model = MLP(pretrained_model, d_model, hidden_size, output_size, dropout_rate=0.1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to device\n",
    "MLP_model = MLP_model.to(device)\n",
    "MLP_model.train()\n",
    "\n",
    "# Prepare the data\n",
    "train_inputs, train_labels = prepare_data(train_labeled_lists)\n",
    "test_inputs, test_labels = prepare_data(test_labeled_lists)\n",
    "\n",
    "train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a76fa74-69ff-4214-b7da-94c646012c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.51it/s, loss=0.524]\n",
      "Epoch 2/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.94it/s, loss=0.195]\n",
      "Epoch 3/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.68it/s, loss=0.153]\n",
      "Epoch 4/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.03it/s, loss=0.326]\n",
      "Epoch 5/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.48it/s, loss=0.103]\n",
      "Epoch 6/300: 100%|████████████████████████████████████████████████████████| 418/418 [00:05<00:00, 69.97it/s, loss=0.39]\n",
      "Epoch 7/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.59it/s, loss=0.213]\n",
      "Epoch 8/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.68it/s, loss=0.113]\n",
      "Epoch 9/300: 100%|████████████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.63it/s, loss=0.37]\n",
      "Epoch 10/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.83it/s, loss=0.356]\n",
      "Epoch 11/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.92it/s, loss=0.0203]\n",
      "Epoch 12/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.50it/s, loss=0.0533]\n",
      "Epoch 13/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:05<00:00, 70.01it/s, loss=0.0704]\n",
      "Epoch 14/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.27it/s, loss=0.00697]\n",
      "Epoch 15/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.81it/s, loss=0.237]\n",
      "Epoch 16/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.93it/s, loss=0.136]\n",
      "Epoch 17/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.63it/s, loss=0.0145]\n",
      "Epoch 18/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.88it/s, loss=0.428]\n",
      "Epoch 19/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.90it/s, loss=0.0776]\n",
      "Epoch 20/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 69.16it/s, loss=0.00821]\n",
      "Epoch 21/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.93it/s, loss=0.0706]\n",
      "Epoch 22/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.51it/s, loss=0.00959]\n",
      "Epoch 23/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.13it/s, loss=0.00124]\n",
      "Epoch 24/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.71it/s, loss=0.00777]\n",
      "Epoch 25/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:05<00:00, 71.47it/s, loss=0.0247]\n",
      "Epoch 26/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.65it/s, loss=0.0476]\n",
      "Epoch 27/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:07<00:00, 56.96it/s, loss=0.0315]\n",
      "Epoch 28/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.88it/s, loss=0.00482]\n",
      "Epoch 29/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.48it/s, loss=0.00146]\n",
      "Epoch 30/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.44it/s, loss=0.228]\n",
      "Epoch 31/300: 100%|███████████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.45it/s, loss=0.04]\n",
      "Epoch 32/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.84it/s, loss=0.00316]\n",
      "Epoch 33/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.44it/s, loss=0.000697]\n",
      "Epoch 34/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.14it/s, loss=0.0515]\n",
      "Epoch 35/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.90it/s, loss=0.0665]\n",
      "Epoch 36/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.06it/s, loss=0.00492]\n",
      "Epoch 37/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.06it/s, loss=0.00529]\n",
      "Epoch 38/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.64it/s, loss=0.000216]\n",
      "Epoch 39/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.40it/s, loss=0.00628]\n",
      "Epoch 40/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.56it/s, loss=0.00509]\n",
      "Epoch 41/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.25it/s, loss=0.000674]\n",
      "Epoch 42/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.07it/s, loss=0.113]\n",
      "Epoch 43/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.53it/s, loss=0.246]\n",
      "Epoch 44/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.65it/s, loss=0.00814]\n",
      "Epoch 45/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.54it/s, loss=0.000111]\n",
      "Epoch 46/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.50it/s, loss=0.0026]\n",
      "Epoch 47/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.83it/s, loss=0.061]\n",
      "Epoch 48/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.38it/s, loss=0.127]\n",
      "Epoch 49/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.67it/s, loss=0.019]\n",
      "Epoch 50/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.19it/s, loss=0.000177]\n",
      "Epoch 51/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.65it/s, loss=0.00244]\n",
      "Epoch 52/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.06it/s, loss=3.26e-5]\n",
      "Epoch 53/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.38it/s, loss=0.000306]\n",
      "Epoch 54/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.27it/s, loss=0.000533]\n",
      "Epoch 55/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.64it/s, loss=9.57e-5]\n",
      "Epoch 56/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.17it/s, loss=7.45e-5]\n",
      "Epoch 57/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.77it/s, loss=0.00109]\n",
      "Epoch 58/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.35it/s, loss=0.000151]\n",
      "Epoch 59/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.23it/s, loss=0.00476]\n",
      "Epoch 60/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.73it/s, loss=0.000317]\n",
      "Epoch 61/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.37it/s, loss=0.000553]\n",
      "Epoch 62/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.26it/s, loss=9.14e-5]\n",
      "Epoch 63/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.90it/s, loss=0.000555]\n",
      "Epoch 64/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.39it/s, loss=0.000304]\n",
      "Epoch 65/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.22it/s, loss=6.46e-5]\n",
      "Epoch 66/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.35it/s, loss=0.000833]\n",
      "Epoch 67/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.10it/s, loss=0.00494]\n",
      "Epoch 68/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.37it/s, loss=0.000265]\n",
      "Epoch 69/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.61it/s, loss=9.66e-5]\n",
      "Epoch 70/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.24it/s, loss=0.000567]\n",
      "Epoch 71/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.35it/s, loss=0.000106]\n",
      "Epoch 72/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.67it/s, loss=9.03e-5]\n",
      "Epoch 73/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.50it/s, loss=6.57e-5]\n",
      "Epoch 74/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.11it/s, loss=0.00703]\n",
      "Epoch 75/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.40it/s, loss=0.000267]\n",
      "Epoch 76/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.47it/s, loss=0.000123]\n",
      "Epoch 77/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.33it/s, loss=7.23e-5]\n",
      "Epoch 78/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.12it/s, loss=0.000114]\n",
      "Epoch 79/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.82it/s, loss=0.000324]\n",
      "Epoch 80/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.38it/s, loss=0.000147]\n",
      "Epoch 81/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 59.83it/s, loss=0.012]\n",
      "Epoch 82/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.83it/s, loss=4.16e-5]\n",
      "Epoch 83/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.04it/s, loss=0.000314]\n",
      "Epoch 84/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.51it/s, loss=0.0944]\n",
      "Epoch 85/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.28it/s, loss=0.000525]\n",
      "Epoch 86/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.06it/s, loss=0.000159]\n",
      "Epoch 87/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.83it/s, loss=0.00187]\n",
      "Epoch 88/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.78it/s, loss=0.0017]\n",
      "Epoch 89/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.21it/s, loss=3.03e-5]\n",
      "Epoch 90/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 68.70it/s, loss=8.61e-5]\n",
      "Epoch 91/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.79it/s, loss=0.000199]\n",
      "Epoch 92/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.37it/s, loss=0.000319]\n",
      "Epoch 93/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.94it/s, loss=0.000492]\n",
      "Epoch 94/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 59.96it/s, loss=0.000209]\n",
      "Epoch 95/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 56.67it/s, loss=5.43e-5]\n",
      "Epoch 96/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.51it/s, loss=0.000104]\n",
      "Epoch 97/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 55.24it/s, loss=0.000288]\n",
      "Epoch 98/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.74it/s, loss=0.000166]\n",
      "Epoch 99/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.86it/s, loss=3.08e-5]\n",
      "Epoch 100/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.21it/s, loss=1.29e-5]\n",
      "Epoch 101/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.32it/s, loss=0.000477]\n",
      "Epoch 102/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.62it/s, loss=0.0157]\n",
      "Epoch 103/300: 100%|██████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.52it/s, loss=0.000112]\n",
      "Epoch 104/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.82it/s, loss=9.74e-5]\n",
      "Epoch 105/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.72it/s, loss=0.000232]\n",
      "Epoch 106/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.02it/s, loss=0.00107]\n",
      "Epoch 107/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 59.76it/s, loss=0.00107]\n",
      "Epoch 108/300: 100%|██████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.21it/s, loss=0.000411]\n",
      "Epoch 109/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.88it/s, loss=0.000183]\n",
      "Epoch 110/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.53it/s, loss=0.0245]\n",
      "Epoch 111/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.61it/s, loss=4.54e-5]\n",
      "Epoch 112/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.28it/s, loss=7.82e-5]\n",
      "Epoch 113/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.23it/s, loss=0.000121]\n",
      "Epoch 114/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.73it/s, loss=3.56e-5]\n",
      "Epoch 115/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.36it/s, loss=0.000113]\n",
      "Epoch 116/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.28it/s, loss=8.48e-5]\n",
      "Epoch 117/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.54it/s, loss=0.000531]\n",
      "Epoch 118/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.76it/s, loss=0.000698]\n",
      "Epoch 119/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.06it/s, loss=0.000394]\n",
      "Epoch 120/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.68it/s, loss=4.65e-5]\n",
      "Epoch 121/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.32it/s, loss=2.88e-5]\n",
      "Epoch 122/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.82it/s, loss=0.000192]\n",
      "Epoch 123/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.49it/s, loss=5.49e-5]\n",
      "Epoch 124/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 68.40it/s, loss=0.000142]\n",
      "Epoch 125/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.98it/s, loss=7.2e-5]\n",
      "Epoch 126/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.14it/s, loss=4.74e-5]\n",
      "Epoch 127/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.97it/s, loss=0.000174]\n",
      "Epoch 128/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.99it/s, loss=3.72e-5]\n",
      "Epoch 129/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.27it/s, loss=2.31e-5]\n",
      "Epoch 130/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.35it/s, loss=0.000209]\n",
      "Epoch 131/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.93it/s, loss=6.78e-5]\n",
      "Epoch 132/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.24it/s, loss=4.99e-5]\n",
      "Epoch 133/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.58it/s, loss=5.75e-5]\n",
      "Epoch 134/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.42it/s, loss=0.000119]\n",
      "Epoch 135/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.83it/s, loss=0.00059]\n",
      "Epoch 136/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.50it/s, loss=0.000306]\n",
      "Epoch 137/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.79it/s, loss=0.000179]\n",
      "Epoch 138/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.21it/s, loss=4.98e-5]\n",
      "Epoch 139/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.10it/s, loss=2.74e-5]\n",
      "Epoch 140/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.38it/s, loss=1.42e-5]\n",
      "Epoch 141/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.21it/s, loss=0.000505]\n",
      "Epoch 142/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.40it/s, loss=0.000133]\n",
      "Epoch 143/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.68it/s, loss=4.91e-5]\n",
      "Epoch 144/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.63it/s, loss=1.99e-5]\n",
      "Epoch 145/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.10it/s, loss=0.0101]\n",
      "Epoch 146/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.62it/s, loss=5.31e-5]\n",
      "Epoch 147/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.19it/s, loss=1.93e-5]\n",
      "Epoch 148/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 59.83it/s, loss=2.17e-5]\n",
      "Epoch 149/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.52it/s, loss=0.000138]\n",
      "Epoch 150/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.22it/s, loss=0.00449]\n",
      "Epoch 151/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.55it/s, loss=3.66e-5]\n",
      "Epoch 152/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.50it/s, loss=0.00011]\n",
      "Epoch 153/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.26it/s, loss=9.62e-5]\n",
      "Epoch 154/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.66it/s, loss=3.18e-5]\n",
      "Epoch 155/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.06it/s, loss=3.6e-5]\n",
      "Epoch 156/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.02it/s, loss=0.0414]\n",
      "Epoch 157/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 56.03it/s, loss=4.47e-5]\n",
      "Epoch 158/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.51it/s, loss=3.85e-5]\n",
      "Epoch 159/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.98it/s, loss=6.29e-5]\n",
      "Epoch 160/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.50it/s, loss=1.52e-5]\n",
      "Epoch 161/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 68.49it/s, loss=4.12e-5]\n",
      "Epoch 162/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 65.34it/s, loss=2.95e-5]\n",
      "Epoch 163/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 69.34it/s, loss=3.64e-5]\n",
      "Epoch 164/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.64it/s, loss=7.39e-5]\n",
      "Epoch 165/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.14it/s, loss=0.107]\n",
      "Epoch 166/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.64it/s, loss=2.42e-5]\n",
      "Epoch 167/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.90it/s, loss=0.000121]\n",
      "Epoch 168/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.23it/s, loss=0.00022]\n",
      "Epoch 169/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.56it/s, loss=0.00818]\n",
      "Epoch 170/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.39it/s, loss=6.96e-5]\n",
      "Epoch 171/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.39it/s, loss=6.14e-5]\n",
      "Epoch 172/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.79it/s, loss=0.000152]\n",
      "Epoch 173/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.27it/s, loss=3.65e-5]\n",
      "Epoch 174/300: 100%|██████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.54it/s, loss=0.000453]\n",
      "Epoch 175/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.29it/s, loss=2.9e-5]\n",
      "Epoch 176/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.45it/s, loss=9.29e-5]\n",
      "Epoch 177/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.71it/s, loss=3.83e-5]\n",
      "Epoch 178/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.43it/s, loss=3.05e-5]\n",
      "Epoch 179/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.94it/s, loss=3.13e-5]\n",
      "Epoch 180/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.21it/s, loss=2.19e-5]\n",
      "Epoch 181/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.94it/s, loss=4.3e-5]\n",
      "Epoch 182/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.73it/s, loss=9.4e-5]\n",
      "Epoch 183/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.02it/s, loss=5.55e-5]\n",
      "Epoch 184/300: 100%|██████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.65it/s, loss=0.000137]\n",
      "Epoch 185/300: 100%|████████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.18it/s, loss=9.8e-5]\n",
      "Epoch 186/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 57.95it/s, loss=2.17e-5]\n",
      "Epoch 187/300: 100%|██████████████████████████████████████████████████| 418/418 [00:05<00:00, 69.99it/s, loss=0.000702]\n",
      "Epoch 188/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.22it/s, loss=3.47e-5]\n",
      "Epoch 189/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.81it/s, loss=0.0393]\n",
      "Epoch 190/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.21it/s, loss=2.73e-5]\n",
      "Epoch 191/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.41it/s, loss=2.52e-5]\n",
      "Epoch 192/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.85it/s, loss=7.03e-5]\n",
      "Epoch 193/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 58.96it/s, loss=5.89e-5]\n",
      "Epoch 194/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.28it/s, loss=3.72e-5]\n",
      "Epoch 195/300: 100%|███████████████████████████████████████████████████| 418/418 [00:07<00:00, 59.35it/s, loss=2.61e-5]\n",
      "Epoch 196/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 59.81it/s, loss=0.00406]\n",
      "Epoch 197/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.70it/s, loss=0.0316]\n",
      "Epoch 198/300: 100%|██████████████████████████████████████████████████| 418/418 [00:07<00:00, 56.74it/s, loss=0.000281]\n",
      "Epoch 199/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 60.16it/s, loss=3.01e-5]\n",
      "Epoch 200/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 66.20it/s, loss=3.06e-5]\n",
      "Epoch 201/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.30it/s, loss=1.73e-5]\n",
      "Epoch 202/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.82it/s, loss=3.7e-5]\n",
      "Epoch 203/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.89it/s, loss=4.1e-5]\n",
      "Epoch 204/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 67.73it/s, loss=5.23e-5]\n",
      "Epoch 205/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.30it/s, loss=0.00061]\n",
      "Epoch 206/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.99it/s, loss=0.000283]\n",
      "Epoch 207/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.25it/s, loss=2.66e-5]\n",
      "Epoch 208/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.38it/s, loss=2.88e-5]\n",
      "Epoch 209/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.79it/s, loss=0.000156]\n",
      "Epoch 210/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.86it/s, loss=2.79e-5]\n",
      "Epoch 211/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.65it/s, loss=2.52e-5]\n",
      "Epoch 212/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.57it/s, loss=0.000425]\n",
      "Epoch 213/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.21it/s, loss=0.000252]\n",
      "Epoch 214/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.94it/s, loss=1.82e-5]\n",
      "Epoch 215/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.20it/s, loss=1.01e-5]\n",
      "Epoch 216/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.68it/s, loss=3.44e-5]\n",
      "Epoch 217/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.84it/s, loss=9.78e-5]\n",
      "Epoch 218/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.23it/s, loss=2.8e-5]\n",
      "Epoch 219/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.33it/s, loss=2.56e-5]\n",
      "Epoch 220/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.53it/s, loss=0.0179]\n",
      "Epoch 221/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.24it/s, loss=1.26e-5]\n",
      "Epoch 222/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.83it/s, loss=4.62e-5]\n",
      "Epoch 223/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.31it/s, loss=1.31e-5]\n",
      "Epoch 224/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.02it/s, loss=1.55e-5]\n",
      "Epoch 225/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.84it/s, loss=3.1e-5]\n",
      "Epoch 226/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.20it/s, loss=0.00727]\n",
      "Epoch 227/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.20it/s, loss=1.04e-5]\n",
      "Epoch 228/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.13it/s, loss=0.000229]\n",
      "Epoch 229/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.83it/s, loss=1.36e-5]\n",
      "Epoch 230/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.96it/s, loss=5.88e-5]\n",
      "Epoch 231/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.82it/s, loss=2.49e-5]\n",
      "Epoch 232/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.70it/s, loss=7.12e-5]\n",
      "Epoch 233/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.10it/s, loss=4.65e-5]\n",
      "Epoch 234/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.86it/s, loss=1.98e-5]\n",
      "Epoch 235/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.44it/s, loss=0.000825]\n",
      "Epoch 236/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.24it/s, loss=0.000102]\n",
      "Epoch 237/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.30it/s, loss=2.69e-5]\n",
      "Epoch 238/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.78it/s, loss=5.81e-5]\n",
      "Epoch 239/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.64it/s, loss=1.76e-5]\n",
      "Epoch 240/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.44it/s, loss=3.92e-5]\n",
      "Epoch 241/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.53it/s, loss=2.21e-5]\n",
      "Epoch 242/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.45it/s, loss=1.73e-5]\n",
      "Epoch 243/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.05it/s, loss=0.000105]\n",
      "Epoch 244/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.52it/s, loss=1.25e-5]\n",
      "Epoch 245/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.84it/s, loss=1.21e-5]\n",
      "Epoch 246/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.58it/s, loss=3.63e-5]\n",
      "Epoch 247/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.41it/s, loss=4.07e-5]\n",
      "Epoch 248/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.63it/s, loss=1.68e-5]\n",
      "Epoch 249/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.13it/s, loss=5.08e-6]\n",
      "Epoch 250/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.23it/s, loss=0.000114]\n",
      "Epoch 251/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.66it/s, loss=0.00363]\n",
      "Epoch 252/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.64it/s, loss=3.13e-5]\n",
      "Epoch 253/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.00it/s, loss=5.61e-5]\n",
      "Epoch 254/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.45it/s, loss=0.000385]\n",
      "Epoch 255/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.16it/s, loss=3.19e-5]\n",
      "Epoch 256/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.70it/s, loss=4.13e-5]\n",
      "Epoch 257/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.61it/s, loss=1.89e-5]\n",
      "Epoch 258/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.55it/s, loss=6.38e-5]\n",
      "Epoch 259/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.44it/s, loss=5.56e-5]\n",
      "Epoch 260/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.85it/s, loss=2.07e-5]\n",
      "Epoch 261/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.84it/s, loss=8.67e-6]\n",
      "Epoch 262/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.09it/s, loss=9.05e-5]\n",
      "Epoch 263/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.90it/s, loss=1.3e-5]\n",
      "Epoch 264/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.44it/s, loss=1.36e-5]\n",
      "Epoch 265/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 61.94it/s, loss=1.38e-5]\n",
      "Epoch 266/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.64it/s, loss=2.69e-5]\n",
      "Epoch 267/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.41it/s, loss=2.94e-5]\n",
      "Epoch 268/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.98it/s, loss=0.00969]\n",
      "Epoch 269/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.37it/s, loss=5.61e-5]\n",
      "Epoch 270/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.55it/s, loss=1.24e-5]\n",
      "Epoch 271/300: 100%|█████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.03it/s, loss=0.036]\n",
      "Epoch 272/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.47it/s, loss=1.24e-5]\n",
      "Epoch 273/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.46it/s, loss=1.41e-5]\n",
      "Epoch 274/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.37it/s, loss=7.41e-5]\n",
      "Epoch 275/300: 100%|██████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.08it/s, loss=3e-5]\n",
      "Epoch 276/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.42it/s, loss=7.38e-5]\n",
      "Epoch 277/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.84it/s, loss=2.25e-5]\n",
      "Epoch 278/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.76it/s, loss=2.94e-5]\n",
      "Epoch 279/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.12it/s, loss=1.82e-5]\n",
      "Epoch 280/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.02it/s, loss=2.62e-5]\n",
      "Epoch 281/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.64it/s, loss=1.9e-5]\n",
      "Epoch 282/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.68it/s, loss=5.03e-5]\n",
      "Epoch 283/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.38it/s, loss=0.000802]\n",
      "Epoch 284/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.82it/s, loss=2.92e-5]\n",
      "Epoch 285/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.40it/s, loss=1.82e-5]\n",
      "Epoch 286/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.29it/s, loss=0.000461]\n",
      "Epoch 287/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 64.05it/s, loss=8.04e-5]\n",
      "Epoch 288/300: 100%|██████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.49it/s, loss=0.000184]\n",
      "Epoch 289/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.42it/s, loss=2.55e-5]\n",
      "Epoch 290/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.85it/s, loss=0.00501]\n",
      "Epoch 291/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.30it/s, loss=1.49e-5]\n",
      "Epoch 292/300: 100%|████████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.22it/s, loss=1.1e-5]\n",
      "Epoch 293/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.44it/s, loss=2.82e-5]\n",
      "Epoch 294/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 62.59it/s, loss=1.08e-5]\n",
      "Epoch 295/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.59it/s, loss=2.61e-5]\n",
      "Epoch 296/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.80it/s, loss=3.35e-5]\n",
      "Epoch 297/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.30it/s, loss=6.09e-5]\n",
      "Epoch 298/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.63it/s, loss=3.97e-5]\n",
      "Epoch 299/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.40it/s, loss=6.81e-6]\n",
      "Epoch 300/300: 100%|███████████████████████████████████████████████████| 418/418 [00:06<00:00, 63.05it/s, loss=9.91e-6]\n"
     ]
    }
   ],
   "source": [
    "MLP_model.train()\n",
    "MLP_model.bert.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n",
    "\n",
    "    for inputs, labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        segments = torch.tensor([[0 for _ in i] for i in inputs])\n",
    "        masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in inputs])\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        segments, masked_poses = segments.to(device), masked_poses.to(device)\n",
    "        \n",
    "        outputs = MLP_model(inputs, segments, masked_poses)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm with the current loss\n",
    "        pbar.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3956915-e016-4c94-8b83-552bf63ad6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Precision: 0.999\n",
      "Recall: 1.000\n",
      "F1 Score: 1.000\n",
      "AUC: 1.000\n"
     ]
    }
   ],
   "source": [
    "segments = torch.tensor([[0 for _ in i] for i in train_inputs])\n",
    "masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in train_inputs])\n",
    "\n",
    "segments = segments.to(device)\n",
    "masked_poses = masked_poses.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = MLP_model(train_inputs, segments, masked_poses)\n",
    "    predictions = (train_outputs > 0.5).float().cpu().numpy()\n",
    "    train_labels_numpy = train_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(train_labels_numpy, predictions_binary)\n",
    "precision = precision_score(train_labels_numpy, predictions_binary)\n",
    "recall = recall_score(train_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(train_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(train_labels_numpy, train_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3576624d-bdf2-4f5a-bf23-d3a1e5d82564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7654\n",
      "Precision: 0.8455\n",
      "Recall: 0.6495\n",
      "F1 Score: 0.7347\n",
      "AUC: 0.8663\n",
      "AUPR: 0.8578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "# ... (previous code)\n",
    "MLP_model.eval()\n",
    "MLP_model.bert.eval()\n",
    "\n",
    "segments = torch.tensor([[0 for _ in i] for i in test_inputs])\n",
    "masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in test_inputs])\n",
    "\n",
    "segments = segments.to(device)\n",
    "masked_poses = masked_poses.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = MLP_model(test_inputs, segments, masked_poses)\n",
    "    predictions = (test_outputs > 0.01).float().cpu().numpy()\n",
    "    test_labels_numpy = test_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.01).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(test_labels_numpy, predictions_binary)\n",
    "precision = precision_score(test_labels_numpy, predictions_binary)\n",
    "recall = recall_score(test_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(test_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(test_labels_numpy, test_outputs.cpu().numpy())\n",
    "aupr = average_precision_score(test_labels_numpy, test_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'AUPR: {aupr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902a30f-23bd-4f69-839b-ec7fa434fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
