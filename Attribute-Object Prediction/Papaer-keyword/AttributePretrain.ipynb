{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2164e0a1-a77d-410d-83e1-2c4233d3376d",
   "metadata": {},
   "source": [
    "# revised\n",
    "Used for revised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the maximum of length of sequences\n",
    "max_len = 380 * 2 + 3\n",
    "# the number of tokens (objects or attributes)\n",
    "max_vocab = 5373\n",
    "# the maximum number of masked tokens\n",
    "max_pred = 4\n",
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 32\n",
    "# dimension of embedding\n",
    "d_model = 224  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 7\n",
    "# number of encoders\n",
    "n_layers = 7\n",
    "# the number of input setences\n",
    "n_segs = 2\n",
    "\n",
    "p_dropout = .1\n",
    "\n",
    "#80% the chosen token is replaced by [mask], 10% is replaced by a random token, 10% do nothing\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in word2idx\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    # The size of pad_mask is [batch, seq_len, seq_len]\n",
    "    # The resulting tensor has True where padding tokens are located and False elsewhere.\n",
    "    \n",
    "    # print(f'the shape of pad_mask is {pad_mask.shape}')\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        word_enc = self.word_emb(x)\n",
    "        \n",
    "        '''\n",
    "        maybe positional embedding can be deleted\n",
    "        '''\n",
    "        # positional embedding\n",
    "        # pos = torch.arange(x.shape[1], dtype=torch.long, device=device) # .long: round down\n",
    "        # pos = pos.unsqueeze(0).expand_as(x) # the shape is [1, seq_len]\n",
    "        # pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# pre-LN is easier to train than post-LN, but if fullly training, post_LN have better result than pre-LN. \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
    "\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The full connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embeddings()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "        \n",
    "        # next sentence prediction. output is 0 or 1.\n",
    "        self.next_cls = nn.Linear(d_model, 2)\n",
    "        self.gelu = gelu\n",
    "        \n",
    "        # Sharing weight between some fully connect layer, this will make training easier.\n",
    "        shared_weight = self.pooler.fc.weight\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.fc.weight = shared_weight\n",
    "\n",
    "        shared_weight = self.embedding.word_emb.weight\n",
    "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
    "        self.word_classifier.weight = shared_weight\n",
    "\n",
    "    def forward(self, tokens, segments, masked_pos):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "\n",
    "        # NSP Task\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        logits_cls = self.next_cls(hidden_pool)\n",
    "\n",
    "        # Masked Language Model Task\n",
    "        '''\n",
    "        extracting representations of masked positions, \n",
    "        passing them through a fully connected layer, \n",
    "        applying the GELU activation function, \n",
    "        and making predictions using the word classifier\n",
    "        '''\n",
    "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
    "\n",
    "        # h_masked: [batch, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
    "        h_masked = self.gelu(self.fc(h_masked))\n",
    "        logits_lm = self.word_classifier(h_masked)\n",
    "        # logits_lm: [batch, max_pred, max_vocab]\n",
    "        # logits_cls: [batch, 2]\n",
    "\n",
    "        return logits_cls, logits_lm, hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9177c2-dcb1-4500-bf67-e0f7ee0d286a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14012f5d-fc34-42ba-a496-1bd61287973d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "def process_train_intents_from_file(filename) :\n",
    "    intents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) <= 2:\n",
    "                intent = parts[0].strip()\n",
    "                intents.append(intent)\n",
    "    intents = intents[1:]\n",
    "    attribute_list = list(set(\" \".join(intents).split()))\n",
    "    sorted_attribute_list = sorted(map(int, attribute_list))\n",
    "    # print(\"The number of attributes is\",len(sorted_attribute_list))\n",
    "    \n",
    "    # Create the object2idx dictionary\n",
    "    attribute2idx = {'a' + str(attri): int(attri) + 162  for  attri in sorted_attribute_list}\n",
    "    sorted_attribute_list = list(map(str, sorted_attribute_list ))\n",
    "    # print(sorted_attribute_list)\n",
    "    # print(attribute2idx)\n",
    "    special_tokens = {'[PAD]': max_vocab-4, '[CLS]': max_vocab-3, '[SEP]': max_vocab-2, '[MASK]': max_vocab-1 }\n",
    "\n",
    "    attribute2idx.update(special_tokens)\n",
    "\n",
    "    idx2attribute = {idx: attribute for attribute, idx in attribute2idx.items()}\n",
    "    vocab_size = len(attribute2idx)\n",
    "    # assert len(attribute2idx) == len(idx2attribute)\n",
    "    \n",
    "    modified_intents = [' '.join(['a' + token for token in item.split()]) for item in intents]\n",
    "    # print(intents)\n",
    "    # print(modified_intents)\n",
    "    \n",
    "    intent_token_list = []\n",
    "    for intent in modified_intents:\n",
    "        intent_token_list.append([\n",
    "            attribute2idx[s] for s in intent.split()\n",
    "        ])\n",
    "    \n",
    "    maxlen = 0\n",
    "    for intent in intent_token_list :\n",
    "        maxlen = max(maxlen, len(intent))\n",
    "    print(maxlen)\n",
    "    \n",
    "    return intent_token_list, attribute2idx, modified_intents, sorted_attribute_list\n",
    "\n",
    "# intent_token_test, attribute2idx, modified_intents = process_test_intents_from_file('icfca-context_concepts.txt') \n",
    "intent_token_train, attribute2idx, modified_intents_train, train_attribute_list = process_train_intents_from_file('paper-keywords-all-with-missing-part-renumbered_concepts.txt')\n",
    "# print(attribute2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f03f47b-2af5-4b2f-90fa-eb9939803580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# padding the token lists to have the same length.\n",
    "def padding(ids, n_pads, pad_symb=0):\n",
    "    return ids.extend([pad_symb for _ in range(n_pads)])\n",
    "\n",
    "def masking_procedure(cand_pos, input_ids, masked_symb='[MASK]'):\n",
    "    masked_pos = []\n",
    "    masked_tokens = []\n",
    "    for pos in cand_pos:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random.random() < p_mask:\n",
    "            input_ids[pos] = masked_symb\n",
    "        elif random.random() > (p_mask + p_replace):\n",
    "            rand_word_idx = random.randint(0, max_vocab - 4)\n",
    "            input_ids[pos] = rand_word_idx\n",
    "\n",
    "    return masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a28666-9619-4be9-b41d-287b585b9d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8854\n"
     ]
    }
   ],
   "source": [
    "def get_neighbor_samples(extents) :\n",
    "    n = len(extents)\n",
    "    samples = []\n",
    "\n",
    "    dep = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "    neighbor = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "\n",
    "    for i in range(n) :\n",
    "        for j in range(i + 1, n) :\n",
    "            if set(extents[i]).issubset(set(extents[j])) :\n",
    "                dep[i][j] = 1\n",
    "            if set(extents[j]).issubset(set(extents[i])) :\n",
    "                dep[j][i] = 1\n",
    "\n",
    "    for i in range(n) :\n",
    "        se = set([])\n",
    "        for j in range(n) :\n",
    "            if j != i :\n",
    "                if dep[j][i] == 1 :\n",
    "                    rep = False\n",
    "                    lst = list(se)\n",
    "                    for idk, k in enumerate(lst) :\n",
    "                        if dep[k][j] :\n",
    "                            se.remove(k)\n",
    "                            se.add(j)\n",
    "                            rep = True\n",
    "                        if dep[j][k] :\n",
    "                            rep = True\n",
    "                    if not rep :\n",
    "                        se.add(j)\n",
    "\n",
    "        for j in range(n) :\n",
    "            if j in se :\n",
    "                samples.append([i, j, True])\n",
    "            elif random.random() < 0.0018 :\n",
    "                samples.append([i, j, False])\n",
    "        \n",
    "    return samples\n",
    "\n",
    "all_samples = get_neighbor_samples(intent_token_train)\n",
    "print(len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06cfdfe8-0dd8-497d-a3d5-e3b9a3a5609c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "nf = 0\n",
    "nt = 0\n",
    "for sample in all_samples :\n",
    "    extent1, extent2, label = sample\n",
    "    if label == False :\n",
    "        nf += 1\n",
    "    else :\n",
    "        nt += 1\n",
    "\n",
    "new_all_samples = []\n",
    "droprate = nt / nf\n",
    "\n",
    "for sample in all_samples :\n",
    "    extent1, extent2, label = sample\n",
    "    if label == True :\n",
    "        new_all_samples.append([extent1, extent2, True])\n",
    "    elif random.random() < droprate :\n",
    "        new_all_samples.append([extent1, extent2, False])\n",
    "        \n",
    "with open('attribute_pretrain_samples.pkl', 'wb') as f:\n",
    "    pickle.dump(new_all_samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78875dc7-d9b7-4e52-b3ba-982eb1d5a3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7429\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('attribute_pretrain_samples.pkl', 'rb') as f:\n",
    "    all_samples = pickle.load(f)\n",
    "print(len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5afe0534-d54a-4aa2-9aaa-384813468cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_permuted_token_list(tokens, thres = 4) :\n",
    "    tokens_list = []\n",
    "    if len(tokens) <= thres :\n",
    "        permutations = itertools.permutations(tokens)\n",
    "        tokens_list = [list(p) for p in permutations]\n",
    "    else :\n",
    "        for i in range(math.comb(thres, thres)) :\n",
    "            random.shuffle(tokens)\n",
    "            tokens_list.append(tokens.copy())\n",
    "    return tokens_list\n",
    "\n",
    "# A list of sentences and the desired number of data samples as input.\n",
    "def make_data(intents, all_samples, word2idx, n_data, num_per_sample = 120):\n",
    "    batch_data = []\n",
    "    # positive = negative = 0\n",
    "    max_len = 0\n",
    "    len_sentences = len(intents)\n",
    "    for intent in intents :\n",
    "        max_len = max(max_len, len(intent))\n",
    "    max_len = max_len * 2 + 3\n",
    "    print(max_len)\n",
    "    for sample in all_samples :\n",
    "        \n",
    "        tokens_a_idx = sample[0]\n",
    "        tokens_b_idx = sample[1]\n",
    "        tokens_a = intent_token_train[tokens_a_idx]\n",
    "        tokens_b = intent_token_train[tokens_b_idx]\n",
    "            \n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0 for i in range(\n",
    "            1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))]\n",
    "\n",
    "        # Determines the number of positions to mask (n_pred) based on the input sequence length.\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * .15)))\n",
    "        cand_pos = [i for i, token in enumerate(input_ids)\n",
    "                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] #exclude special tokens.\n",
    "\n",
    "        # shuffle all candidate position index, to sampling maksed position from first n_pred\n",
    "        masked_pos, masked_tokens = masking_procedure(\n",
    "            cand_pos[:n_pred], input_ids, word2idx['[MASK]'])\n",
    "\n",
    "        # zero padding for tokens to ensure that the input sequences and segment IDs have the maximum sequence length\n",
    "        padding(input_ids, max_len - len(input_ids))\n",
    "        # print(\"the size of input_ids is \" ,len(input_ids))\n",
    "        padding(segment_ids, max_len - len(segment_ids))\n",
    "        # print(\"the size of segment_ids is \" ,len(segment_ids))\n",
    "\n",
    "        # zero padding for mask\n",
    "        if max_pred > n_pred:\n",
    "            n_pads = max_pred - n_pred\n",
    "            padding(masked_pos, n_pads)\n",
    "            padding(masked_tokens, n_pads)\n",
    "\n",
    "        # Creating Batch Data:\n",
    "        batch_data.append(\n",
    "            [input_ids, segment_ids, masked_tokens, masked_pos, sample[2]])\n",
    "    \n",
    "    random.shuffle(batch_data)\n",
    "    print(len(batch_data))\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.is_next = is_next\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.is_next[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b5fd-3776-4641-baa3-d82a8c496007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-Train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b7abc56-ac8f-449d-9709-e60d720dace3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DO_NSP_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3a030f9-caff-4a5e-8763-9a061fbabe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 22 \n",
    "lr = 2e-5\n",
    "epochs = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40521dbb-258b-4a9e-a94f-859e4a6836a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n",
      "8875\n",
      "Entering training process...\n",
      "Epoch:1 \t loss: 4.312276\n",
      "Epoch:2 \t loss: 3.355243\n",
      "Epoch:3 \t loss: 4.346218\n",
      "Epoch:4 \t loss: 5.445357\n",
      "Epoch:5 \t loss: 2.498304\n",
      "Epoch:6 \t loss: 3.531572\n",
      "Epoch:7 \t loss: 3.751374\n",
      "Epoch:8 \t loss: 2.953616\n",
      "Epoch:9 \t loss: 2.580443\n",
      "Epoch:10 \t loss: 2.824664\n",
      "Epoch:11 \t loss: 3.138757\n",
      "Epoch:12 \t loss: 2.425591\n",
      "Epoch:13 \t loss: 2.037939\n",
      "Epoch:14 \t loss: 1.429157\n",
      "Epoch:15 \t loss: 1.993575\n",
      "Epoch:16 \t loss: 1.634073\n",
      "Epoch:17 \t loss: 2.568707\n",
      "Epoch:18 \t loss: 1.956083\n",
      "Epoch:19 \t loss: 1.826522\n",
      "Epoch:20 \t loss: 1.388265\n",
      "Epoch:21 \t loss: 1.132976\n",
      "Epoch:22 \t loss: 1.338442\n",
      "Epoch:23 \t loss: 1.354661\n",
      "Epoch:24 \t loss: 2.367274\n",
      "Epoch:25 \t loss: 1.795287\n",
      "Epoch:26 \t loss: 1.703192\n",
      "Epoch:27 \t loss: 1.305958\n",
      "Epoch:28 \t loss: 1.431230\n",
      "Epoch:29 \t loss: 1.904119\n",
      "Epoch:30 \t loss: 1.557137\n",
      "Epoch:31 \t loss: 1.472818\n",
      "Epoch:32 \t loss: 1.403951\n",
      "Epoch:33 \t loss: 0.881397\n",
      "Epoch:34 \t loss: 1.592016\n",
      "Epoch:35 \t loss: 1.065827\n",
      "Epoch:36 \t loss: 1.200033\n",
      "Epoch:37 \t loss: 1.055885\n",
      "Epoch:38 \t loss: 1.237871\n",
      "Epoch:39 \t loss: 1.285274\n",
      "Epoch:40 \t loss: 0.760003\n",
      "Epoch:41 \t loss: 1.146934\n",
      "Epoch:42 \t loss: 1.045199\n",
      "Epoch:43 \t loss: 1.436242\n",
      "Epoch:44 \t loss: 0.796569\n",
      "Epoch:45 \t loss: 0.632455\n",
      "Epoch:46 \t loss: 0.737840\n",
      "Epoch:47 \t loss: 1.433126\n",
      "Epoch:48 \t loss: 0.875031\n",
      "Epoch:49 \t loss: 1.220601\n",
      "Epoch:50 \t loss: 2.083555\n",
      "Epoch:51 \t loss: 0.855571\n",
      "Epoch:52 \t loss: 0.938236\n",
      "Epoch:53 \t loss: 1.180676\n",
      "Epoch:54 \t loss: 1.147219\n",
      "Epoch:55 \t loss: 0.824263\n",
      "Epoch:56 \t loss: 0.797578\n",
      "Epoch:57 \t loss: 1.501930\n",
      "Epoch:58 \t loss: 0.904549\n",
      "Epoch:59 \t loss: 0.913660\n",
      "Epoch:60 \t loss: 0.737448\n",
      "Epoch:61 \t loss: 0.678687\n",
      "Epoch:62 \t loss: 0.853076\n",
      "Epoch:63 \t loss: 0.938935\n",
      "Epoch:64 \t loss: 0.748791\n",
      "Epoch:65 \t loss: 1.378633\n",
      "Epoch:66 \t loss: 1.238168\n",
      "Epoch:67 \t loss: 0.775360\n",
      "Epoch:68 \t loss: 1.621946\n",
      "Epoch:69 \t loss: 1.236719\n",
      "Epoch:70 \t loss: 0.807881\n",
      "Epoch:71 \t loss: 0.569750\n",
      "Epoch:72 \t loss: 0.614614\n",
      "Epoch:73 \t loss: 0.986582\n",
      "Epoch:74 \t loss: 0.889103\n",
      "Epoch:75 \t loss: 0.981590\n",
      "Epoch:76 \t loss: 0.690094\n",
      "Epoch:77 \t loss: 0.679274\n",
      "Epoch:78 \t loss: 0.590431\n",
      "Epoch:79 \t loss: 1.122215\n",
      "Epoch:80 \t loss: 0.975103\n",
      "Epoch:81 \t loss: 0.388442\n",
      "Epoch:82 \t loss: 0.867613\n",
      "Epoch:83 \t loss: 0.769550\n",
      "Epoch:84 \t loss: 0.744121\n",
      "Epoch:85 \t loss: 0.545645\n",
      "Epoch:86 \t loss: 0.895991\n",
      "Epoch:87 \t loss: 0.491447\n",
      "Epoch:88 \t loss: 0.645401\n",
      "Epoch:89 \t loss: 0.917291\n",
      "Epoch:90 \t loss: 0.686628\n",
      "Epoch:91 \t loss: 0.700468\n",
      "Epoch:92 \t loss: 0.711716\n",
      "Epoch:93 \t loss: 0.508295\n",
      "Epoch:94 \t loss: 0.950451\n",
      "Epoch:95 \t loss: 0.609160\n",
      "Epoch:96 \t loss: 0.812770\n",
      "Epoch:97 \t loss: 0.741937\n",
      "Epoch:98 \t loss: 0.734110\n",
      "Epoch:99 \t loss: 0.751606\n",
      "Epoch:100 \t loss: 0.538523\n",
      "Epoch:101 \t loss: 0.785991\n",
      "Epoch:102 \t loss: 1.023189\n",
      "Epoch:103 \t loss: 0.842463\n",
      "Epoch:104 \t loss: 1.139260\n",
      "Epoch:105 \t loss: 0.816876\n",
      "Epoch:106 \t loss: 0.519869\n",
      "Epoch:107 \t loss: 0.553308\n",
      "Epoch:108 \t loss: 0.354919\n",
      "Epoch:109 \t loss: 0.779757\n",
      "Epoch:110 \t loss: 0.058342\n",
      "Epoch:111 \t loss: 0.866516\n",
      "Epoch:112 \t loss: 0.769111\n",
      "Epoch:113 \t loss: 0.397960\n",
      "Epoch:114 \t loss: 0.572947\n",
      "Epoch:115 \t loss: 0.094948\n",
      "Epoch:116 \t loss: 0.263829\n",
      "Epoch:117 \t loss: 0.794547\n",
      "Epoch:118 \t loss: 0.916034\n",
      "Epoch:119 \t loss: 0.453031\n",
      "Epoch:120 \t loss: 0.637538\n",
      "Epoch:121 \t loss: 0.809977\n",
      "Epoch:122 \t loss: 0.202124\n",
      "Epoch:123 \t loss: 0.381754\n",
      "Epoch:124 \t loss: 0.512610\n",
      "Epoch:125 \t loss: 0.554516\n",
      "Epoch:126 \t loss: 0.493331\n",
      "Epoch:127 \t loss: 1.013269\n",
      "Epoch:128 \t loss: 0.462583\n",
      "Epoch:129 \t loss: 0.511673\n",
      "Epoch:130 \t loss: 0.181121\n",
      "Epoch:131 \t loss: 0.301081\n",
      "Epoch:132 \t loss: 0.409288\n",
      "Epoch:133 \t loss: 0.465056\n",
      "Epoch:134 \t loss: 0.501619\n",
      "Epoch:135 \t loss: 0.502291\n",
      "Epoch:136 \t loss: 0.469648\n",
      "Epoch:137 \t loss: 0.305217\n",
      "Epoch:138 \t loss: 0.812710\n",
      "Epoch:139 \t loss: 0.644319\n",
      "Epoch:140 \t loss: 0.544402\n",
      "Epoch:141 \t loss: 0.382698\n",
      "Epoch:142 \t loss: 0.115846\n",
      "Epoch:143 \t loss: 0.923015\n",
      "Epoch:144 \t loss: 0.834065\n",
      "Epoch:145 \t loss: 0.777574\n",
      "Epoch:146 \t loss: 0.869590\n",
      "Epoch:147 \t loss: 0.573149\n",
      "Epoch:148 \t loss: 0.234721\n",
      "Epoch:149 \t loss: 0.413723\n",
      "Epoch:150 \t loss: 0.381125\n",
      "Epoch:151 \t loss: 0.234315\n",
      "Epoch:152 \t loss: 0.008272\n",
      "Epoch:153 \t loss: 0.547090\n",
      "Epoch:154 \t loss: 0.408400\n",
      "Epoch:155 \t loss: 0.492381\n",
      "Epoch:156 \t loss: 0.252797\n",
      "Epoch:157 \t loss: 0.588481\n",
      "Epoch:158 \t loss: 0.360366\n",
      "Epoch:159 \t loss: 0.249888\n",
      "Epoch:160 \t loss: 0.383483\n",
      "Epoch:161 \t loss: 0.687588\n",
      "Epoch:162 \t loss: 0.806264\n",
      "Epoch:163 \t loss: 0.786505\n",
      "Epoch:164 \t loss: 0.695273\n",
      "Epoch:165 \t loss: 0.330335\n",
      "Epoch:166 \t loss: 0.503527\n",
      "Epoch:167 \t loss: 0.794412\n",
      "Epoch:168 \t loss: 0.400408\n",
      "Epoch:169 \t loss: 0.502994\n",
      "Epoch:170 \t loss: 0.718926\n",
      "Epoch:171 \t loss: 0.293180\n",
      "Epoch:172 \t loss: 0.565320\n",
      "Epoch:173 \t loss: 0.571975\n",
      "Epoch:174 \t loss: 0.243077\n",
      "Epoch:175 \t loss: 0.660848\n",
      "Epoch:176 \t loss: 0.514512\n",
      "Epoch:177 \t loss: 0.238684\n",
      "Epoch:178 \t loss: 0.806805\n",
      "Epoch:179 \t loss: 0.461668\n",
      "Epoch:180 \t loss: 0.120371\n",
      "Epoch:181 \t loss: 0.434901\n",
      "Epoch:182 \t loss: 0.481332\n",
      "Epoch:183 \t loss: 0.667235\n",
      "Epoch:184 \t loss: 0.300323\n",
      "Epoch:185 \t loss: 0.398851\n",
      "Epoch:186 \t loss: 0.525634\n",
      "Epoch:187 \t loss: 0.400268\n",
      "Epoch:188 \t loss: 0.318146\n",
      "Epoch:189 \t loss: 0.613182\n",
      "Epoch:190 \t loss: 0.516458\n",
      "Epoch:191 \t loss: 0.506854\n",
      "Epoch:192 \t loss: 0.987057\n",
      "Epoch:193 \t loss: 0.758732\n",
      "Epoch:194 \t loss: 0.454767\n",
      "Epoch:195 \t loss: 0.434524\n",
      "Epoch:196 \t loss: 0.572255\n",
      "Epoch:197 \t loss: 0.311136\n",
      "Epoch:198 \t loss: 0.495886\n",
      "Epoch:199 \t loss: 0.583444\n",
      "Epoch:200 \t loss: 0.496325\n",
      "Epoch:201 \t loss: 0.185986\n",
      "Epoch:202 \t loss: 0.430199\n",
      "Epoch:203 \t loss: 0.114065\n",
      "Epoch:204 \t loss: 0.211542\n",
      "Epoch:205 \t loss: 0.730061\n",
      "Epoch:206 \t loss: 0.584947\n",
      "Epoch:207 \t loss: 0.325328\n",
      "Epoch:208 \t loss: 0.409659\n",
      "Epoch:209 \t loss: 0.857322\n",
      "Epoch:210 \t loss: 0.086618\n",
      "Epoch:211 \t loss: 0.603395\n",
      "Epoch:212 \t loss: 0.748705\n",
      "Epoch:213 \t loss: 0.502704\n",
      "Epoch:214 \t loss: 0.459187\n",
      "Epoch:215 \t loss: 0.219548\n",
      "Epoch:216 \t loss: 0.425777\n",
      "Epoch:217 \t loss: 0.262894\n",
      "Epoch:218 \t loss: 0.331849\n",
      "Epoch:219 \t loss: 0.412706\n",
      "Epoch:220 \t loss: 0.420114\n",
      "Epoch:221 \t loss: 0.385976\n",
      "Epoch:222 \t loss: 0.324304\n",
      "Epoch:223 \t loss: 0.473900\n",
      "Epoch:224 \t loss: 0.396316\n",
      "Epoch:225 \t loss: 0.405566\n",
      "Epoch:226 \t loss: 0.486739\n",
      "Epoch:227 \t loss: 0.835840\n",
      "Epoch:228 \t loss: 0.525921\n",
      "Epoch:229 \t loss: 0.672946\n",
      "Epoch:230 \t loss: 0.190965\n",
      "Epoch:231 \t loss: 0.270501\n",
      "Epoch:232 \t loss: 0.371694\n",
      "Epoch:233 \t loss: 0.369784\n",
      "Epoch:234 \t loss: 0.665055\n",
      "Epoch:235 \t loss: 0.629093\n",
      "Epoch:236 \t loss: 0.506226\n",
      "Epoch:237 \t loss: 0.543366\n",
      "Epoch:238 \t loss: 0.350314\n",
      "Epoch:239 \t loss: 0.482754\n",
      "Epoch:240 \t loss: 0.787945\n",
      "Epoch:241 \t loss: 0.172646\n",
      "Epoch:242 \t loss: 0.298647\n",
      "Epoch:243 \t loss: 0.629750\n",
      "Epoch:244 \t loss: 0.547814\n",
      "Epoch:245 \t loss: 0.510239\n",
      "Epoch:246 \t loss: 0.661582\n",
      "Epoch:247 \t loss: 0.696814\n",
      "Epoch:248 \t loss: 0.142942\n",
      "Epoch:249 \t loss: 0.458225\n",
      "Epoch:250 \t loss: 0.546382\n",
      "Epoch:251 \t loss: 0.456089\n",
      "Epoch:252 \t loss: 0.496682\n",
      "Epoch:253 \t loss: 0.470970\n",
      "Epoch:254 \t loss: 0.094276\n",
      "Epoch:255 \t loss: 0.511569\n",
      "Epoch:256 \t loss: 0.447287\n",
      "Epoch:257 \t loss: 0.419006\n",
      "Epoch:258 \t loss: 0.373925\n",
      "Epoch:259 \t loss: 0.528917\n",
      "Epoch:260 \t loss: 0.521993\n",
      "Epoch:261 \t loss: 0.411508\n",
      "Epoch:262 \t loss: 0.559997\n",
      "Epoch:263 \t loss: 0.145079\n",
      "Epoch:264 \t loss: 0.296725\n",
      "Epoch:265 \t loss: 0.519382\n",
      "Epoch:266 \t loss: 0.183421\n",
      "Epoch:267 \t loss: 0.187720\n",
      "Epoch:268 \t loss: 0.433689\n",
      "Epoch:269 \t loss: 0.722627\n",
      "Epoch:270 \t loss: 0.114320\n",
      "Epoch:271 \t loss: 0.449469\n",
      "Epoch:272 \t loss: 0.312414\n",
      "Epoch:273 \t loss: 0.380897\n",
      "Epoch:274 \t loss: 0.416074\n",
      "Epoch:275 \t loss: 0.390406\n",
      "Epoch:276 \t loss: 0.328724\n",
      "Epoch:277 \t loss: 0.157654\n",
      "Epoch:278 \t loss: 0.231352\n",
      "Epoch:279 \t loss: 0.445127\n",
      "Epoch:280 \t loss: 0.286973\n",
      "Epoch:281 \t loss: 0.297999\n",
      "Epoch:282 \t loss: 0.358581\n",
      "Epoch:283 \t loss: 0.228659\n",
      "Epoch:284 \t loss: 0.385589\n",
      "Epoch:285 \t loss: 0.361877\n",
      "Epoch:286 \t loss: 0.308908\n",
      "Epoch:287 \t loss: 0.613043\n",
      "Epoch:288 \t loss: 0.244938\n",
      "Epoch:289 \t loss: 0.253730\n",
      "Epoch:290 \t loss: 0.673304\n",
      "Epoch:291 \t loss: 0.293729\n",
      "Epoch:292 \t loss: 0.324306\n",
      "Epoch:293 \t loss: 0.443971\n",
      "Epoch:294 \t loss: 0.746801\n",
      "Epoch:295 \t loss: 0.280664\n",
      "Epoch:296 \t loss: 0.304763\n",
      "Epoch:297 \t loss: 0.544331\n",
      "Epoch:298 \t loss: 0.394957\n",
      "Epoch:299 \t loss: 0.595219\n",
      "Epoch:300 \t loss: 0.296737\n",
      "Epoch:301 \t loss: 0.288544\n",
      "Epoch:302 \t loss: 0.786049\n",
      "Epoch:303 \t loss: 0.168910\n",
      "Epoch:304 \t loss: 0.384778\n",
      "Epoch:305 \t loss: 0.755777\n",
      "Epoch:306 \t loss: 0.471161\n",
      "Epoch:307 \t loss: 0.486195\n",
      "Epoch:308 \t loss: 0.725515\n",
      "Epoch:309 \t loss: 0.241328\n",
      "Epoch:310 \t loss: 0.217479\n",
      "Epoch:311 \t loss: 0.148610\n",
      "Epoch:312 \t loss: 0.757299\n",
      "Epoch:313 \t loss: 0.247290\n",
      "Epoch:314 \t loss: 0.302351\n",
      "Epoch:315 \t loss: 0.097730\n",
      "Epoch:316 \t loss: 0.452333\n",
      "Epoch:317 \t loss: 0.433648\n",
      "Epoch:318 \t loss: 0.579003\n",
      "Epoch:319 \t loss: 0.039353\n",
      "Epoch:320 \t loss: 0.428083\n",
      "Epoch:321 \t loss: 0.208185\n",
      "Epoch:322 \t loss: 0.491023\n",
      "Epoch:323 \t loss: 0.163063\n",
      "Epoch:324 \t loss: 0.564364\n",
      "Epoch:325 \t loss: 0.474306\n",
      "Epoch:326 \t loss: 0.648287\n",
      "Epoch:327 \t loss: 0.332706\n",
      "Epoch:328 \t loss: 0.204813\n",
      "Epoch:329 \t loss: 0.245123\n",
      "Epoch:330 \t loss: 0.494214\n",
      "Epoch:331 \t loss: 0.206265\n",
      "Epoch:332 \t loss: 0.395677\n",
      "Epoch:333 \t loss: 0.170234\n",
      "Epoch:334 \t loss: 0.482886\n",
      "Epoch:335 \t loss: 0.392933\n",
      "Epoch:336 \t loss: 0.385341\n",
      "Epoch:337 \t loss: 0.646500\n",
      "Epoch:338 \t loss: 0.631792\n",
      "Epoch:339 \t loss: 0.424350\n",
      "Epoch:340 \t loss: 0.105539\n",
      "Epoch:341 \t loss: 0.252843\n",
      "Epoch:342 \t loss: 0.377098\n",
      "Epoch:343 \t loss: 0.305379\n",
      "Epoch:344 \t loss: 0.313995\n",
      "Epoch:345 \t loss: 0.234520\n",
      "Epoch:346 \t loss: 0.216374\n",
      "Epoch:347 \t loss: 0.366781\n",
      "Epoch:348 \t loss: 0.182647\n",
      "Epoch:349 \t loss: 0.396296\n",
      "Epoch:350 \t loss: 0.168395\n",
      "Epoch:351 \t loss: 0.292106\n",
      "Epoch:352 \t loss: 0.409693\n",
      "Epoch:353 \t loss: 0.114092\n",
      "Epoch:354 \t loss: 0.722586\n",
      "Epoch:355 \t loss: 0.718691\n",
      "Epoch:356 \t loss: 0.232683\n",
      "Epoch:357 \t loss: 0.533668\n",
      "Epoch:358 \t loss: 0.353573\n",
      "Epoch:359 \t loss: 0.178388\n",
      "Epoch:360 \t loss: 0.388199\n",
      "Epoch:361 \t loss: 0.355086\n",
      "Epoch:362 \t loss: 0.354722\n",
      "Epoch:363 \t loss: 0.276962\n",
      "Epoch:364 \t loss: 0.371645\n",
      "Epoch:365 \t loss: 0.481603\n",
      "Epoch:366 \t loss: 0.299317\n",
      "Epoch:367 \t loss: 0.502222\n",
      "Epoch:368 \t loss: 0.205309\n",
      "Epoch:369 \t loss: 0.331850\n",
      "Epoch:370 \t loss: 0.623360\n",
      "Epoch:371 \t loss: 0.560727\n",
      "Epoch:372 \t loss: 0.266768\n",
      "Epoch:373 \t loss: 0.338859\n",
      "Epoch:374 \t loss: 0.260778\n",
      "Epoch:375 \t loss: 0.295611\n",
      "Epoch:376 \t loss: 0.481873\n",
      "Epoch:377 \t loss: 0.502758\n",
      "Epoch:378 \t loss: 0.362138\n",
      "Epoch:379 \t loss: 0.215236\n",
      "Epoch:380 \t loss: 0.469069\n",
      "Epoch:381 \t loss: 0.367831\n",
      "Epoch:382 \t loss: 0.877837\n",
      "Epoch:383 \t loss: 0.527663\n",
      "Epoch:384 \t loss: 0.215626\n",
      "Epoch:385 \t loss: 0.391000\n",
      "Epoch:386 \t loss: 0.813828\n",
      "Epoch:387 \t loss: 0.551248\n",
      "Epoch:388 \t loss: 0.226792\n",
      "Epoch:389 \t loss: 0.685906\n",
      "Epoch:390 \t loss: 0.073476\n",
      "Epoch:391 \t loss: 0.392728\n",
      "Epoch:392 \t loss: 0.333807\n",
      "Epoch:393 \t loss: 0.590015\n",
      "Epoch:394 \t loss: 0.413373\n",
      "Epoch:395 \t loss: 0.255308\n",
      "Epoch:396 \t loss: 0.195697\n",
      "Epoch:397 \t loss: 0.066319\n",
      "Epoch:398 \t loss: 0.178337\n",
      "Epoch:399 \t loss: 0.355375\n",
      "Epoch:400 \t loss: 0.474772\n",
      "Epoch:401 \t loss: 0.038058\n",
      "Epoch:402 \t loss: 0.192150\n",
      "Epoch:403 \t loss: 0.302882\n",
      "Epoch:404 \t loss: 0.458503\n",
      "Epoch:405 \t loss: 0.431493\n",
      "Epoch:406 \t loss: 0.740947\n",
      "Epoch:407 \t loss: 0.646554\n",
      "Epoch:408 \t loss: 0.310102\n",
      "Epoch:409 \t loss: 0.375392\n",
      "Epoch:410 \t loss: 0.225716\n",
      "Epoch:411 \t loss: 0.566735\n",
      "Epoch:412 \t loss: 0.774715\n",
      "Epoch:413 \t loss: 0.490575\n",
      "Epoch:414 \t loss: 0.576783\n",
      "Epoch:415 \t loss: 0.205195\n",
      "Epoch:416 \t loss: 0.079820\n",
      "Epoch:417 \t loss: 0.351759\n",
      "Epoch:418 \t loss: 0.813737\n",
      "Epoch:419 \t loss: 0.284648\n",
      "Epoch:420 \t loss: 0.533746\n",
      "Epoch:421 \t loss: 0.354516\n",
      "Epoch:422 \t loss: 0.524315\n",
      "Epoch:423 \t loss: 0.381273\n",
      "Epoch:424 \t loss: 0.409284\n",
      "Epoch:425 \t loss: 0.237676\n",
      "Epoch:426 \t loss: 0.316588\n",
      "Epoch:427 \t loss: 0.376205\n",
      "Epoch:428 \t loss: 0.339148\n",
      "Epoch:429 \t loss: 0.214606\n",
      "Epoch:430 \t loss: 0.428419\n",
      "Epoch:431 \t loss: 0.577459\n",
      "Epoch:432 \t loss: 0.241795\n",
      "Epoch:433 \t loss: 0.359938\n",
      "Epoch:434 \t loss: 0.575231\n",
      "Epoch:435 \t loss: 0.543420\n",
      "Epoch:436 \t loss: 0.319587\n",
      "Epoch:437 \t loss: 0.763464\n",
      "Epoch:438 \t loss: 0.747889\n",
      "Epoch:439 \t loss: 0.538989\n",
      "Epoch:440 \t loss: 0.308779\n",
      "Epoch:441 \t loss: 0.235767\n",
      "Epoch:442 \t loss: 0.246720\n",
      "Epoch:443 \t loss: 0.493562\n",
      "Epoch:444 \t loss: 0.068656\n",
      "Epoch:445 \t loss: 0.599510\n",
      "Epoch:446 \t loss: 0.271589\n",
      "Epoch:447 \t loss: 0.432767\n",
      "Epoch:448 \t loss: 0.497423\n",
      "Epoch:449 \t loss: 0.395975\n",
      "Epoch:450 \t loss: 0.541768\n",
      "Epoch:451 \t loss: 0.499395\n",
      "Epoch:452 \t loss: 0.279920\n",
      "Epoch:453 \t loss: 0.555196\n",
      "Epoch:454 \t loss: 0.524133\n",
      "Epoch:455 \t loss: 0.294110\n",
      "Epoch:456 \t loss: 0.636822\n",
      "Epoch:457 \t loss: 0.411488\n",
      "Epoch:458 \t loss: 0.499072\n",
      "Epoch:459 \t loss: 0.167133\n",
      "Epoch:460 \t loss: 0.230099\n",
      "Epoch:461 \t loss: 0.364442\n",
      "Epoch:462 \t loss: 0.456550\n",
      "Epoch:463 \t loss: 0.427990\n",
      "Epoch:464 \t loss: 0.114009\n",
      "Epoch:465 \t loss: 0.150476\n",
      "Epoch:466 \t loss: 0.364221\n",
      "Epoch:467 \t loss: 0.819873\n",
      "Epoch:468 \t loss: 0.337736\n",
      "Epoch:469 \t loss: 0.282604\n",
      "Epoch:470 \t loss: 0.150980\n",
      "Epoch:471 \t loss: 0.303130\n",
      "Epoch:472 \t loss: 0.501730\n",
      "Epoch:473 \t loss: 0.412298\n",
      "Epoch:474 \t loss: 0.188025\n",
      "Epoch:475 \t loss: 0.465265\n",
      "Epoch:476 \t loss: 0.395436\n",
      "Epoch:477 \t loss: 0.714062\n",
      "Epoch:478 \t loss: 0.359420\n",
      "Epoch:479 \t loss: 0.191817\n",
      "Epoch:480 \t loss: 0.248064\n",
      "Epoch:481 \t loss: 0.535097\n",
      "Epoch:482 \t loss: 0.345421\n",
      "Epoch:483 \t loss: 0.260931\n",
      "Epoch:484 \t loss: 0.281608\n",
      "Epoch:485 \t loss: 0.539537\n",
      "Epoch:486 \t loss: 0.357950\n",
      "Epoch:487 \t loss: 0.194979\n",
      "Epoch:488 \t loss: 0.251914\n",
      "Epoch:489 \t loss: 0.480783\n",
      "Epoch:490 \t loss: 0.113633\n",
      "Epoch:491 \t loss: 0.442371\n",
      "Epoch:492 \t loss: 0.297106\n",
      "Epoch:493 \t loss: 0.599900\n",
      "Epoch:494 \t loss: 0.181171\n",
      "Epoch:495 \t loss: 0.340116\n",
      "Epoch:496 \t loss: 0.246125\n",
      "Epoch:497 \t loss: 0.519558\n",
      "Epoch:498 \t loss: 0.437885\n",
      "Epoch:499 \t loss: 0.352160\n",
      "Epoch:500 \t loss: 0.335992\n",
      "Epoch:501 \t loss: 0.194108\n",
      "Epoch:502 \t loss: 0.359388\n",
      "Epoch:503 \t loss: 0.017781\n",
      "Epoch:504 \t loss: 0.415373\n",
      "Epoch:505 \t loss: 0.127411\n",
      "Epoch:506 \t loss: 0.617444\n",
      "Epoch:507 \t loss: 0.195238\n",
      "Epoch:508 \t loss: 0.386150\n",
      "Epoch:509 \t loss: 0.189274\n",
      "Epoch:510 \t loss: 0.773916\n",
      "Epoch:511 \t loss: 0.341933\n",
      "Epoch:512 \t loss: 0.462013\n",
      "Epoch:513 \t loss: 0.407580\n",
      "Epoch:514 \t loss: 0.399855\n",
      "Epoch:515 \t loss: 0.614650\n",
      "Epoch:516 \t loss: 0.456586\n",
      "Epoch:517 \t loss: 0.384696\n",
      "Epoch:518 \t loss: 0.520083\n",
      "Epoch:519 \t loss: 0.245321\n",
      "Epoch:520 \t loss: 0.270055\n",
      "Epoch:521 \t loss: 0.778304\n",
      "Epoch:522 \t loss: 0.482380\n",
      "Epoch:523 \t loss: 0.111674\n",
      "Epoch:524 \t loss: 0.398921\n",
      "Epoch:525 \t loss: 0.312498\n",
      "Epoch:526 \t loss: 0.447741\n",
      "Epoch:527 \t loss: 0.288048\n",
      "Epoch:528 \t loss: 0.412823\n",
      "Epoch:529 \t loss: 0.417820\n",
      "Epoch:530 \t loss: 0.193453\n",
      "Epoch:531 \t loss: 0.613683\n",
      "Epoch:532 \t loss: 0.337810\n",
      "Epoch:533 \t loss: 0.530407\n",
      "Epoch:534 \t loss: 0.284594\n",
      "Epoch:535 \t loss: 0.353974\n",
      "Epoch:536 \t loss: 0.294860\n",
      "Epoch:537 \t loss: 0.244686\n",
      "Epoch:538 \t loss: 0.514897\n",
      "Epoch:539 \t loss: 0.283447\n",
      "Epoch:540 \t loss: 0.202410\n",
      "Epoch:541 \t loss: 0.358573\n",
      "Epoch:542 \t loss: 0.168710\n",
      "Epoch:543 \t loss: 0.435791\n",
      "Epoch:544 \t loss: 0.301187\n",
      "Epoch:545 \t loss: 0.280357\n",
      "Epoch:546 \t loss: 0.247398\n",
      "Epoch:547 \t loss: 0.150426\n",
      "Epoch:548 \t loss: 0.150688\n",
      "Epoch:549 \t loss: 0.160909\n",
      "Epoch:550 \t loss: 0.246008\n",
      "Epoch:551 \t loss: 0.609567\n",
      "Epoch:552 \t loss: 0.299233\n",
      "Epoch:553 \t loss: 0.459369\n",
      "Epoch:554 \t loss: 0.194655\n",
      "Epoch:555 \t loss: 0.374458\n",
      "Epoch:556 \t loss: 0.561002\n",
      "Epoch:557 \t loss: 0.794239\n",
      "Epoch:558 \t loss: 0.254120\n",
      "Epoch:559 \t loss: 0.147776\n",
      "Epoch:560 \t loss: 0.366559\n",
      "Epoch:561 \t loss: 0.201998\n",
      "Epoch:562 \t loss: 0.357776\n",
      "Epoch:563 \t loss: 0.276700\n",
      "Epoch:564 \t loss: 0.248325\n",
      "Epoch:565 \t loss: 0.228436\n",
      "Epoch:566 \t loss: 0.378288\n",
      "Epoch:567 \t loss: 0.566790\n",
      "Epoch:568 \t loss: 0.195336\n",
      "Epoch:569 \t loss: 0.070991\n",
      "Epoch:570 \t loss: 0.414388\n",
      "Epoch:571 \t loss: 0.270606\n",
      "Epoch:572 \t loss: 0.343411\n",
      "Epoch:573 \t loss: 0.580910\n",
      "Epoch:574 \t loss: 0.473493\n",
      "Epoch:575 \t loss: 0.241105\n",
      "Epoch:576 \t loss: 0.267076\n",
      "Epoch:577 \t loss: 0.258542\n",
      "Epoch:578 \t loss: 0.312703\n",
      "Epoch:579 \t loss: 0.196019\n",
      "Epoch:580 \t loss: 0.211329\n",
      "Epoch:581 \t loss: 0.543810\n",
      "Epoch:582 \t loss: 0.358002\n",
      "Epoch:583 \t loss: 0.438700\n",
      "Epoch:584 \t loss: 0.166443\n",
      "Epoch:585 \t loss: 0.627822\n",
      "Epoch:586 \t loss: 0.282079\n",
      "Epoch:587 \t loss: 0.294414\n",
      "Epoch:588 \t loss: 0.591992\n",
      "Epoch:589 \t loss: 0.303356\n",
      "Epoch:590 \t loss: 0.253140\n",
      "Epoch:591 \t loss: 0.485185\n",
      "Epoch:592 \t loss: 0.533077\n",
      "Epoch:593 \t loss: 0.497466\n",
      "Epoch:594 \t loss: 0.425165\n",
      "Epoch:595 \t loss: 0.112766\n",
      "Epoch:596 \t loss: 0.720629\n",
      "Epoch:597 \t loss: 0.405630\n",
      "Epoch:598 \t loss: 0.180582\n",
      "Epoch:599 \t loss: 0.655136\n",
      "Epoch:600 \t loss: 0.304800\n",
      "Epoch:601 \t loss: 0.189639\n",
      "Epoch:602 \t loss: 0.207309\n",
      "Epoch:603 \t loss: 0.487845\n",
      "Epoch:604 \t loss: 0.362261\n",
      "Epoch:605 \t loss: 0.231060\n",
      "Epoch:606 \t loss: 0.275134\n",
      "Epoch:607 \t loss: 0.657981\n",
      "Epoch:608 \t loss: 0.229198\n",
      "Epoch:609 \t loss: 0.409866\n",
      "Epoch:610 \t loss: 0.452365\n",
      "Epoch:611 \t loss: 0.702763\n",
      "Epoch:612 \t loss: 0.240769\n",
      "Epoch:613 \t loss: 0.329158\n",
      "Epoch:614 \t loss: 0.551151\n",
      "Epoch:615 \t loss: 0.351115\n",
      "Epoch:616 \t loss: 0.371053\n",
      "Epoch:617 \t loss: 0.482781\n",
      "Epoch:618 \t loss: 0.434613\n",
      "Epoch:619 \t loss: 0.721391\n",
      "Epoch:620 \t loss: 0.494470\n",
      "Epoch:621 \t loss: 0.453423\n",
      "Epoch:622 \t loss: 0.415106\n",
      "Epoch:623 \t loss: 0.335987\n",
      "Epoch:624 \t loss: 0.196796\n",
      "Epoch:625 \t loss: 0.316777\n",
      "Epoch:626 \t loss: 0.399206\n",
      "Epoch:627 \t loss: 0.529396\n",
      "Epoch:628 \t loss: 0.511029\n",
      "Epoch:629 \t loss: 0.378979\n",
      "Epoch:630 \t loss: 0.528269\n",
      "Epoch:631 \t loss: 0.327091\n",
      "Epoch:632 \t loss: 0.258655\n",
      "Epoch:633 \t loss: 0.465666\n",
      "Epoch:634 \t loss: 0.200115\n",
      "Epoch:635 \t loss: 0.522644\n",
      "Epoch:636 \t loss: 0.274126\n",
      "Epoch:637 \t loss: 0.262603\n",
      "Epoch:638 \t loss: 0.563916\n",
      "Epoch:639 \t loss: 0.317257\n",
      "Epoch:640 \t loss: 0.284292\n",
      "Epoch:641 \t loss: 0.200365\n",
      "Epoch:642 \t loss: 0.594767\n",
      "Epoch:643 \t loss: 0.296519\n",
      "Epoch:644 \t loss: 0.231499\n",
      "Epoch:645 \t loss: 0.184385\n",
      "Epoch:646 \t loss: 0.426086\n",
      "Epoch:647 \t loss: 0.211847\n",
      "Epoch:648 \t loss: 0.178434\n",
      "Epoch:649 \t loss: 0.431148\n",
      "Epoch:650 \t loss: 0.576694\n",
      "Epoch:651 \t loss: 0.578299\n",
      "Epoch:652 \t loss: 0.397122\n",
      "Epoch:653 \t loss: 0.559767\n",
      "Epoch:654 \t loss: 0.287697\n",
      "Epoch:655 \t loss: 0.154670\n",
      "Epoch:656 \t loss: 0.454333\n",
      "Epoch:657 \t loss: 0.241273\n",
      "Epoch:658 \t loss: 0.377821\n",
      "Epoch:659 \t loss: 0.118521\n",
      "Epoch:660 \t loss: 0.582152\n",
      "Epoch:661 \t loss: 0.547025\n",
      "Epoch:662 \t loss: 0.216324\n",
      "Epoch:663 \t loss: 0.299891\n",
      "Epoch:664 \t loss: 0.447825\n",
      "Epoch:665 \t loss: 0.194097\n",
      "Epoch:666 \t loss: 0.498903\n",
      "Epoch:667 \t loss: 0.447460\n",
      "Epoch:668 \t loss: 0.279183\n",
      "Epoch:669 \t loss: 0.324480\n",
      "Epoch:670 \t loss: 0.146346\n",
      "Epoch:671 \t loss: 0.136318\n",
      "Epoch:672 \t loss: 0.294917\n",
      "Epoch:673 \t loss: 0.507704\n",
      "Epoch:674 \t loss: 0.398442\n",
      "Epoch:675 \t loss: 0.124973\n",
      "Epoch:676 \t loss: 0.383880\n",
      "Epoch:677 \t loss: 0.174870\n",
      "Epoch:678 \t loss: 0.468297\n",
      "Epoch:679 \t loss: 0.085116\n",
      "Epoch:680 \t loss: 0.392130\n",
      "Epoch:681 \t loss: 0.195815\n",
      "Epoch:682 \t loss: 0.543203\n",
      "Epoch:683 \t loss: 0.296671\n",
      "Epoch:684 \t loss: 0.283204\n",
      "Epoch:685 \t loss: 0.473996\n",
      "Epoch:686 \t loss: 0.223822\n",
      "Epoch:687 \t loss: 0.426476\n",
      "Epoch:688 \t loss: 0.416642\n",
      "Epoch:689 \t loss: 0.639082\n",
      "Epoch:690 \t loss: 0.537988\n",
      "Epoch:691 \t loss: 0.557914\n",
      "Epoch:692 \t loss: 0.375684\n",
      "Epoch:693 \t loss: 0.223956\n",
      "Epoch:694 \t loss: 0.185919\n",
      "Epoch:695 \t loss: 0.222900\n",
      "Epoch:696 \t loss: 0.434504\n",
      "Epoch:697 \t loss: 0.163868\n",
      "Epoch:698 \t loss: 0.368906\n",
      "Epoch:699 \t loss: 0.580592\n",
      "Epoch:700 \t loss: 0.514627\n",
      "Epoch:701 \t loss: 0.634688\n",
      "Epoch:702 \t loss: 0.216652\n",
      "Epoch:703 \t loss: 0.267228\n",
      "Epoch:704 \t loss: 0.406289\n",
      "Epoch:705 \t loss: 0.246009\n",
      "Epoch:706 \t loss: 0.665121\n",
      "Epoch:707 \t loss: 0.709471\n",
      "Epoch:708 \t loss: 0.358464\n",
      "Epoch:709 \t loss: 0.424837\n",
      "Epoch:710 \t loss: 0.202767\n",
      "Epoch:711 \t loss: 0.788831\n",
      "Epoch:712 \t loss: 0.219937\n",
      "Epoch:713 \t loss: 0.452102\n",
      "Epoch:714 \t loss: 0.590916\n",
      "Epoch:715 \t loss: 0.407251\n",
      "Epoch:716 \t loss: 0.107446\n",
      "Epoch:717 \t loss: 0.155824\n",
      "Epoch:718 \t loss: 0.314353\n",
      "Epoch:719 \t loss: 0.203882\n",
      "Epoch:720 \t loss: 0.325004\n",
      "Epoch:721 \t loss: 0.091486\n",
      "Epoch:722 \t loss: 0.437936\n",
      "Epoch:723 \t loss: 0.197297\n",
      "Epoch:724 \t loss: 0.349334\n",
      "Epoch:725 \t loss: 0.526185\n",
      "Epoch:726 \t loss: 0.244032\n",
      "Epoch:727 \t loss: 0.154179\n",
      "Epoch:728 \t loss: 0.445689\n",
      "Epoch:729 \t loss: 0.337583\n",
      "Epoch:730 \t loss: 0.255784\n",
      "Epoch:731 \t loss: 0.215831\n",
      "Epoch:732 \t loss: 0.416936\n",
      "Epoch:733 \t loss: 0.210724\n",
      "Epoch:734 \t loss: 0.477050\n",
      "Epoch:735 \t loss: 0.409268\n",
      "Epoch:736 \t loss: 0.283527\n",
      "Epoch:737 \t loss: 0.186194\n",
      "Epoch:738 \t loss: 0.299339\n",
      "Epoch:739 \t loss: 0.297829\n",
      "Epoch:740 \t loss: 0.309076\n",
      "Epoch:741 \t loss: 0.589540\n",
      "Epoch:742 \t loss: 0.393339\n",
      "Epoch:743 \t loss: 0.405909\n",
      "Epoch:744 \t loss: 0.230388\n",
      "Epoch:745 \t loss: 0.528130\n",
      "Epoch:746 \t loss: 0.439869\n",
      "Epoch:747 \t loss: 0.505016\n",
      "Epoch:748 \t loss: 0.398669\n",
      "Epoch:749 \t loss: 0.450340\n",
      "Epoch:750 \t loss: 0.460569\n",
      "Epoch:751 \t loss: 0.508515\n",
      "Epoch:752 \t loss: 0.340832\n",
      "Epoch:753 \t loss: 0.258271\n",
      "Epoch:754 \t loss: 0.443378\n",
      "Epoch:755 \t loss: 0.304802\n",
      "Epoch:756 \t loss: 0.175586\n",
      "Epoch:757 \t loss: 0.314071\n",
      "Epoch:758 \t loss: 0.300696\n",
      "Epoch:759 \t loss: 0.211875\n",
      "Epoch:760 \t loss: 0.224538\n",
      "Epoch:761 \t loss: 0.200896\n",
      "Epoch:762 \t loss: 0.386493\n",
      "Epoch:763 \t loss: 0.235887\n",
      "Epoch:764 \t loss: 0.514107\n",
      "Epoch:765 \t loss: 0.679465\n",
      "Epoch:766 \t loss: 0.364569\n",
      "Epoch:767 \t loss: 0.426865\n",
      "Epoch:768 \t loss: 0.483250\n",
      "Epoch:769 \t loss: 0.333233\n",
      "Epoch:770 \t loss: 0.300476\n",
      "Epoch:771 \t loss: 0.167829\n",
      "Epoch:772 \t loss: 0.563287\n",
      "Epoch:773 \t loss: 0.366386\n",
      "Epoch:774 \t loss: 0.409592\n",
      "Epoch:775 \t loss: 0.209681\n",
      "Epoch:776 \t loss: 0.304956\n",
      "Epoch:777 \t loss: 0.522312\n",
      "Epoch:778 \t loss: 0.202268\n",
      "Epoch:779 \t loss: 0.370482\n",
      "Epoch:780 \t loss: 0.241738\n",
      "Epoch:781 \t loss: 0.384745\n",
      "Epoch:782 \t loss: 0.471402\n",
      "Epoch:783 \t loss: 0.290309\n",
      "Epoch:784 \t loss: 0.582007\n",
      "Epoch:785 \t loss: 0.331379\n",
      "Epoch:786 \t loss: 0.413039\n",
      "Epoch:787 \t loss: 0.688089\n",
      "Epoch:788 \t loss: 0.737926\n",
      "Epoch:789 \t loss: 0.322747\n",
      "Epoch:790 \t loss: 0.208352\n",
      "Epoch:791 \t loss: 0.362110\n",
      "Epoch:792 \t loss: 0.261450\n",
      "Epoch:793 \t loss: 0.347070\n",
      "Epoch:794 \t loss: 0.262964\n",
      "Epoch:795 \t loss: 0.155434\n",
      "Epoch:796 \t loss: 0.272251\n",
      "Epoch:797 \t loss: 0.452508\n",
      "Epoch:798 \t loss: 0.526146\n",
      "Epoch:799 \t loss: 0.327853\n",
      "Epoch:800 \t loss: 0.298988\n",
      "Epoch:801 \t loss: 0.413522\n",
      "Epoch:802 \t loss: 0.181537\n",
      "Epoch:803 \t loss: 0.247789\n",
      "Epoch:804 \t loss: 0.342762\n",
      "Epoch:805 \t loss: 0.284873\n",
      "Epoch:806 \t loss: 0.002162\n",
      "Epoch:807 \t loss: 0.433452\n",
      "Epoch:808 \t loss: 0.360225\n",
      "Epoch:809 \t loss: 0.188049\n",
      "Epoch:810 \t loss: 0.481972\n",
      "Epoch:811 \t loss: 0.298473\n",
      "Epoch:812 \t loss: 0.211224\n",
      "Epoch:813 \t loss: 0.183972\n",
      "Epoch:814 \t loss: 0.570100\n",
      "Epoch:815 \t loss: 0.545893\n",
      "Epoch:816 \t loss: 0.497137\n",
      "Epoch:817 \t loss: 0.612402\n",
      "Epoch:818 \t loss: 0.586104\n",
      "Epoch:819 \t loss: 0.444358\n",
      "Epoch:820 \t loss: 0.257834\n",
      "Epoch:821 \t loss: 0.331976\n",
      "Epoch:822 \t loss: 0.200287\n",
      "Epoch:823 \t loss: 0.131956\n",
      "Epoch:824 \t loss: 0.541202\n",
      "Epoch:825 \t loss: 0.444411\n",
      "Epoch:826 \t loss: 0.239782\n",
      "Epoch:827 \t loss: 0.107240\n",
      "Epoch:828 \t loss: 0.239302\n",
      "Epoch:829 \t loss: 0.314936\n",
      "Epoch:830 \t loss: 0.416086\n",
      "Epoch:831 \t loss: 0.614803\n",
      "Epoch:832 \t loss: 0.565414\n",
      "Epoch:833 \t loss: 0.088832\n",
      "Epoch:834 \t loss: 0.303245\n",
      "Epoch:835 \t loss: 0.347120\n",
      "Epoch:836 \t loss: 0.328134\n",
      "Epoch:837 \t loss: 0.507125\n",
      "Epoch:838 \t loss: 0.527517\n",
      "Epoch:839 \t loss: 0.349343\n",
      "Epoch:840 \t loss: 0.372799\n",
      "Epoch:841 \t loss: 0.542032\n",
      "Epoch:842 \t loss: 0.283698\n",
      "Epoch:843 \t loss: 0.346330\n",
      "Epoch:844 \t loss: 0.232218\n",
      "Epoch:845 \t loss: 0.327738\n",
      "Epoch:846 \t loss: 0.282596\n",
      "Epoch:847 \t loss: 0.395362\n",
      "Epoch:848 \t loss: 0.355753\n",
      "Epoch:849 \t loss: 0.537552\n",
      "Epoch:850 \t loss: 0.252094\n",
      "Epoch:851 \t loss: 0.083464\n",
      "Epoch:852 \t loss: 0.380453\n",
      "Epoch:853 \t loss: 0.275940\n",
      "Epoch:854 \t loss: 0.248246\n",
      "Epoch:855 \t loss: 0.711292\n",
      "Epoch:856 \t loss: 0.460485\n",
      "Epoch:857 \t loss: 0.042796\n",
      "Epoch:858 \t loss: 0.113560\n",
      "Epoch:859 \t loss: 0.158944\n",
      "Epoch:860 \t loss: 0.357974\n",
      "Epoch:861 \t loss: 0.475025\n",
      "Epoch:862 \t loss: 0.386076\n",
      "Epoch:863 \t loss: 0.003927\n",
      "Epoch:864 \t loss: 0.190382\n",
      "Epoch:865 \t loss: 0.199205\n",
      "Epoch:866 \t loss: 0.242155\n",
      "Epoch:867 \t loss: 0.148165\n",
      "Epoch:868 \t loss: 0.177789\n",
      "Epoch:869 \t loss: 0.446301\n",
      "Epoch:870 \t loss: 0.367337\n",
      "Epoch:871 \t loss: 0.307611\n",
      "Epoch:872 \t loss: 0.142998\n",
      "Epoch:873 \t loss: 0.454429\n",
      "Epoch:874 \t loss: 0.249284\n",
      "Epoch:875 \t loss: 0.829599\n",
      "Epoch:876 \t loss: 0.302413\n",
      "Epoch:877 \t loss: 0.447257\n",
      "Epoch:878 \t loss: 0.191018\n",
      "Epoch:879 \t loss: 0.671029\n",
      "Epoch:880 \t loss: 0.086156\n",
      "Epoch:881 \t loss: 0.632784\n",
      "Epoch:882 \t loss: 0.298197\n",
      "Epoch:883 \t loss: 0.478375\n",
      "Epoch:884 \t loss: 0.268717\n",
      "Epoch:885 \t loss: 0.603352\n",
      "Epoch:886 \t loss: 0.281048\n",
      "Epoch:887 \t loss: 0.347162\n",
      "Epoch:888 \t loss: 0.221230\n",
      "Epoch:889 \t loss: 0.667267\n",
      "Epoch:890 \t loss: 0.080643\n",
      "Epoch:891 \t loss: 0.084648\n",
      "Epoch:892 \t loss: 0.203199\n",
      "Epoch:893 \t loss: 0.596688\n",
      "Epoch:894 \t loss: 0.498446\n",
      "Epoch:895 \t loss: 0.108348\n",
      "Epoch:896 \t loss: 0.591532\n",
      "Epoch:897 \t loss: 0.383006\n",
      "Epoch:898 \t loss: 0.454675\n",
      "Epoch:899 \t loss: 0.354543\n",
      "Epoch:900 \t loss: 0.116906\n",
      "Epoch:901 \t loss: 0.393689\n",
      "Epoch:902 \t loss: 0.248570\n",
      "Epoch:903 \t loss: 0.322985\n",
      "Epoch:904 \t loss: 0.384911\n",
      "Epoch:905 \t loss: 0.545351\n",
      "Epoch:906 \t loss: 0.283238\n",
      "Epoch:907 \t loss: 0.257407\n",
      "Epoch:908 \t loss: 0.356779\n",
      "Epoch:909 \t loss: 0.422828\n",
      "Epoch:910 \t loss: 0.305352\n",
      "Epoch:911 \t loss: 0.207133\n",
      "Epoch:912 \t loss: 0.573817\n",
      "Epoch:913 \t loss: 0.040356\n",
      "Epoch:914 \t loss: 0.426014\n",
      "Epoch:915 \t loss: 0.570995\n",
      "Epoch:916 \t loss: 0.245519\n",
      "Epoch:917 \t loss: 0.348500\n",
      "Epoch:918 \t loss: 0.335759\n",
      "Epoch:919 \t loss: 0.471697\n",
      "Epoch:920 \t loss: 0.193280\n",
      "Epoch:921 \t loss: 0.382266\n",
      "Epoch:922 \t loss: 0.140770\n",
      "Epoch:923 \t loss: 0.326638\n",
      "Epoch:924 \t loss: 0.222338\n",
      "Epoch:925 \t loss: 0.038012\n",
      "Epoch:926 \t loss: 0.387496\n",
      "Epoch:927 \t loss: 0.563975\n",
      "Epoch:928 \t loss: 0.209382\n",
      "Epoch:929 \t loss: 0.737955\n",
      "Epoch:930 \t loss: 0.273243\n",
      "Epoch:931 \t loss: 0.069333\n",
      "Epoch:932 \t loss: 0.172307\n",
      "Epoch:933 \t loss: 0.492554\n",
      "Epoch:934 \t loss: 0.297811\n",
      "Epoch:935 \t loss: 0.338749\n",
      "Epoch:936 \t loss: 0.526380\n",
      "Epoch:937 \t loss: 0.325104\n",
      "Epoch:938 \t loss: 0.512035\n",
      "Epoch:939 \t loss: 0.616656\n",
      "Epoch:940 \t loss: 0.230950\n",
      "Epoch:941 \t loss: 0.254862\n",
      "Epoch:942 \t loss: 0.160466\n",
      "Epoch:943 \t loss: 0.424865\n",
      "Epoch:944 \t loss: 0.455406\n",
      "Epoch:945 \t loss: 0.384777\n",
      "Epoch:946 \t loss: 0.498333\n",
      "Epoch:947 \t loss: 0.665776\n",
      "Epoch:948 \t loss: 0.221909\n",
      "Epoch:949 \t loss: 0.444696\n",
      "Epoch:950 \t loss: 0.254345\n",
      "Epoch:951 \t loss: 0.100334\n",
      "Epoch:952 \t loss: 0.460827\n",
      "Epoch:953 \t loss: 0.361079\n",
      "Epoch:954 \t loss: 0.189527\n",
      "Epoch:955 \t loss: 0.136198\n",
      "Epoch:956 \t loss: 0.380906\n",
      "Epoch:957 \t loss: 0.195849\n",
      "Epoch:958 \t loss: 0.192611\n",
      "Epoch:959 \t loss: 0.632114\n",
      "Epoch:960 \t loss: 0.406561\n",
      "Epoch:961 \t loss: 0.413723\n",
      "Epoch:962 \t loss: 0.357017\n",
      "Epoch:963 \t loss: 0.047452\n",
      "Epoch:964 \t loss: 0.044926\n",
      "Epoch:965 \t loss: 0.580338\n",
      "Epoch:966 \t loss: 0.157039\n",
      "Epoch:967 \t loss: 0.371978\n",
      "Epoch:968 \t loss: 0.656105\n",
      "Epoch:969 \t loss: 0.471064\n",
      "Epoch:970 \t loss: 0.065165\n",
      "Epoch:971 \t loss: 0.328051\n",
      "Epoch:972 \t loss: 0.287552\n",
      "Epoch:973 \t loss: 0.469499\n",
      "Epoch:974 \t loss: 0.044532\n",
      "Epoch:975 \t loss: 0.335247\n",
      "Epoch:976 \t loss: 0.173966\n",
      "Epoch:977 \t loss: 0.157466\n",
      "Epoch:978 \t loss: 0.584013\n",
      "Epoch:979 \t loss: 0.350244\n",
      "Epoch:980 \t loss: 0.250425\n",
      "Epoch:981 \t loss: 0.179007\n",
      "Epoch:982 \t loss: 0.643369\n",
      "Epoch:983 \t loss: 0.565859\n",
      "Epoch:984 \t loss: 0.197388\n",
      "Epoch:985 \t loss: 0.456879\n",
      "Epoch:986 \t loss: 0.320606\n",
      "Epoch:987 \t loss: 0.562423\n",
      "Epoch:988 \t loss: 0.393280\n",
      "Epoch:989 \t loss: 0.147805\n",
      "Epoch:990 \t loss: 0.160415\n",
      "Epoch:991 \t loss: 0.500857\n",
      "Epoch:992 \t loss: 0.300747\n",
      "Epoch:993 \t loss: 0.548924\n",
      "Epoch:994 \t loss: 0.229389\n",
      "Epoch:995 \t loss: 0.497162\n",
      "Epoch:996 \t loss: 0.535131\n",
      "Epoch:997 \t loss: 0.687377\n",
      "Epoch:998 \t loss: 0.196428\n",
      "Epoch:999 \t loss: 0.186079\n",
      "Epoch:1000 \t loss: 0.683590\n",
      "Epoch:1001 \t loss: 0.261910\n",
      "Epoch:1002 \t loss: 0.697806\n",
      "Epoch:1003 \t loss: 0.389330\n",
      "Epoch:1004 \t loss: 0.640195\n",
      "Epoch:1005 \t loss: 0.478334\n",
      "Epoch:1006 \t loss: 0.325541\n",
      "Epoch:1007 \t loss: 0.331772\n",
      "Epoch:1008 \t loss: 0.124839\n",
      "Epoch:1009 \t loss: 0.390961\n",
      "Epoch:1010 \t loss: 0.373029\n",
      "Epoch:1011 \t loss: 0.328739\n",
      "Epoch:1012 \t loss: 0.140132\n",
      "Epoch:1013 \t loss: 0.276775\n",
      "Epoch:1014 \t loss: 0.149152\n",
      "Epoch:1015 \t loss: 0.334771\n",
      "Epoch:1016 \t loss: 0.565726\n",
      "Epoch:1017 \t loss: 0.075985\n",
      "Epoch:1018 \t loss: 0.381865\n",
      "Epoch:1019 \t loss: 0.382790\n",
      "Epoch:1020 \t loss: 0.213192\n",
      "Epoch:1021 \t loss: 0.370251\n",
      "Epoch:1022 \t loss: 0.107814\n",
      "Epoch:1023 \t loss: 0.316118\n",
      "Epoch:1024 \t loss: 0.202374\n",
      "Epoch:1025 \t loss: 0.300907\n",
      "Epoch:1026 \t loss: 0.199309\n",
      "Epoch:1027 \t loss: 0.212125\n",
      "Epoch:1028 \t loss: 0.237128\n",
      "Epoch:1029 \t loss: 0.195511\n",
      "Epoch:1030 \t loss: 0.295470\n",
      "Epoch:1031 \t loss: 0.239606\n",
      "Epoch:1032 \t loss: 0.239881\n",
      "Epoch:1033 \t loss: 0.472226\n",
      "Epoch:1034 \t loss: 0.136779\n",
      "Epoch:1035 \t loss: 0.405459\n",
      "Epoch:1036 \t loss: 0.349030\n",
      "Epoch:1037 \t loss: 0.350022\n",
      "Epoch:1038 \t loss: 0.245106\n",
      "Epoch:1039 \t loss: 0.531713\n",
      "Epoch:1040 \t loss: 0.574213\n",
      "Epoch:1041 \t loss: 0.368462\n",
      "Epoch:1042 \t loss: 0.418700\n",
      "Epoch:1043 \t loss: 0.661016\n",
      "Epoch:1044 \t loss: 0.656572\n",
      "Epoch:1045 \t loss: 0.369052\n",
      "Epoch:1046 \t loss: 0.393960\n",
      "Epoch:1047 \t loss: 0.243398\n",
      "Epoch:1048 \t loss: 0.431478\n",
      "Epoch:1049 \t loss: 0.306796\n",
      "Epoch:1050 \t loss: 0.371239\n",
      "Epoch:1051 \t loss: 0.343205\n",
      "Epoch:1052 \t loss: 0.662814\n",
      "Epoch:1053 \t loss: 0.546795\n",
      "Epoch:1054 \t loss: 0.317202\n",
      "Epoch:1055 \t loss: 0.343096\n",
      "Epoch:1056 \t loss: 0.231878\n",
      "Epoch:1057 \t loss: 0.282644\n",
      "Epoch:1058 \t loss: 0.303492\n",
      "Epoch:1059 \t loss: 0.241884\n",
      "Epoch:1060 \t loss: 0.258181\n",
      "Epoch:1061 \t loss: 0.152296\n",
      "Epoch:1062 \t loss: 0.105764\n",
      "Epoch:1063 \t loss: 0.391375\n",
      "Epoch:1064 \t loss: 0.383497\n",
      "Epoch:1065 \t loss: 0.476624\n",
      "Epoch:1066 \t loss: 0.236586\n",
      "Epoch:1067 \t loss: 0.291450\n",
      "Epoch:1068 \t loss: 0.273936\n",
      "Epoch:1069 \t loss: 0.389260\n",
      "Epoch:1070 \t loss: 0.132172\n",
      "Epoch:1071 \t loss: 0.316966\n",
      "Epoch:1072 \t loss: 0.310571\n",
      "Epoch:1073 \t loss: 0.440685\n",
      "Epoch:1074 \t loss: 0.353843\n",
      "Epoch:1075 \t loss: 0.326729\n",
      "Epoch:1076 \t loss: 0.135598\n",
      "Epoch:1077 \t loss: 0.718088\n",
      "Epoch:1078 \t loss: 0.238439\n",
      "Epoch:1079 \t loss: 0.105367\n",
      "Epoch:1080 \t loss: 0.482868\n",
      "Epoch:1081 \t loss: 0.334812\n",
      "Epoch:1082 \t loss: 0.330770\n",
      "Epoch:1083 \t loss: 0.217153\n",
      "Epoch:1084 \t loss: 0.454904\n",
      "Epoch:1085 \t loss: 0.244210\n",
      "Epoch:1086 \t loss: 0.329855\n",
      "Epoch:1087 \t loss: 0.599441\n",
      "Epoch:1088 \t loss: 0.225552\n",
      "Epoch:1089 \t loss: 0.321525\n",
      "Epoch:1090 \t loss: 0.235168\n",
      "Epoch:1091 \t loss: 0.303007\n",
      "Epoch:1092 \t loss: 0.421009\n",
      "Epoch:1093 \t loss: 0.197926\n",
      "Epoch:1094 \t loss: 0.354188\n",
      "Epoch:1095 \t loss: 0.422411\n",
      "Epoch:1096 \t loss: 0.447505\n",
      "Epoch:1097 \t loss: 0.329666\n",
      "Epoch:1098 \t loss: 0.248298\n",
      "Epoch:1099 \t loss: 0.221001\n",
      "Epoch:1100 \t loss: 0.397611\n",
      "Epoch:1101 \t loss: 0.190399\n",
      "Epoch:1102 \t loss: 0.196798\n",
      "Epoch:1103 \t loss: 0.526022\n",
      "Epoch:1104 \t loss: 0.411466\n",
      "Epoch:1105 \t loss: 0.524565\n",
      "Epoch:1106 \t loss: 0.287048\n",
      "Epoch:1107 \t loss: 0.732113\n",
      "Epoch:1108 \t loss: 0.452804\n",
      "Epoch:1109 \t loss: 0.491786\n",
      "Epoch:1110 \t loss: 0.677987\n",
      "Epoch:1111 \t loss: 0.421448\n",
      "Epoch:1112 \t loss: 0.401696\n",
      "Epoch:1113 \t loss: 0.208300\n",
      "Epoch:1114 \t loss: 0.501749\n",
      "Epoch:1115 \t loss: 0.436896\n",
      "Epoch:1116 \t loss: 0.539664\n",
      "Epoch:1117 \t loss: 0.329922\n",
      "Epoch:1118 \t loss: 0.283162\n",
      "Epoch:1119 \t loss: 0.237778\n",
      "Epoch:1120 \t loss: 0.563733\n",
      "Epoch:1121 \t loss: 0.396225\n",
      "Epoch:1122 \t loss: 0.402734\n",
      "Epoch:1123 \t loss: 0.476142\n",
      "Epoch:1124 \t loss: 0.377213\n",
      "Epoch:1125 \t loss: 0.630948\n",
      "Epoch:1126 \t loss: 0.330787\n",
      "Epoch:1127 \t loss: 0.435883\n",
      "Epoch:1128 \t loss: 0.350391\n",
      "Epoch:1129 \t loss: 0.142109\n",
      "Epoch:1130 \t loss: 0.136623\n",
      "Epoch:1131 \t loss: 0.083918\n",
      "Epoch:1132 \t loss: 0.531032\n",
      "Epoch:1133 \t loss: 0.286082\n",
      "Epoch:1134 \t loss: 0.290974\n",
      "Epoch:1135 \t loss: 0.293674\n",
      "Epoch:1136 \t loss: 0.420997\n",
      "Epoch:1137 \t loss: 0.551926\n",
      "Epoch:1138 \t loss: 0.508566\n",
      "Epoch:1139 \t loss: 0.532676\n",
      "Epoch:1140 \t loss: 0.447919\n",
      "Epoch:1141 \t loss: 0.173588\n",
      "Epoch:1142 \t loss: 0.456585\n",
      "Epoch:1143 \t loss: 0.359040\n",
      "Epoch:1144 \t loss: 0.477031\n",
      "Epoch:1145 \t loss: 0.553499\n",
      "Epoch:1146 \t loss: 0.072828\n",
      "Epoch:1147 \t loss: 0.128216\n",
      "Epoch:1148 \t loss: 0.563718\n",
      "Epoch:1149 \t loss: 0.397264\n",
      "Epoch:1150 \t loss: 0.103108\n",
      "Epoch:1151 \t loss: 0.615863\n",
      "Epoch:1152 \t loss: 0.432110\n",
      "Epoch:1153 \t loss: 0.324424\n",
      "Epoch:1154 \t loss: 0.465472\n",
      "Epoch:1155 \t loss: 0.429906\n",
      "Epoch:1156 \t loss: 0.457174\n",
      "Epoch:1157 \t loss: 0.225873\n",
      "Epoch:1158 \t loss: 0.115581\n",
      "Epoch:1159 \t loss: 0.216128\n",
      "Epoch:1160 \t loss: 0.381627\n",
      "Epoch:1161 \t loss: 0.434807\n",
      "Epoch:1162 \t loss: 0.375551\n",
      "Epoch:1163 \t loss: 0.353806\n",
      "Epoch:1164 \t loss: 0.426314\n",
      "Epoch:1165 \t loss: 0.721242\n",
      "Epoch:1166 \t loss: 0.531929\n",
      "Epoch:1167 \t loss: 0.514408\n",
      "Epoch:1168 \t loss: 0.418651\n",
      "Epoch:1169 \t loss: 0.163524\n",
      "Epoch:1170 \t loss: 0.339238\n",
      "Epoch:1171 \t loss: 0.346283\n",
      "Epoch:1172 \t loss: 0.475701\n",
      "Epoch:1173 \t loss: 0.354417\n",
      "Epoch:1174 \t loss: 0.530229\n",
      "Epoch:1175 \t loss: 0.535564\n",
      "Epoch:1176 \t loss: 0.335008\n",
      "Epoch:1177 \t loss: 0.233031\n",
      "Epoch:1178 \t loss: 0.102989\n",
      "Epoch:1179 \t loss: 0.345570\n",
      "Epoch:1180 \t loss: 0.495890\n",
      "Epoch:1181 \t loss: 0.096714\n",
      "Epoch:1182 \t loss: 0.273174\n",
      "Epoch:1183 \t loss: 0.250359\n",
      "Epoch:1184 \t loss: 0.607968\n",
      "Epoch:1185 \t loss: 0.369212\n",
      "Epoch:1186 \t loss: 0.403581\n",
      "Epoch:1187 \t loss: 0.220364\n",
      "Epoch:1188 \t loss: 0.096257\n",
      "Epoch:1189 \t loss: 0.371531\n",
      "Epoch:1190 \t loss: 0.266581\n",
      "Epoch:1191 \t loss: 0.202605\n",
      "Epoch:1192 \t loss: 0.343721\n",
      "Epoch:1193 \t loss: 0.239738\n",
      "Epoch:1194 \t loss: 0.157157\n",
      "Epoch:1195 \t loss: 0.335982\n",
      "Epoch:1196 \t loss: 0.313826\n",
      "Epoch:1197 \t loss: 0.303006\n",
      "Epoch:1198 \t loss: 0.318141\n",
      "Epoch:1199 \t loss: 0.404166\n",
      "Epoch:1200 \t loss: 0.209485\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = [], []\n",
    "\n",
    "if DO_NSP_TEST :\n",
    "    train_samples, test_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "else :\n",
    "    train_samples = all_samples\n",
    "\n",
    "batch_data = make_data(intent_token_train, train_samples, attribute2idx, n_data=len(all_samples))\n",
    "\n",
    "batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)]\n",
    "dataset = BERTDataset(*batch_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "model = BERT(n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print('Entering training process...')\n",
    "for epoch in range(epochs):\n",
    "    for one_batch in dataloader:\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch]\n",
    "\n",
    "        logits_cls, logits_lm, _ = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        loss_cls = criterion(logits_cls, is_next)\n",
    "        loss_lm = criterion(logits_lm.view(-1, max_vocab), masked_tokens.view(-1))\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n",
    "    \n",
    "    # 30epoch\n",
    "    if epoch % 50 == 0 :\n",
    "        torch.save(model.state_dict(), 'attribute_pretrained.dat')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f732c-cf95-47c4-bed2-3d143fd17037",
   "metadata": {},
   "source": [
    "# Saving the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3072fc2-4142-48f4-a340-c75bca49b15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'attribute_pretrained.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
