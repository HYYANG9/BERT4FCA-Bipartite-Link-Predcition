{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the maximum number of masked tokens\n",
    "max_pred = 4\n",
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 64\n",
    "# dimension of embedding\n",
    "d_model = 768  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 12\n",
    "# number of encoders\n",
    "n_layers = 6\n",
    "# the number of input setences\n",
    "n_segs = 2\n",
    "\n",
    "p_dropout = .1\n",
    "\n",
    "#80% the chosen token is replaced by [mask], 10% is replaced by a random token, 10% do nothing\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in word2idx\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    # The size of pad_mask is [batch, seq_len, seq_len]\n",
    "    # The resulting tensor has True where padding tokens are located and False elsewhere.\n",
    "    \n",
    "    # print(f'the shape of pad_mask is {pad_mask.shape}')\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,max_vocab, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        # print(\"Input to Embeddings.forward - x:\", x.size())\n",
    "        word_enc = self.word_emb(x)\n",
    "        # print(\"Output from Embeddings.forward - word_enc:\", word_enc.size())\n",
    "        '''\n",
    "        maybe positional embedding can be deleted\n",
    "        '''\n",
    "        \n",
    "        # positional embedding\n",
    "        # pos = torch.arange(x.shape[1], dtype=torch.long, device=device) # .long: round down\n",
    "        # pos = pos.unsqueeze(0).expand_as(x) # the shape is [1, seq_len]\n",
    "        # pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# pre-LN is easier to train than post-LN, but if fullly training, post_LN have better result than pre-LN. \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
    "\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The full connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, max_vocab, max_len):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embeddings(max_vocab, max_len)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "        \n",
    "        # next sentence prediction. output is 0 or 1.\n",
    "        self.next_cls = nn.Linear(d_model, 2)\n",
    "        self.gelu = gelu\n",
    "        \n",
    "        # Sharing weight between some fully connect layer, this will make training easier.\n",
    "        shared_weight = self.pooler.fc.weight\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.fc.weight = shared_weight\n",
    "\n",
    "        shared_weight = self.embedding.word_emb.weight\n",
    "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
    "        self.word_classifier.weight = shared_weight\n",
    "\n",
    "    def forward(self, tokens, segments, masked_pos):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "\n",
    "        # NSP Task\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        logits_cls = self.next_cls(hidden_pool)\n",
    "\n",
    "        # Masked Language Model Task\n",
    "        '''\n",
    "        extracting representations of masked positions, \n",
    "        passing them through a fully connected layer, \n",
    "        applying the GELU activation function, \n",
    "        and making predictions using the word classifier\n",
    "        '''\n",
    "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
    "\n",
    "        # h_masked: [batch, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
    "        h_masked = self.gelu(self.fc(h_masked))\n",
    "        logits_lm = self.word_classifier(h_masked)\n",
    "        # logits_lm: [batch, max_pred, max_vocab]\n",
    "        # logits_cls: [batch, 2]\n",
    "\n",
    "        return logits_cls, logits_lm, hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9177c2-dcb1-4500-bf67-e0f7ee0d286a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14012f5d-fc34-42ba-a496-1bd61287973d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " '''\n",
    "Extract all extents, modify the form of extents as \"o1,o2,...\" named as modified_extents\n",
    "Change objects to indices in extents, named as extent_token_list. It is a list of INDICES not objects!\n",
    "Indices of objects and special tokens are from 1 to 338\n",
    "'[PAD]': 12966, '[CLS]': 12967, '[SEP]': 12968, '[MASK]': 12969\n",
    "'''\n",
    "def process_train_extents_from_file(filename, max_vocab) :\n",
    "    extents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) >= 2:\n",
    "                extent = parts[1].strip()\n",
    "                extents.append(extent)\n",
    "    # print(\"The number of concepts is\",len(extents))\n",
    "    object_list = list(set(\" \".join(extents).split()))\n",
    "    sorted_object_list = sorted(map(int, object_list))\n",
    "    # print(\"The number of objects is \",len(sorted_object_list))\n",
    "    \n",
    "    # Create the object2idx dictionary\n",
    "    object2idx = {'o' + str(obj): int(obj)  for  obj in sorted_object_list}\n",
    "    sorted_object_list = list(map(str, sorted_object_list ))\n",
    "    # print(sorted_object_list)\n",
    "    special_tokens = {'[PAD]': max_vocab-4, '[CLS]': max_vocab-3, '[SEP]': max_vocab-2, '[MASK]': max_vocab-1 }\n",
    "\n",
    "    object2idx.update(special_tokens)\n",
    "    # print(object2idx) \n",
    "\n",
    "    idx2object = {idx: object for object, idx in object2idx.items()}\n",
    "    vocab_size = len(object2idx)\n",
    "    assert len(object2idx) == len(idx2object)\n",
    "    \n",
    "    modified_extents = [' '.join(['o' + token for token in item.split()]) for item in extents]\n",
    "\n",
    "    # print(len(modified_extents))\n",
    "    \n",
    "    extent_token_list = []\n",
    "    for extent in modified_extents:\n",
    "        extent_token_list.append([\n",
    "            object2idx[s] for s in extent.split()\n",
    "        ])\n",
    "    # print(len(extent_token_list))\n",
    "    return extent_token_list, object2idx, modified_extents, sorted_object_list\n",
    "\n",
    "extent_token_train, object2idx , modified_extents_train, train_object_list  = process_train_extents_from_file('BMS-POS-with-missing-part-renumbered_concepts.txt',  473)\n",
    "# print(object2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f03f47b-2af5-4b2f-90fa-eb9939803580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# padding the token lists to have the same length.\n",
    "def padding(ids, n_pads, pad_symb=0):\n",
    "    return ids.extend([pad_symb for _ in range(n_pads)])\n",
    "\n",
    "def masking_procedure(cand_pos, input_ids, max_vocab, masked_symb='[MASK]'):\n",
    "    masked_pos = []\n",
    "    masked_tokens = []\n",
    "    for pos in cand_pos:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random.random() < p_mask:\n",
    "            input_ids[pos] = masked_symb\n",
    "        elif random.random() > (p_mask + p_replace):\n",
    "            rand_word_idx = random.randint(0, max_vocab-4)\n",
    "            input_ids[pos] = rand_word_idx\n",
    "\n",
    "    return masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a28666-9619-4be9-b41d-287b585b9d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9149\n"
     ]
    }
   ],
   "source": [
    "def get_neighbor_samples(extents) :\n",
    "    n = len(extents)\n",
    "    samples = []\n",
    "\n",
    "    dep = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "    neighbor = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "\n",
    "    for i in range(n) :\n",
    "        for j in range(i + 1, n) :\n",
    "            if set(extents[i]).issubset(set(extents[j])) :\n",
    "                dep[i][j] = 1\n",
    "            if set(extents[j]).issubset(set(extents[i])) :\n",
    "                dep[j][i] = 1\n",
    "\n",
    "    for i in range(n) :\n",
    "        se = set([])\n",
    "        for j in range(n) :\n",
    "            if j != i :\n",
    "                if dep[j][i] == 1 :\n",
    "                    rep = False\n",
    "                    lst = list(se)\n",
    "                    for idk, k in enumerate(lst) :\n",
    "                        if dep[k][j] :\n",
    "                            se.remove(k)\n",
    "                            se.add(j)\n",
    "                            rep = True\n",
    "                        if dep[j][k] :\n",
    "                            rep = True\n",
    "                    if not rep :\n",
    "                        se.add(j)\n",
    "\n",
    "        for j in range(n) :\n",
    "            if j in se :\n",
    "                samples.append([i, j, True])\n",
    "            elif random.random() < 0.0018 :\n",
    "                samples.append([i, j, False])\n",
    "        \n",
    "    return samples\n",
    "\n",
    "all_samples = get_neighbor_samples(extent_token_train)\n",
    "print(len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5afe0534-d54a-4aa2-9aaa-384813468cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A list of sentences and the desired number of data samples as input.\n",
    "def make_data(extents, all_samples, word2idx, max_vocab , num_per_sample = 120):\n",
    "    batch_data = []\n",
    "    max_len = 0\n",
    "    # len_sentences = len(extents)\n",
    "    for extent in extents :\n",
    "        max_len = max(max_len, len(extent))\n",
    "    max_len = max_len * 2 + 3\n",
    "    print(max_len)\n",
    "    for sample in all_samples :\n",
    "        \n",
    "        tokens_a_idx = sample[0]\n",
    "        tokens_b_idx = sample[1]\n",
    "        tokens_a = extent_token_train[tokens_a_idx]\n",
    "        tokens_b = extent_token_train[tokens_b_idx]\n",
    "             \n",
    "\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0 for i in range(\n",
    "            1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))]\n",
    "\n",
    "        # Determines the number of positions to mask (n_pred) based on the input sequence length.\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * .15)))\n",
    "        cand_pos = [i for i, token in enumerate(input_ids)\n",
    "                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] #exclude special tokens.\n",
    "\n",
    "        # shuffle all candidate position index, to sampling maksed position from first n_pred\n",
    "        masked_pos, masked_tokens = masking_procedure(\n",
    "            cand_pos[:n_pred], input_ids, max_vocab, word2idx['[MASK]'])\n",
    "\n",
    "        # zero padding for tokens to ensure that the input sequences and segment IDs have the maximum sequence length\n",
    "        padding(input_ids, max_len - len(input_ids))\n",
    "        # print(\"the size of input_ids is \" ,len(input_ids))\n",
    "        padding(segment_ids, max_len - len(segment_ids))\n",
    "        # print(\"the size of segment_ids is \" ,len(segment_ids))\n",
    "\n",
    "        # zero padding for mask\n",
    "        if max_pred > n_pred:\n",
    "            n_pads = max_pred - n_pred\n",
    "            padding(masked_pos, n_pads)\n",
    "            padding(masked_tokens, n_pads)\n",
    "\n",
    "        # Creating Batch Data:\n",
    "        batch_data.append(\n",
    "            [input_ids, segment_ids, masked_tokens, masked_pos, sample[2]])\n",
    "\n",
    "    random.shuffle(batch_data)\n",
    "    print(len(batch_data))\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.is_next = is_next\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.is_next[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b5fd-3776-4641-baa3-d82a8c496007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-Train BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b7abc56-ac8f-449d-9709-e60d720dace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NSP_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3a030f9-caff-4a5e-8763-9a061fbabe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 22 \n",
    "lr = 1.9e-5\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40521dbb-258b-4a9e-a94f-859e4a6836a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "9149\n",
      "Epoch:1 \t loss: 2.121423\n",
      "Epoch:2 \t loss: 1.735193\n",
      "Epoch:3 \t loss: 1.542264\n",
      "Epoch:4 \t loss: 1.124309\n",
      "Epoch:5 \t loss: 1.617097\n",
      "Epoch:6 \t loss: 1.229761\n",
      "Epoch:7 \t loss: 1.284080\n",
      "Epoch:8 \t loss: 1.397664\n",
      "Epoch:9 \t loss: 1.256024\n",
      "Epoch:10 \t loss: 1.042464\n",
      "Epoch:11 \t loss: 1.094801\n",
      "Epoch:12 \t loss: 1.169070\n",
      "Epoch:13 \t loss: 0.878116\n",
      "Epoch:14 \t loss: 0.843291\n",
      "Epoch:15 \t loss: 0.867151\n",
      "Epoch:16 \t loss: 0.909081\n",
      "Epoch:17 \t loss: 0.613068\n",
      "Epoch:18 \t loss: 0.807563\n",
      "Epoch:19 \t loss: 0.837455\n",
      "Epoch:20 \t loss: 1.082645\n",
      "Epoch:21 \t loss: 0.725282\n",
      "Epoch:22 \t loss: 0.676637\n",
      "Epoch:23 \t loss: 0.611411\n",
      "Epoch:24 \t loss: 0.574755\n",
      "Epoch:25 \t loss: 0.733791\n",
      "Epoch:26 \t loss: 0.681018\n",
      "Epoch:27 \t loss: 0.622307\n",
      "Epoch:28 \t loss: 0.590849\n",
      "Epoch:29 \t loss: 0.464857\n",
      "Epoch:30 \t loss: 0.577869\n",
      "Epoch:31 \t loss: 0.708081\n",
      "Epoch:32 \t loss: 0.535326\n",
      "Epoch:33 \t loss: 0.629490\n",
      "Epoch:34 \t loss: 0.459576\n",
      "Epoch:35 \t loss: 0.624190\n",
      "Epoch:36 \t loss: 0.464660\n",
      "Epoch:37 \t loss: 0.472054\n",
      "Epoch:38 \t loss: 0.449040\n",
      "Epoch:39 \t loss: 0.769290\n",
      "Epoch:40 \t loss: 0.447319\n",
      "Epoch:41 \t loss: 0.528960\n",
      "Epoch:42 \t loss: 0.266551\n",
      "Epoch:43 \t loss: 0.388742\n",
      "Epoch:44 \t loss: 0.406415\n",
      "Epoch:45 \t loss: 0.477018\n",
      "Epoch:46 \t loss: 0.409790\n",
      "Epoch:47 \t loss: 0.418064\n",
      "Epoch:48 \t loss: 0.407372\n",
      "Epoch:49 \t loss: 0.322646\n",
      "Epoch:50 \t loss: 0.337631\n",
      "Epoch:51 \t loss: 0.373778\n",
      "Epoch:52 \t loss: 0.344640\n",
      "Epoch:53 \t loss: 0.391436\n",
      "Epoch:54 \t loss: 0.390908\n",
      "Epoch:55 \t loss: 0.240225\n",
      "Epoch:56 \t loss: 0.391565\n",
      "Epoch:57 \t loss: 0.315612\n",
      "Epoch:58 \t loss: 0.321166\n",
      "Epoch:59 \t loss: 0.149814\n",
      "Epoch:60 \t loss: 0.331603\n",
      "Epoch:61 \t loss: 0.244434\n",
      "Epoch:62 \t loss: 0.357794\n",
      "Epoch:63 \t loss: 0.328759\n",
      "Epoch:64 \t loss: 0.265593\n",
      "Epoch:65 \t loss: 0.266040\n",
      "Epoch:66 \t loss: 0.289038\n",
      "Epoch:67 \t loss: 0.351569\n",
      "Epoch:68 \t loss: 0.247415\n",
      "Epoch:69 \t loss: 0.253736\n",
      "Epoch:70 \t loss: 0.335030\n",
      "Epoch:71 \t loss: 0.246043\n",
      "Epoch:72 \t loss: 0.169832\n",
      "Epoch:73 \t loss: 0.243814\n",
      "Epoch:74 \t loss: 0.192251\n",
      "Epoch:75 \t loss: 0.407565\n",
      "Epoch:76 \t loss: 0.316225\n",
      "Epoch:77 \t loss: 0.157701\n",
      "Epoch:78 \t loss: 0.156126\n",
      "Epoch:79 \t loss: 0.155951\n",
      "Epoch:80 \t loss: 0.196709\n",
      "Epoch:81 \t loss: 0.197860\n",
      "Epoch:82 \t loss: 0.301834\n",
      "Epoch:83 \t loss: 0.236154\n",
      "Epoch:84 \t loss: 0.229846\n",
      "Epoch:85 \t loss: 0.180334\n",
      "Epoch:86 \t loss: 0.274409\n",
      "Epoch:87 \t loss: 0.126998\n",
      "Epoch:88 \t loss: 0.301418\n",
      "Epoch:89 \t loss: 0.161793\n",
      "Epoch:90 \t loss: 0.107114\n",
      "Epoch:91 \t loss: 0.207661\n",
      "Epoch:92 \t loss: 0.231271\n",
      "Epoch:93 \t loss: 0.216376\n",
      "Epoch:94 \t loss: 0.066891\n",
      "Epoch:95 \t loss: 0.188143\n",
      "Epoch:96 \t loss: 0.143282\n",
      "Epoch:97 \t loss: 0.192366\n",
      "Epoch:98 \t loss: 0.226286\n",
      "Epoch:99 \t loss: 0.169999\n",
      "Epoch:100 \t loss: 0.281668\n",
      "Epoch:101 \t loss: 0.185530\n",
      "Epoch:102 \t loss: 0.108748\n",
      "Epoch:103 \t loss: 0.091669\n",
      "Epoch:104 \t loss: 0.160782\n",
      "Epoch:105 \t loss: 0.184160\n",
      "Epoch:106 \t loss: 0.218474\n",
      "Epoch:107 \t loss: 0.184866\n",
      "Epoch:108 \t loss: 0.280358\n",
      "Epoch:109 \t loss: 0.092710\n",
      "Epoch:110 \t loss: 0.097399\n",
      "Epoch:111 \t loss: 0.058883\n",
      "Epoch:112 \t loss: 0.106978\n",
      "Epoch:113 \t loss: 0.122916\n",
      "Epoch:114 \t loss: 0.017199\n",
      "Epoch:115 \t loss: 0.137517\n",
      "Epoch:116 \t loss: 0.139676\n",
      "Epoch:117 \t loss: 0.081294\n",
      "Epoch:118 \t loss: 0.159425\n",
      "Epoch:119 \t loss: 0.105799\n",
      "Epoch:120 \t loss: 0.211712\n",
      "Epoch:121 \t loss: 0.159207\n",
      "Epoch:122 \t loss: 0.077866\n",
      "Epoch:123 \t loss: 0.055434\n",
      "Epoch:124 \t loss: 0.136209\n",
      "Epoch:125 \t loss: 0.127754\n",
      "Epoch:126 \t loss: 0.017947\n",
      "Epoch:127 \t loss: 0.060807\n",
      "Epoch:128 \t loss: 0.018794\n",
      "Epoch:129 \t loss: 0.111172\n",
      "Epoch:130 \t loss: 0.117005\n",
      "Epoch:131 \t loss: 0.186377\n",
      "Epoch:132 \t loss: 0.156707\n",
      "Epoch:133 \t loss: 0.116583\n",
      "Epoch:134 \t loss: 0.071745\n",
      "Epoch:135 \t loss: 0.121315\n",
      "Epoch:136 \t loss: 0.154343\n",
      "Epoch:137 \t loss: 0.088344\n",
      "Epoch:138 \t loss: 0.146686\n",
      "Epoch:139 \t loss: 0.140520\n",
      "Epoch:140 \t loss: 0.083060\n",
      "Epoch:141 \t loss: 0.164717\n",
      "Epoch:142 \t loss: 0.166760\n",
      "Epoch:143 \t loss: 0.170272\n",
      "Epoch:144 \t loss: 0.048535\n",
      "Epoch:145 \t loss: 0.076748\n",
      "Epoch:146 \t loss: 0.149681\n",
      "Epoch:147 \t loss: 0.052349\n",
      "Epoch:148 \t loss: 0.034761\n",
      "Epoch:149 \t loss: 0.055114\n",
      "Epoch:150 \t loss: 0.177132\n",
      "Epoch:151 \t loss: 0.126574\n",
      "Epoch:152 \t loss: 0.106540\n",
      "Epoch:153 \t loss: 0.112492\n",
      "Epoch:154 \t loss: 0.043342\n",
      "Epoch:155 \t loss: 0.071037\n",
      "Epoch:156 \t loss: 0.128250\n",
      "Epoch:157 \t loss: 0.100660\n",
      "Epoch:158 \t loss: 0.041807\n",
      "Epoch:159 \t loss: 0.071714\n",
      "Epoch:160 \t loss: 0.076785\n",
      "Epoch:161 \t loss: 0.042566\n",
      "Epoch:162 \t loss: 0.015012\n",
      "Epoch:163 \t loss: 0.137734\n",
      "Epoch:164 \t loss: 0.051744\n",
      "Epoch:165 \t loss: 0.081974\n",
      "Epoch:166 \t loss: 0.098286\n",
      "Epoch:167 \t loss: 0.056072\n",
      "Epoch:168 \t loss: 0.101400\n",
      "Epoch:169 \t loss: 0.038705\n",
      "Epoch:170 \t loss: 0.173179\n",
      "Epoch:171 \t loss: 0.184602\n",
      "Epoch:172 \t loss: 0.103871\n",
      "Epoch:173 \t loss: 0.088667\n",
      "Epoch:174 \t loss: 0.346505\n",
      "Epoch:175 \t loss: 0.188221\n",
      "Epoch:176 \t loss: 0.044517\n",
      "Epoch:177 \t loss: 0.107488\n",
      "Epoch:178 \t loss: 0.046521\n",
      "Epoch:179 \t loss: 0.043260\n",
      "Epoch:180 \t loss: 0.101176\n",
      "Epoch:181 \t loss: 0.036627\n",
      "Epoch:182 \t loss: 0.011708\n",
      "Epoch:183 \t loss: 0.046185\n",
      "Epoch:184 \t loss: 0.094793\n",
      "Epoch:185 \t loss: 0.062319\n",
      "Epoch:186 \t loss: 0.098288\n",
      "Epoch:187 \t loss: 0.026927\n",
      "Epoch:188 \t loss: 0.086962\n",
      "Epoch:189 \t loss: 0.130441\n",
      "Epoch:190 \t loss: 0.097670\n",
      "Epoch:191 \t loss: 0.126557\n",
      "Epoch:192 \t loss: 0.088353\n",
      "Epoch:193 \t loss: 0.037757\n",
      "Epoch:194 \t loss: 0.243487\n",
      "Epoch:195 \t loss: 0.055169\n",
      "Epoch:196 \t loss: 0.064061\n",
      "Epoch:197 \t loss: 0.070956\n",
      "Epoch:198 \t loss: 0.138130\n",
      "Epoch:199 \t loss: 0.058799\n",
      "Epoch:200 \t loss: 0.022363\n",
      "Epoch:201 \t loss: 0.107264\n",
      "Epoch:202 \t loss: 0.151181\n",
      "Epoch:203 \t loss: 0.066201\n",
      "Epoch:204 \t loss: 0.034385\n",
      "Epoch:205 \t loss: 0.094513\n",
      "Epoch:206 \t loss: 0.002695\n",
      "Epoch:207 \t loss: 0.028573\n",
      "Epoch:208 \t loss: 0.066317\n",
      "Epoch:209 \t loss: 0.059275\n",
      "Epoch:210 \t loss: 0.108155\n",
      "Epoch:211 \t loss: 0.140810\n",
      "Epoch:212 \t loss: 0.078129\n",
      "Epoch:213 \t loss: 0.115711\n",
      "Epoch:214 \t loss: 0.188321\n",
      "Epoch:215 \t loss: 0.094197\n",
      "Epoch:216 \t loss: 0.055144\n",
      "Epoch:217 \t loss: 0.053173\n",
      "Epoch:218 \t loss: 0.019142\n",
      "Epoch:219 \t loss: 0.042091\n",
      "Epoch:220 \t loss: 0.062033\n",
      "Epoch:221 \t loss: 0.141425\n",
      "Epoch:222 \t loss: 0.165621\n",
      "Epoch:223 \t loss: 0.010895\n",
      "Epoch:224 \t loss: 0.085628\n",
      "Epoch:225 \t loss: 0.057380\n",
      "Epoch:226 \t loss: 0.130283\n",
      "Epoch:227 \t loss: 0.017128\n",
      "Epoch:228 \t loss: 0.099277\n",
      "Epoch:229 \t loss: 0.039373\n",
      "Epoch:230 \t loss: 0.032928\n",
      "Epoch:231 \t loss: 0.110829\n",
      "Epoch:232 \t loss: 0.029037\n",
      "Epoch:233 \t loss: 0.083690\n",
      "Epoch:234 \t loss: 0.125428\n",
      "Epoch:235 \t loss: 0.110975\n",
      "Epoch:236 \t loss: 0.103295\n",
      "Epoch:237 \t loss: 0.096872\n",
      "Epoch:238 \t loss: 0.050975\n",
      "Epoch:239 \t loss: 0.061145\n",
      "Epoch:240 \t loss: 0.130449\n",
      "Epoch:241 \t loss: 0.037247\n",
      "Epoch:242 \t loss: 0.061933\n",
      "Epoch:243 \t loss: 0.045232\n",
      "Epoch:244 \t loss: 0.099463\n",
      "Epoch:245 \t loss: 0.067315\n",
      "Epoch:246 \t loss: 0.103409\n",
      "Epoch:247 \t loss: 0.057808\n",
      "Epoch:248 \t loss: 0.051219\n",
      "Epoch:249 \t loss: 0.090484\n",
      "Epoch:250 \t loss: 0.063760\n",
      "Epoch:251 \t loss: 0.116475\n",
      "Epoch:252 \t loss: 0.185348\n",
      "Epoch:253 \t loss: 0.028758\n",
      "Epoch:254 \t loss: 0.040221\n",
      "Epoch:255 \t loss: 0.053620\n",
      "Epoch:256 \t loss: 0.122355\n",
      "Epoch:257 \t loss: 0.171819\n",
      "Epoch:258 \t loss: 0.078485\n",
      "Epoch:259 \t loss: 0.051158\n",
      "Epoch:260 \t loss: 0.049962\n",
      "Epoch:261 \t loss: 0.073540\n",
      "Epoch:262 \t loss: 0.134020\n",
      "Epoch:263 \t loss: 0.022116\n",
      "Epoch:264 \t loss: 0.119799\n",
      "Epoch:265 \t loss: 0.095836\n",
      "Epoch:266 \t loss: 0.044829\n",
      "Epoch:267 \t loss: 0.098142\n",
      "Epoch:268 \t loss: 0.125874\n",
      "Epoch:269 \t loss: 0.043083\n",
      "Epoch:270 \t loss: 0.078039\n",
      "Epoch:271 \t loss: 0.096527\n",
      "Epoch:272 \t loss: 0.101117\n",
      "Epoch:273 \t loss: 0.020728\n",
      "Epoch:274 \t loss: 0.055341\n",
      "Epoch:275 \t loss: 0.187665\n",
      "Epoch:276 \t loss: 0.092315\n",
      "Epoch:277 \t loss: 0.072200\n",
      "Epoch:278 \t loss: 0.013672\n",
      "Epoch:279 \t loss: 0.079457\n",
      "Epoch:280 \t loss: 0.003416\n",
      "Epoch:281 \t loss: 0.116204\n",
      "Epoch:282 \t loss: 0.040638\n",
      "Epoch:283 \t loss: 0.126301\n",
      "Epoch:284 \t loss: 0.064279\n",
      "Epoch:285 \t loss: 0.196119\n",
      "Epoch:286 \t loss: 0.068475\n",
      "Epoch:287 \t loss: 0.137666\n",
      "Epoch:288 \t loss: 0.110836\n",
      "Epoch:289 \t loss: 0.041346\n",
      "Epoch:290 \t loss: 0.022905\n",
      "Epoch:291 \t loss: 0.075671\n",
      "Epoch:292 \t loss: 0.060223\n",
      "Epoch:293 \t loss: 0.043154\n",
      "Epoch:294 \t loss: 0.139362\n",
      "Epoch:295 \t loss: 0.070402\n",
      "Epoch:296 \t loss: 0.129761\n",
      "Epoch:297 \t loss: 0.007642\n",
      "Epoch:298 \t loss: 0.040679\n",
      "Epoch:299 \t loss: 0.086631\n",
      "Epoch:300 \t loss: 0.069803\n",
      "Epoch:301 \t loss: 0.013861\n",
      "Epoch:302 \t loss: 0.053270\n",
      "Epoch:303 \t loss: 0.120013\n",
      "Epoch:304 \t loss: 0.076453\n",
      "Epoch:305 \t loss: 0.112151\n",
      "Epoch:306 \t loss: 0.042429\n",
      "Epoch:307 \t loss: 0.167400\n",
      "Epoch:308 \t loss: 0.097017\n",
      "Epoch:309 \t loss: 0.022016\n",
      "Epoch:310 \t loss: 0.151482\n",
      "Epoch:311 \t loss: 0.011390\n",
      "Epoch:312 \t loss: 0.029643\n",
      "Epoch:313 \t loss: 0.095145\n",
      "Epoch:314 \t loss: 0.093988\n",
      "Epoch:315 \t loss: 0.098574\n",
      "Epoch:316 \t loss: 0.076248\n",
      "Epoch:317 \t loss: 0.029172\n",
      "Epoch:318 \t loss: 0.093079\n",
      "Epoch:319 \t loss: 0.164771\n",
      "Epoch:320 \t loss: 0.016692\n",
      "Epoch:321 \t loss: 0.027069\n",
      "Epoch:322 \t loss: 0.112844\n",
      "Epoch:323 \t loss: 0.026593\n",
      "Epoch:324 \t loss: 0.054940\n",
      "Epoch:325 \t loss: 0.028166\n",
      "Epoch:326 \t loss: 0.175215\n",
      "Epoch:327 \t loss: 0.083265\n",
      "Epoch:328 \t loss: 0.037558\n",
      "Epoch:329 \t loss: 0.134174\n",
      "Epoch:330 \t loss: 0.005028\n",
      "Epoch:331 \t loss: 0.068183\n",
      "Epoch:332 \t loss: 0.049565\n",
      "Epoch:333 \t loss: 0.172066\n",
      "Epoch:334 \t loss: 0.051499\n",
      "Epoch:335 \t loss: 0.098591\n",
      "Epoch:336 \t loss: 0.066159\n",
      "Epoch:337 \t loss: 0.061232\n",
      "Epoch:338 \t loss: 0.038285\n",
      "Epoch:339 \t loss: 0.018530\n",
      "Epoch:340 \t loss: 0.062535\n",
      "Epoch:341 \t loss: 0.012527\n",
      "Epoch:342 \t loss: 0.084925\n",
      "Epoch:343 \t loss: 0.040360\n",
      "Epoch:344 \t loss: 0.095751\n",
      "Epoch:345 \t loss: 0.104275\n",
      "Epoch:346 \t loss: 0.068320\n",
      "Epoch:347 \t loss: 0.206733\n",
      "Epoch:348 \t loss: 0.098702\n",
      "Epoch:349 \t loss: 0.055597\n",
      "Epoch:350 \t loss: 0.039985\n",
      "Epoch:351 \t loss: 0.092064\n",
      "Epoch:352 \t loss: 0.017491\n",
      "Epoch:353 \t loss: 0.132146\n",
      "Epoch:354 \t loss: 0.001964\n",
      "Epoch:355 \t loss: 0.069167\n",
      "Epoch:356 \t loss: 0.141353\n",
      "Epoch:357 \t loss: 0.068871\n",
      "Epoch:358 \t loss: 0.084013\n",
      "Epoch:359 \t loss: 0.159501\n",
      "Epoch:360 \t loss: 0.055565\n",
      "Epoch:361 \t loss: 0.075654\n",
      "Epoch:362 \t loss: 0.104070\n",
      "Epoch:363 \t loss: 0.061591\n",
      "Epoch:364 \t loss: 0.032360\n",
      "Epoch:365 \t loss: 0.071241\n",
      "Epoch:366 \t loss: 0.003089\n",
      "Epoch:367 \t loss: 0.070007\n",
      "Epoch:368 \t loss: 0.050150\n",
      "Epoch:369 \t loss: 0.052113\n",
      "Epoch:370 \t loss: 0.139278\n",
      "Epoch:371 \t loss: 0.149079\n",
      "Epoch:372 \t loss: 0.036785\n",
      "Epoch:373 \t loss: 0.059049\n",
      "Epoch:374 \t loss: 0.116243\n",
      "Epoch:375 \t loss: 0.136818\n",
      "Epoch:376 \t loss: 0.138904\n",
      "Epoch:377 \t loss: 0.066220\n",
      "Epoch:378 \t loss: 0.123556\n",
      "Epoch:379 \t loss: 0.038740\n",
      "Epoch:380 \t loss: 0.138062\n",
      "Epoch:381 \t loss: 0.042075\n",
      "Epoch:382 \t loss: 0.065865\n",
      "Epoch:383 \t loss: 0.122203\n",
      "Epoch:384 \t loss: 0.046437\n",
      "Epoch:385 \t loss: 0.082354\n",
      "Epoch:386 \t loss: 0.070469\n",
      "Epoch:387 \t loss: 0.044148\n",
      "Epoch:388 \t loss: 0.092692\n",
      "Epoch:389 \t loss: 0.062929\n",
      "Epoch:390 \t loss: 0.048272\n",
      "Epoch:391 \t loss: 0.203231\n",
      "Epoch:392 \t loss: 0.013136\n",
      "Epoch:393 \t loss: 0.030004\n",
      "Epoch:394 \t loss: 0.053184\n",
      "Epoch:395 \t loss: 0.091272\n",
      "Epoch:396 \t loss: 0.044068\n",
      "Epoch:397 \t loss: 0.091022\n",
      "Epoch:398 \t loss: 0.212930\n",
      "Epoch:399 \t loss: 0.025707\n",
      "Epoch:400 \t loss: 0.101564\n",
      "Epoch:401 \t loss: 0.067854\n",
      "Epoch:402 \t loss: 0.115984\n",
      "Epoch:403 \t loss: 0.033738\n",
      "Epoch:404 \t loss: 0.075397\n",
      "Epoch:405 \t loss: 0.035479\n",
      "Epoch:406 \t loss: 0.112704\n",
      "Epoch:407 \t loss: 0.002308\n",
      "Epoch:408 \t loss: 0.109505\n",
      "Epoch:409 \t loss: 0.049090\n",
      "Epoch:410 \t loss: 0.073271\n",
      "Epoch:411 \t loss: 0.084543\n",
      "Epoch:412 \t loss: 0.004685\n",
      "Epoch:413 \t loss: 0.104659\n",
      "Epoch:414 \t loss: 0.070789\n",
      "Epoch:415 \t loss: 0.051464\n",
      "Epoch:416 \t loss: 0.123356\n",
      "Epoch:417 \t loss: 0.104232\n",
      "Epoch:418 \t loss: 0.154667\n",
      "Epoch:419 \t loss: 0.025811\n",
      "Epoch:420 \t loss: 0.055608\n",
      "Epoch:421 \t loss: 0.093952\n",
      "Epoch:422 \t loss: 0.056926\n",
      "Epoch:423 \t loss: 0.044418\n",
      "Epoch:424 \t loss: 0.001332\n",
      "Epoch:425 \t loss: 0.042052\n",
      "Epoch:426 \t loss: 0.067335\n",
      "Epoch:427 \t loss: 0.046932\n",
      "Epoch:428 \t loss: 0.049174\n",
      "Epoch:429 \t loss: 0.044737\n",
      "Epoch:430 \t loss: 0.022486\n",
      "Epoch:431 \t loss: 0.029379\n",
      "Epoch:432 \t loss: 0.090352\n",
      "Epoch:433 \t loss: 0.115943\n",
      "Epoch:434 \t loss: 0.036804\n",
      "Epoch:435 \t loss: 0.023887\n",
      "Epoch:436 \t loss: 0.089079\n",
      "Epoch:437 \t loss: 0.092243\n",
      "Epoch:438 \t loss: 0.042512\n",
      "Epoch:439 \t loss: 0.024190\n",
      "Epoch:440 \t loss: 0.021770\n",
      "Epoch:441 \t loss: 0.041429\n",
      "Epoch:442 \t loss: 0.109584\n",
      "Epoch:443 \t loss: 0.082990\n",
      "Epoch:444 \t loss: 0.054228\n",
      "Epoch:445 \t loss: 0.024783\n",
      "Epoch:446 \t loss: 0.146765\n",
      "Epoch:447 \t loss: 0.062451\n",
      "Epoch:448 \t loss: 0.060209\n",
      "Epoch:449 \t loss: 0.058714\n",
      "Epoch:450 \t loss: 0.040199\n",
      "Epoch:451 \t loss: 0.047997\n",
      "Epoch:452 \t loss: 0.075768\n",
      "Epoch:453 \t loss: 0.181086\n",
      "Epoch:454 \t loss: 0.158580\n",
      "Epoch:455 \t loss: 0.113486\n",
      "Epoch:456 \t loss: 0.072512\n",
      "Epoch:457 \t loss: 0.067860\n",
      "Epoch:458 \t loss: 0.083837\n",
      "Epoch:459 \t loss: 0.034666\n",
      "Epoch:460 \t loss: 0.097039\n",
      "Epoch:461 \t loss: 0.037214\n",
      "Epoch:462 \t loss: 0.132383\n",
      "Epoch:463 \t loss: 0.046331\n",
      "Epoch:464 \t loss: 0.135133\n",
      "Epoch:465 \t loss: 0.058688\n",
      "Epoch:466 \t loss: 0.225858\n",
      "Epoch:467 \t loss: 0.009794\n",
      "Epoch:468 \t loss: 0.005204\n",
      "Epoch:469 \t loss: 0.110603\n",
      "Epoch:470 \t loss: 0.026188\n",
      "Epoch:471 \t loss: 0.021433\n",
      "Epoch:472 \t loss: 0.149631\n",
      "Epoch:473 \t loss: 0.041155\n",
      "Epoch:474 \t loss: 0.044924\n",
      "Epoch:475 \t loss: 0.099206\n",
      "Epoch:476 \t loss: 0.072039\n",
      "Epoch:477 \t loss: 0.043171\n",
      "Epoch:478 \t loss: 0.040508\n",
      "Epoch:479 \t loss: 0.046018\n",
      "Epoch:480 \t loss: 0.032882\n",
      "Epoch:481 \t loss: 0.094496\n",
      "Epoch:482 \t loss: 0.088818\n",
      "Epoch:483 \t loss: 0.075513\n",
      "Epoch:484 \t loss: 0.061012\n",
      "Epoch:485 \t loss: 0.055712\n",
      "Epoch:486 \t loss: 0.065345\n",
      "Epoch:487 \t loss: 0.057114\n",
      "Epoch:488 \t loss: 0.111381\n",
      "Epoch:489 \t loss: 0.051335\n",
      "Epoch:490 \t loss: 0.038698\n",
      "Epoch:491 \t loss: 0.061598\n",
      "Epoch:492 \t loss: 0.108388\n",
      "Epoch:493 \t loss: 0.131737\n",
      "Epoch:494 \t loss: 0.133611\n",
      "Epoch:495 \t loss: 0.052245\n",
      "Epoch:496 \t loss: 0.102833\n",
      "Epoch:497 \t loss: 0.053672\n",
      "Epoch:498 \t loss: 0.071472\n",
      "Epoch:499 \t loss: 0.087237\n",
      "Epoch:500 \t loss: 0.079971\n",
      "Epoch:501 \t loss: 0.081468\n",
      "Epoch:502 \t loss: 0.053383\n",
      "Epoch:503 \t loss: 0.037745\n",
      "Epoch:504 \t loss: 0.051797\n",
      "Epoch:505 \t loss: 0.009682\n",
      "Epoch:506 \t loss: 0.099823\n",
      "Epoch:507 \t loss: 0.068976\n",
      "Epoch:508 \t loss: 0.020706\n",
      "Epoch:509 \t loss: 0.052081\n",
      "Epoch:510 \t loss: 0.035681\n",
      "Epoch:511 \t loss: 0.063447\n",
      "Epoch:512 \t loss: 0.112292\n",
      "Epoch:513 \t loss: 0.043520\n",
      "Epoch:514 \t loss: 0.086915\n",
      "Epoch:515 \t loss: 0.056725\n",
      "Epoch:516 \t loss: 0.034582\n",
      "Epoch:517 \t loss: 0.083276\n",
      "Epoch:518 \t loss: 0.053992\n",
      "Epoch:519 \t loss: 0.052277\n",
      "Epoch:520 \t loss: 0.026107\n",
      "Epoch:521 \t loss: 0.020243\n",
      "Epoch:522 \t loss: 0.035207\n",
      "Epoch:523 \t loss: 0.044800\n",
      "Epoch:524 \t loss: 0.034655\n",
      "Epoch:525 \t loss: 0.053729\n",
      "Epoch:526 \t loss: 0.085988\n",
      "Epoch:527 \t loss: 0.153453\n",
      "Epoch:528 \t loss: 0.031762\n",
      "Epoch:529 \t loss: 0.142447\n",
      "Epoch:530 \t loss: 0.068199\n",
      "Epoch:531 \t loss: 0.023277\n",
      "Epoch:532 \t loss: 0.042361\n",
      "Epoch:533 \t loss: 0.091693\n",
      "Epoch:534 \t loss: 0.134586\n",
      "Epoch:535 \t loss: 0.053275\n",
      "Epoch:536 \t loss: 0.061531\n",
      "Epoch:537 \t loss: 0.053258\n",
      "Epoch:538 \t loss: 0.118342\n",
      "Epoch:539 \t loss: 0.056302\n",
      "Epoch:540 \t loss: 0.053543\n",
      "Epoch:541 \t loss: 0.112155\n",
      "Epoch:542 \t loss: 0.084222\n",
      "Epoch:543 \t loss: 0.085639\n",
      "Epoch:544 \t loss: 0.081108\n",
      "Epoch:545 \t loss: 0.031456\n",
      "Epoch:546 \t loss: 0.008698\n",
      "Epoch:547 \t loss: 0.037889\n",
      "Epoch:548 \t loss: 0.035209\n",
      "Epoch:549 \t loss: 0.101396\n",
      "Epoch:550 \t loss: 0.038098\n",
      "Epoch:551 \t loss: 0.025072\n",
      "Epoch:552 \t loss: 0.023888\n",
      "Epoch:553 \t loss: 0.084827\n",
      "Epoch:554 \t loss: 0.098230\n",
      "Epoch:555 \t loss: 0.067198\n",
      "Epoch:556 \t loss: 0.070618\n",
      "Epoch:557 \t loss: 0.093873\n",
      "Epoch:558 \t loss: 0.112904\n",
      "Epoch:559 \t loss: 0.066082\n",
      "Epoch:560 \t loss: 0.161865\n",
      "Epoch:561 \t loss: 0.065321\n",
      "Epoch:562 \t loss: 0.105866\n",
      "Epoch:563 \t loss: 0.053414\n",
      "Epoch:564 \t loss: 0.028937\n",
      "Epoch:565 \t loss: 0.094573\n",
      "Epoch:566 \t loss: 0.045628\n",
      "Epoch:567 \t loss: 0.080183\n",
      "Epoch:568 \t loss: 0.037663\n",
      "Epoch:569 \t loss: 0.048109\n",
      "Epoch:570 \t loss: 0.058286\n",
      "Epoch:571 \t loss: 0.000384\n",
      "Epoch:572 \t loss: 0.097751\n",
      "Epoch:573 \t loss: 0.070372\n",
      "Epoch:574 \t loss: 0.088723\n",
      "Epoch:575 \t loss: 0.050360\n",
      "Epoch:576 \t loss: 0.064988\n",
      "Epoch:577 \t loss: 0.112255\n",
      "Epoch:578 \t loss: 0.098139\n",
      "Epoch:579 \t loss: 0.067467\n",
      "Epoch:580 \t loss: 0.050510\n",
      "Epoch:581 \t loss: 0.026350\n",
      "Epoch:582 \t loss: 0.050573\n",
      "Epoch:583 \t loss: 0.096339\n",
      "Epoch:584 \t loss: 0.092533\n",
      "Epoch:585 \t loss: 0.064915\n",
      "Epoch:586 \t loss: 0.130582\n",
      "Epoch:587 \t loss: 0.068361\n",
      "Epoch:588 \t loss: 0.063854\n",
      "Epoch:589 \t loss: 0.151363\n",
      "Epoch:590 \t loss: 0.000678\n",
      "Epoch:591 \t loss: 0.043830\n",
      "Epoch:592 \t loss: 0.053622\n",
      "Epoch:593 \t loss: 0.163563\n",
      "Epoch:594 \t loss: 0.069804\n",
      "Epoch:595 \t loss: 0.088371\n",
      "Epoch:596 \t loss: 0.073739\n",
      "Epoch:597 \t loss: 0.122155\n",
      "Epoch:598 \t loss: 0.088915\n",
      "Epoch:599 \t loss: 0.043859\n",
      "Epoch:600 \t loss: 0.090504\n",
      "Epoch:601 \t loss: 0.049438\n",
      "Epoch:602 \t loss: 0.081122\n",
      "Epoch:603 \t loss: 0.126907\n",
      "Epoch:604 \t loss: 0.122754\n",
      "Epoch:605 \t loss: 0.068221\n",
      "Epoch:606 \t loss: 0.047191\n",
      "Epoch:607 \t loss: 0.073480\n",
      "Epoch:608 \t loss: 0.023392\n",
      "Epoch:609 \t loss: 0.027740\n",
      "Epoch:610 \t loss: 0.087461\n",
      "Epoch:611 \t loss: 0.058587\n",
      "Epoch:612 \t loss: 0.062042\n",
      "Epoch:613 \t loss: 0.102809\n",
      "Epoch:614 \t loss: 0.085440\n",
      "Epoch:615 \t loss: 0.026375\n",
      "Epoch:616 \t loss: 0.111329\n",
      "Epoch:617 \t loss: 0.096996\n",
      "Epoch:618 \t loss: 0.102372\n",
      "Epoch:619 \t loss: 0.100142\n",
      "Epoch:620 \t loss: 0.071166\n",
      "Epoch:621 \t loss: 0.046644\n",
      "Epoch:622 \t loss: 0.048603\n",
      "Epoch:623 \t loss: 0.076529\n",
      "Epoch:624 \t loss: 0.058048\n",
      "Epoch:625 \t loss: 0.075635\n",
      "Epoch:626 \t loss: 0.095584\n",
      "Epoch:627 \t loss: 0.019505\n",
      "Epoch:628 \t loss: 0.125579\n",
      "Epoch:629 \t loss: 0.041806\n",
      "Epoch:630 \t loss: 0.044120\n",
      "Epoch:631 \t loss: 0.061529\n",
      "Epoch:632 \t loss: 0.063008\n",
      "Epoch:633 \t loss: 0.116331\n",
      "Epoch:634 \t loss: 0.042931\n",
      "Epoch:635 \t loss: 0.039298\n",
      "Epoch:636 \t loss: 0.051837\n",
      "Epoch:637 \t loss: 0.001009\n",
      "Epoch:638 \t loss: 0.077243\n",
      "Epoch:639 \t loss: 0.059959\n",
      "Epoch:640 \t loss: 0.077129\n",
      "Epoch:641 \t loss: 0.079621\n",
      "Epoch:642 \t loss: 0.060935\n",
      "Epoch:643 \t loss: 0.027314\n",
      "Epoch:644 \t loss: 0.044783\n",
      "Epoch:645 \t loss: 0.061176\n",
      "Epoch:646 \t loss: 0.046613\n",
      "Epoch:647 \t loss: 0.081579\n",
      "Epoch:648 \t loss: 0.092582\n",
      "Epoch:649 \t loss: 0.031082\n",
      "Epoch:650 \t loss: 0.042257\n",
      "Epoch:651 \t loss: 0.089775\n",
      "Epoch:652 \t loss: 0.099068\n",
      "Epoch:653 \t loss: 0.020813\n",
      "Epoch:654 \t loss: 0.052056\n",
      "Epoch:655 \t loss: 0.052221\n",
      "Epoch:656 \t loss: 0.115415\n",
      "Epoch:657 \t loss: 0.040786\n",
      "Epoch:658 \t loss: 0.125815\n",
      "Epoch:659 \t loss: 0.020130\n",
      "Epoch:660 \t loss: 0.049780\n",
      "Epoch:661 \t loss: 0.035128\n",
      "Epoch:662 \t loss: 0.052481\n",
      "Epoch:663 \t loss: 0.145228\n",
      "Epoch:664 \t loss: 0.056441\n",
      "Epoch:665 \t loss: 0.061106\n",
      "Epoch:666 \t loss: 0.070481\n",
      "Epoch:667 \t loss: 0.022124\n",
      "Epoch:668 \t loss: 0.091571\n",
      "Epoch:669 \t loss: 0.110314\n",
      "Epoch:670 \t loss: 0.049869\n",
      "Epoch:671 \t loss: 0.071382\n",
      "Epoch:672 \t loss: 0.148115\n",
      "Epoch:673 \t loss: 0.086372\n",
      "Epoch:674 \t loss: 0.032800\n",
      "Epoch:675 \t loss: 0.065534\n",
      "Epoch:676 \t loss: 0.012270\n",
      "Epoch:677 \t loss: 0.090021\n",
      "Epoch:678 \t loss: 0.044888\n",
      "Epoch:679 \t loss: 0.036436\n",
      "Epoch:680 \t loss: 0.000648\n",
      "Epoch:681 \t loss: 0.086966\n",
      "Epoch:682 \t loss: 0.031505\n",
      "Epoch:683 \t loss: 0.049556\n",
      "Epoch:684 \t loss: 0.057744\n",
      "Epoch:685 \t loss: 0.042535\n",
      "Epoch:686 \t loss: 0.107897\n",
      "Epoch:687 \t loss: 0.123284\n",
      "Epoch:688 \t loss: 0.047999\n",
      "Epoch:689 \t loss: 0.084547\n",
      "Epoch:690 \t loss: 0.043107\n",
      "Epoch:691 \t loss: 0.101809\n",
      "Epoch:692 \t loss: 0.090697\n",
      "Epoch:693 \t loss: 0.027397\n",
      "Epoch:694 \t loss: 0.109339\n",
      "Epoch:695 \t loss: 0.104649\n",
      "Epoch:696 \t loss: 0.067146\n",
      "Epoch:697 \t loss: 0.074827\n",
      "Epoch:698 \t loss: 0.015080\n",
      "Epoch:699 \t loss: 0.000461\n",
      "Epoch:700 \t loss: 0.060349\n",
      "Epoch:701 \t loss: 0.041376\n",
      "Epoch:702 \t loss: 0.073730\n",
      "Epoch:703 \t loss: 0.109342\n",
      "Epoch:704 \t loss: 0.081222\n",
      "Epoch:705 \t loss: 0.058354\n",
      "Epoch:706 \t loss: 0.046075\n",
      "Epoch:707 \t loss: 0.166330\n",
      "Epoch:708 \t loss: 0.040597\n",
      "Epoch:709 \t loss: 0.120538\n",
      "Epoch:710 \t loss: 0.031527\n",
      "Epoch:711 \t loss: 0.049031\n",
      "Epoch:712 \t loss: 0.086857\n",
      "Epoch:713 \t loss: 0.065738\n",
      "Epoch:714 \t loss: 0.139559\n",
      "Epoch:715 \t loss: 0.062878\n",
      "Epoch:716 \t loss: 0.059120\n",
      "Epoch:717 \t loss: 0.000314\n",
      "Epoch:718 \t loss: 0.058440\n",
      "Epoch:719 \t loss: 0.082704\n",
      "Epoch:720 \t loss: 0.047638\n",
      "Epoch:721 \t loss: 0.050913\n",
      "Epoch:722 \t loss: 0.111143\n",
      "Epoch:723 \t loss: 0.034286\n",
      "Epoch:724 \t loss: 0.018972\n",
      "Epoch:725 \t loss: 0.000361\n",
      "Epoch:726 \t loss: 0.068704\n",
      "Epoch:727 \t loss: 0.067689\n",
      "Epoch:728 \t loss: 0.058034\n",
      "Epoch:729 \t loss: 0.015722\n",
      "Epoch:730 \t loss: 0.000209\n",
      "Epoch:731 \t loss: 0.074441\n",
      "Epoch:732 \t loss: 0.034940\n",
      "Epoch:733 \t loss: 0.134287\n",
      "Epoch:734 \t loss: 0.158561\n",
      "Epoch:735 \t loss: 0.130084\n",
      "Epoch:736 \t loss: 0.051212\n",
      "Epoch:737 \t loss: 0.092944\n",
      "Epoch:738 \t loss: 0.077381\n",
      "Epoch:739 \t loss: 0.067903\n",
      "Epoch:740 \t loss: 0.107505\n",
      "Epoch:741 \t loss: 0.082698\n",
      "Epoch:742 \t loss: 0.080442\n",
      "Epoch:743 \t loss: 0.066527\n",
      "Epoch:744 \t loss: 0.175000\n",
      "Epoch:745 \t loss: 0.082654\n",
      "Epoch:746 \t loss: 0.059868\n",
      "Epoch:747 \t loss: 0.014751\n",
      "Epoch:748 \t loss: 0.098327\n",
      "Epoch:749 \t loss: 0.048581\n",
      "Epoch:750 \t loss: 0.153532\n",
      "Epoch:751 \t loss: 0.001869\n",
      "Epoch:752 \t loss: 0.015918\n",
      "Epoch:753 \t loss: 0.017668\n",
      "Epoch:754 \t loss: 0.042494\n",
      "Epoch:755 \t loss: 0.063444\n",
      "Epoch:756 \t loss: 0.110938\n",
      "Epoch:757 \t loss: 0.058611\n",
      "Epoch:758 \t loss: 0.097716\n",
      "Epoch:759 \t loss: 0.093078\n",
      "Epoch:760 \t loss: 0.061487\n",
      "Epoch:761 \t loss: 0.090045\n",
      "Epoch:762 \t loss: 0.097480\n",
      "Epoch:763 \t loss: 0.028366\n",
      "Epoch:764 \t loss: 0.038821\n",
      "Epoch:765 \t loss: 0.061506\n",
      "Epoch:766 \t loss: 0.061405\n",
      "Epoch:767 \t loss: 0.074506\n",
      "Epoch:768 \t loss: 0.012939\n",
      "Epoch:769 \t loss: 0.085227\n",
      "Epoch:770 \t loss: 0.063785\n",
      "Epoch:771 \t loss: 0.073111\n",
      "Epoch:772 \t loss: 0.022759\n",
      "Epoch:773 \t loss: 0.058591\n",
      "Epoch:774 \t loss: 0.068111\n",
      "Epoch:775 \t loss: 0.009320\n",
      "Epoch:776 \t loss: 0.056055\n",
      "Epoch:777 \t loss: 0.107662\n",
      "Epoch:778 \t loss: 0.019462\n",
      "Epoch:779 \t loss: 0.058276\n",
      "Epoch:780 \t loss: 0.113848\n",
      "Epoch:781 \t loss: 0.102009\n",
      "Epoch:782 \t loss: 0.070471\n",
      "Epoch:783 \t loss: 0.021204\n",
      "Epoch:784 \t loss: 0.032606\n",
      "Epoch:785 \t loss: 0.032462\n",
      "Epoch:786 \t loss: 0.060642\n",
      "Epoch:787 \t loss: 0.008119\n",
      "Epoch:788 \t loss: 0.068347\n",
      "Epoch:789 \t loss: 0.026086\n",
      "Epoch:790 \t loss: 0.116215\n",
      "Epoch:791 \t loss: 0.060799\n",
      "Epoch:792 \t loss: 0.098784\n",
      "Epoch:793 \t loss: 0.129643\n",
      "Epoch:794 \t loss: 0.001090\n",
      "Epoch:795 \t loss: 0.011248\n",
      "Epoch:796 \t loss: 0.050282\n",
      "Epoch:797 \t loss: 0.063953\n",
      "Epoch:798 \t loss: 0.034012\n",
      "Epoch:799 \t loss: 0.046374\n",
      "Epoch:800 \t loss: 0.111957\n",
      "Epoch:801 \t loss: 0.076635\n",
      "Epoch:802 \t loss: 0.104626\n",
      "Epoch:803 \t loss: 0.063456\n",
      "Epoch:804 \t loss: 0.081233\n",
      "Epoch:805 \t loss: 0.108604\n",
      "Epoch:806 \t loss: 0.086557\n",
      "Epoch:807 \t loss: 0.038471\n",
      "Epoch:808 \t loss: 0.021203\n",
      "Epoch:809 \t loss: 0.015520\n",
      "Epoch:810 \t loss: 0.061880\n",
      "Epoch:811 \t loss: 0.028239\n",
      "Epoch:812 \t loss: 0.013287\n",
      "Epoch:813 \t loss: 0.080191\n",
      "Epoch:814 \t loss: 0.037211\n",
      "Epoch:815 \t loss: 0.032203\n",
      "Epoch:816 \t loss: 0.030781\n",
      "Epoch:817 \t loss: 0.049074\n",
      "Epoch:818 \t loss: 0.115029\n",
      "Epoch:819 \t loss: 0.000393\n",
      "Epoch:820 \t loss: 0.000173\n",
      "Epoch:821 \t loss: 0.073959\n",
      "Epoch:822 \t loss: 0.077509\n",
      "Epoch:823 \t loss: 0.033270\n",
      "Epoch:824 \t loss: 0.049831\n",
      "Epoch:825 \t loss: 0.079511\n",
      "Epoch:826 \t loss: 0.044842\n",
      "Epoch:827 \t loss: 0.104701\n",
      "Epoch:828 \t loss: 0.073134\n",
      "Epoch:829 \t loss: 0.043127\n",
      "Epoch:830 \t loss: 0.024785\n",
      "Epoch:831 \t loss: 0.052319\n",
      "Epoch:832 \t loss: 0.071646\n",
      "Epoch:833 \t loss: 0.059447\n",
      "Epoch:834 \t loss: 0.016879\n",
      "Epoch:835 \t loss: 0.002047\n",
      "Epoch:836 \t loss: 0.051843\n",
      "Epoch:837 \t loss: 0.045267\n",
      "Epoch:838 \t loss: 0.122192\n",
      "Epoch:839 \t loss: 0.109322\n",
      "Epoch:840 \t loss: 0.007882\n",
      "Epoch:841 \t loss: 0.097613\n",
      "Epoch:842 \t loss: 0.075948\n",
      "Epoch:843 \t loss: 0.007523\n",
      "Epoch:844 \t loss: 0.070439\n",
      "Epoch:845 \t loss: 0.116256\n",
      "Epoch:846 \t loss: 0.068443\n",
      "Epoch:847 \t loss: 0.052391\n",
      "Epoch:848 \t loss: 0.000461\n",
      "Epoch:849 \t loss: 0.050567\n",
      "Epoch:850 \t loss: 0.071377\n",
      "Epoch:851 \t loss: 0.083631\n",
      "Epoch:852 \t loss: 0.034428\n",
      "Epoch:853 \t loss: 0.046566\n",
      "Epoch:854 \t loss: 0.067974\n",
      "Epoch:855 \t loss: 0.132451\n",
      "Epoch:856 \t loss: 0.060961\n",
      "Epoch:857 \t loss: 0.075282\n",
      "Epoch:858 \t loss: 0.057878\n",
      "Epoch:859 \t loss: 0.080382\n",
      "Epoch:860 \t loss: 0.097389\n",
      "Epoch:861 \t loss: 0.000589\n",
      "Epoch:862 \t loss: 0.064264\n",
      "Epoch:863 \t loss: 0.074558\n",
      "Epoch:864 \t loss: 0.043727\n",
      "Epoch:865 \t loss: 0.069559\n",
      "Epoch:866 \t loss: 0.034746\n",
      "Epoch:867 \t loss: 0.112720\n",
      "Epoch:868 \t loss: 0.109283\n",
      "Epoch:869 \t loss: 0.059106\n",
      "Epoch:870 \t loss: 0.054045\n",
      "Epoch:871 \t loss: 0.109199\n",
      "Epoch:872 \t loss: 0.060858\n",
      "Epoch:873 \t loss: 0.045499\n",
      "Epoch:874 \t loss: 0.000566\n",
      "Epoch:875 \t loss: 0.014298\n",
      "Epoch:876 \t loss: 0.071297\n",
      "Epoch:877 \t loss: 0.017185\n",
      "Epoch:878 \t loss: 0.027187\n",
      "Epoch:879 \t loss: 0.073126\n",
      "Epoch:880 \t loss: 0.023303\n",
      "Epoch:881 \t loss: 0.034038\n",
      "Epoch:882 \t loss: 0.079201\n",
      "Epoch:883 \t loss: 0.058313\n",
      "Epoch:884 \t loss: 0.132466\n",
      "Epoch:885 \t loss: 0.077238\n",
      "Epoch:886 \t loss: 0.013820\n",
      "Epoch:887 \t loss: 0.034208\n",
      "Epoch:888 \t loss: 0.044121\n",
      "Epoch:889 \t loss: 0.074114\n",
      "Epoch:890 \t loss: 0.009719\n",
      "Epoch:891 \t loss: 0.098748\n",
      "Epoch:892 \t loss: 0.000225\n",
      "Epoch:893 \t loss: 0.063253\n",
      "Epoch:894 \t loss: 0.084664\n",
      "Epoch:895 \t loss: 0.007314\n",
      "Epoch:896 \t loss: 0.091278\n",
      "Epoch:897 \t loss: 0.047917\n",
      "Epoch:898 \t loss: 0.062225\n",
      "Epoch:899 \t loss: 0.065815\n",
      "Epoch:900 \t loss: 0.009076\n",
      "Epoch:901 \t loss: 0.062343\n",
      "Epoch:902 \t loss: 0.075156\n",
      "Epoch:903 \t loss: 0.100498\n",
      "Epoch:904 \t loss: 0.014732\n",
      "Epoch:905 \t loss: 0.039587\n",
      "Epoch:906 \t loss: 0.024051\n",
      "Epoch:907 \t loss: 0.048072\n",
      "Epoch:908 \t loss: 0.023300\n",
      "Epoch:909 \t loss: 0.064896\n",
      "Epoch:910 \t loss: 0.088256\n",
      "Epoch:911 \t loss: 0.095316\n",
      "Epoch:912 \t loss: 0.033278\n",
      "Epoch:913 \t loss: 0.022714\n",
      "Epoch:914 \t loss: 0.018351\n",
      "Epoch:915 \t loss: 0.126082\n",
      "Epoch:916 \t loss: 0.106300\n",
      "Epoch:917 \t loss: 0.105425\n",
      "Epoch:918 \t loss: 0.097037\n",
      "Epoch:919 \t loss: 0.051374\n",
      "Epoch:920 \t loss: 0.106207\n",
      "Epoch:921 \t loss: 0.132063\n",
      "Epoch:922 \t loss: 0.058795\n",
      "Epoch:923 \t loss: 0.031409\n",
      "Epoch:924 \t loss: 0.066551\n",
      "Epoch:925 \t loss: 0.027263\n",
      "Epoch:926 \t loss: 0.000449\n",
      "Epoch:927 \t loss: 0.014067\n",
      "Epoch:928 \t loss: 0.057004\n",
      "Epoch:929 \t loss: 0.114634\n",
      "Epoch:930 \t loss: 0.022503\n",
      "Epoch:931 \t loss: 0.050514\n",
      "Epoch:932 \t loss: 0.022054\n",
      "Epoch:933 \t loss: 0.015647\n",
      "Epoch:934 \t loss: 0.109621\n",
      "Epoch:935 \t loss: 0.089745\n",
      "Epoch:936 \t loss: 0.029640\n",
      "Epoch:937 \t loss: 0.046701\n",
      "Epoch:938 \t loss: 0.097642\n",
      "Epoch:939 \t loss: 0.023503\n",
      "Epoch:940 \t loss: 0.088375\n",
      "Epoch:941 \t loss: 0.066814\n",
      "Epoch:942 \t loss: 0.055837\n",
      "Epoch:943 \t loss: 0.083927\n",
      "Epoch:944 \t loss: 0.011341\n",
      "Epoch:945 \t loss: 0.110211\n",
      "Epoch:946 \t loss: 0.071383\n",
      "Epoch:947 \t loss: 0.047791\n",
      "Epoch:948 \t loss: 0.078335\n",
      "Epoch:949 \t loss: 0.090490\n",
      "Epoch:950 \t loss: 0.066783\n",
      "Epoch:951 \t loss: 0.000302\n",
      "Epoch:952 \t loss: 0.103275\n",
      "Epoch:953 \t loss: 0.140323\n",
      "Epoch:954 \t loss: 0.081673\n",
      "Epoch:955 \t loss: 0.036655\n",
      "Epoch:956 \t loss: 0.139865\n",
      "Epoch:957 \t loss: 0.046176\n",
      "Epoch:958 \t loss: 0.032279\n",
      "Epoch:959 \t loss: 0.008432\n",
      "Epoch:960 \t loss: 0.044595\n",
      "Epoch:961 \t loss: 0.035596\n",
      "Epoch:962 \t loss: 0.029304\n",
      "Epoch:963 \t loss: 0.102242\n",
      "Epoch:964 \t loss: 0.081729\n",
      "Epoch:965 \t loss: 0.043457\n",
      "Epoch:966 \t loss: 0.129458\n",
      "Epoch:967 \t loss: 0.000591\n",
      "Epoch:968 \t loss: 0.082036\n",
      "Epoch:969 \t loss: 0.057538\n",
      "Epoch:970 \t loss: 0.074481\n",
      "Epoch:971 \t loss: 0.020127\n",
      "Epoch:972 \t loss: 0.000375\n",
      "Epoch:973 \t loss: 0.044552\n",
      "Epoch:974 \t loss: 0.048259\n",
      "Epoch:975 \t loss: 0.019552\n",
      "Epoch:976 \t loss: 0.081731\n",
      "Epoch:977 \t loss: 0.031768\n",
      "Epoch:978 \t loss: 0.049957\n",
      "Epoch:979 \t loss: 0.141013\n",
      "Epoch:980 \t loss: 0.085368\n",
      "Epoch:981 \t loss: 0.113273\n",
      "Epoch:982 \t loss: 0.110653\n",
      "Epoch:983 \t loss: 0.042893\n",
      "Epoch:984 \t loss: 0.048065\n",
      "Epoch:985 \t loss: 0.060613\n",
      "Epoch:986 \t loss: 0.072606\n",
      "Epoch:987 \t loss: 0.075539\n",
      "Epoch:988 \t loss: 0.059270\n",
      "Epoch:989 \t loss: 0.000146\n",
      "Epoch:990 \t loss: 0.007899\n",
      "Epoch:991 \t loss: 0.116246\n",
      "Epoch:992 \t loss: 0.029638\n",
      "Epoch:993 \t loss: 0.071115\n",
      "Epoch:994 \t loss: 0.048972\n",
      "Epoch:995 \t loss: 0.066545\n",
      "Epoch:996 \t loss: 0.060769\n",
      "Epoch:997 \t loss: 0.055991\n",
      "Epoch:998 \t loss: 0.033589\n",
      "Epoch:999 \t loss: 0.054226\n",
      "Epoch:1000 \t loss: 0.064691\n",
      "Epoch:1001 \t loss: 0.059559\n",
      "Epoch:1002 \t loss: 0.055337\n",
      "Epoch:1003 \t loss: 0.044680\n",
      "Epoch:1004 \t loss: 0.066588\n",
      "Epoch:1005 \t loss: 0.079606\n",
      "Epoch:1006 \t loss: 0.018455\n",
      "Epoch:1007 \t loss: 0.065813\n",
      "Epoch:1008 \t loss: 0.103980\n",
      "Epoch:1009 \t loss: 0.055480\n",
      "Epoch:1010 \t loss: 0.035710\n",
      "Epoch:1011 \t loss: 0.018185\n",
      "Epoch:1012 \t loss: 0.058218\n",
      "Epoch:1013 \t loss: 0.000108\n",
      "Epoch:1014 \t loss: 0.018106\n",
      "Epoch:1015 \t loss: 0.080109\n",
      "Epoch:1016 \t loss: 0.051124\n",
      "Epoch:1017 \t loss: 0.000372\n",
      "Epoch:1018 \t loss: 0.027933\n",
      "Epoch:1019 \t loss: 0.048596\n",
      "Epoch:1020 \t loss: 0.114947\n",
      "Epoch:1021 \t loss: 0.057717\n",
      "Epoch:1022 \t loss: 0.059548\n",
      "Epoch:1023 \t loss: 0.069401\n",
      "Epoch:1024 \t loss: 0.000781\n",
      "Epoch:1025 \t loss: 0.000178\n",
      "Epoch:1026 \t loss: 0.147075\n",
      "Epoch:1027 \t loss: 0.018818\n",
      "Epoch:1028 \t loss: 0.037468\n",
      "Epoch:1029 \t loss: 0.069919\n",
      "Epoch:1030 \t loss: 0.056949\n",
      "Epoch:1031 \t loss: 0.076339\n",
      "Epoch:1032 \t loss: 0.026486\n",
      "Epoch:1033 \t loss: 0.033078\n",
      "Epoch:1034 \t loss: 0.039213\n",
      "Epoch:1035 \t loss: 0.043578\n",
      "Epoch:1036 \t loss: 0.044367\n",
      "Epoch:1037 \t loss: 0.136824\n",
      "Epoch:1038 \t loss: 0.078730\n",
      "Epoch:1039 \t loss: 0.062907\n",
      "Epoch:1040 \t loss: 0.172361\n",
      "Epoch:1041 \t loss: 0.018706\n",
      "Epoch:1042 \t loss: 0.049435\n",
      "Epoch:1043 \t loss: 0.064428\n",
      "Epoch:1044 \t loss: 0.054026\n",
      "Epoch:1045 \t loss: 0.058949\n",
      "Epoch:1046 \t loss: 0.056337\n",
      "Epoch:1047 \t loss: 0.129985\n",
      "Epoch:1048 \t loss: 0.051524\n",
      "Epoch:1049 \t loss: 0.104623\n",
      "Epoch:1050 \t loss: 0.028831\n",
      "Epoch:1051 \t loss: 0.029694\n",
      "Epoch:1052 \t loss: 0.063339\n",
      "Epoch:1053 \t loss: 0.046848\n",
      "Epoch:1054 \t loss: 0.103658\n",
      "Epoch:1055 \t loss: 0.104909\n",
      "Epoch:1056 \t loss: 0.088535\n",
      "Epoch:1057 \t loss: 0.033250\n",
      "Epoch:1058 \t loss: 0.052543\n",
      "Epoch:1059 \t loss: 0.077325\n",
      "Epoch:1060 \t loss: 0.031140\n",
      "Epoch:1061 \t loss: 0.008861\n",
      "Epoch:1062 \t loss: 0.023802\n",
      "Epoch:1063 \t loss: 0.054795\n",
      "Epoch:1064 \t loss: 0.102660\n",
      "Epoch:1065 \t loss: 0.043173\n",
      "Epoch:1066 \t loss: 0.087814\n",
      "Epoch:1067 \t loss: 0.029975\n",
      "Epoch:1068 \t loss: 0.052857\n",
      "Epoch:1069 \t loss: 0.020361\n",
      "Epoch:1070 \t loss: 0.008478\n",
      "Epoch:1071 \t loss: 0.018856\n",
      "Epoch:1072 \t loss: 0.040416\n",
      "Epoch:1073 \t loss: 0.091949\n",
      "Epoch:1074 \t loss: 0.090707\n",
      "Epoch:1075 \t loss: 0.044526\n",
      "Epoch:1076 \t loss: 0.014603\n",
      "Epoch:1077 \t loss: 0.057720\n",
      "Epoch:1078 \t loss: 0.125997\n",
      "Epoch:1079 \t loss: 0.116928\n",
      "Epoch:1080 \t loss: 0.044209\n",
      "Epoch:1081 \t loss: 0.081095\n",
      "Epoch:1082 \t loss: 0.059783\n",
      "Epoch:1083 \t loss: 0.066043\n",
      "Epoch:1084 \t loss: 0.049230\n",
      "Epoch:1085 \t loss: 0.075041\n",
      "Epoch:1086 \t loss: 0.093422\n",
      "Epoch:1087 \t loss: 0.067834\n",
      "Epoch:1088 \t loss: 0.180367\n",
      "Epoch:1089 \t loss: 0.078928\n",
      "Epoch:1090 \t loss: 0.044535\n",
      "Epoch:1091 \t loss: 0.054957\n",
      "Epoch:1092 \t loss: 0.041721\n",
      "Epoch:1093 \t loss: 0.008668\n",
      "Epoch:1094 \t loss: 0.108995\n",
      "Epoch:1095 \t loss: 0.130769\n",
      "Epoch:1096 \t loss: 0.060679\n",
      "Epoch:1097 \t loss: 0.028547\n",
      "Epoch:1098 \t loss: 0.088740\n",
      "Epoch:1099 \t loss: 0.080651\n",
      "Epoch:1100 \t loss: 0.123909\n",
      "Epoch:1101 \t loss: 0.052035\n",
      "Epoch:1102 \t loss: 0.035141\n",
      "Epoch:1103 \t loss: 0.077023\n",
      "Epoch:1104 \t loss: 0.037826\n",
      "Epoch:1105 \t loss: 0.023670\n",
      "Epoch:1106 \t loss: 0.093770\n",
      "Epoch:1107 \t loss: 0.013643\n",
      "Epoch:1108 \t loss: 0.001633\n",
      "Epoch:1109 \t loss: 0.053332\n",
      "Epoch:1110 \t loss: 0.028456\n",
      "Epoch:1111 \t loss: 0.045225\n",
      "Epoch:1112 \t loss: 0.079601\n",
      "Epoch:1113 \t loss: 0.111531\n",
      "Epoch:1114 \t loss: 0.032691\n",
      "Epoch:1115 \t loss: 0.169463\n",
      "Epoch:1116 \t loss: 0.052535\n",
      "Epoch:1117 \t loss: 0.108989\n",
      "Epoch:1118 \t loss: 0.093723\n",
      "Epoch:1119 \t loss: 0.103328\n",
      "Epoch:1120 \t loss: 0.022516\n",
      "Epoch:1121 \t loss: 0.020876\n",
      "Epoch:1122 \t loss: 0.079544\n",
      "Epoch:1123 \t loss: 0.034323\n",
      "Epoch:1124 \t loss: 0.037882\n",
      "Epoch:1125 \t loss: 0.094338\n",
      "Epoch:1126 \t loss: 0.000265\n",
      "Epoch:1127 \t loss: 0.027089\n",
      "Epoch:1128 \t loss: 0.053339\n",
      "Epoch:1129 \t loss: 0.096115\n",
      "Epoch:1130 \t loss: 0.074853\n",
      "Epoch:1131 \t loss: 0.063224\n",
      "Epoch:1132 \t loss: 0.054834\n",
      "Epoch:1133 \t loss: 0.065383\n",
      "Epoch:1134 \t loss: 0.002148\n",
      "Epoch:1135 \t loss: 0.077837\n",
      "Epoch:1136 \t loss: 0.000306\n",
      "Epoch:1137 \t loss: 0.046283\n",
      "Epoch:1138 \t loss: 0.059797\n",
      "Epoch:1139 \t loss: 0.056456\n",
      "Epoch:1140 \t loss: 0.066986\n",
      "Epoch:1141 \t loss: 0.024368\n",
      "Epoch:1142 \t loss: 0.064429\n",
      "Epoch:1143 \t loss: 0.070610\n",
      "Epoch:1144 \t loss: 0.000117\n",
      "Epoch:1145 \t loss: 0.025934\n",
      "Epoch:1146 \t loss: 0.082583\n",
      "Epoch:1147 \t loss: 0.060760\n",
      "Epoch:1148 \t loss: 0.074757\n",
      "Epoch:1149 \t loss: 0.064035\n",
      "Epoch:1150 \t loss: 0.030423\n",
      "Epoch:1151 \t loss: 0.037910\n",
      "Epoch:1152 \t loss: 0.058358\n",
      "Epoch:1153 \t loss: 0.131000\n",
      "Epoch:1154 \t loss: 0.082758\n",
      "Epoch:1155 \t loss: 0.012867\n",
      "Epoch:1156 \t loss: 0.112733\n",
      "Epoch:1157 \t loss: 0.037907\n",
      "Epoch:1158 \t loss: 0.036053\n",
      "Epoch:1159 \t loss: 0.057481\n",
      "Epoch:1160 \t loss: 0.028075\n",
      "Epoch:1161 \t loss: 0.092588\n",
      "Epoch:1162 \t loss: 0.042249\n",
      "Epoch:1163 \t loss: 0.071351\n",
      "Epoch:1164 \t loss: 0.058451\n",
      "Epoch:1165 \t loss: 0.034336\n",
      "Epoch:1166 \t loss: 0.016984\n",
      "Epoch:1167 \t loss: 0.000148\n",
      "Epoch:1168 \t loss: 0.015048\n",
      "Epoch:1169 \t loss: 0.075451\n",
      "Epoch:1170 \t loss: 0.074486\n",
      "Epoch:1171 \t loss: 0.043209\n",
      "Epoch:1172 \t loss: 0.094752\n",
      "Epoch:1173 \t loss: 0.153696\n",
      "Epoch:1174 \t loss: 0.064401\n",
      "Epoch:1175 \t loss: 0.007045\n",
      "Epoch:1176 \t loss: 0.009366\n",
      "Epoch:1177 \t loss: 0.089444\n",
      "Epoch:1178 \t loss: 0.119440\n",
      "Epoch:1179 \t loss: 0.067079\n",
      "Epoch:1180 \t loss: 0.026898\n",
      "Epoch:1181 \t loss: 0.044813\n",
      "Epoch:1182 \t loss: 0.080826\n",
      "Epoch:1183 \t loss: 0.062309\n",
      "Epoch:1184 \t loss: 0.040114\n",
      "Epoch:1185 \t loss: 0.019621\n",
      "Epoch:1186 \t loss: 0.060834\n",
      "Epoch:1187 \t loss: 0.038573\n",
      "Epoch:1188 \t loss: 0.069867\n",
      "Epoch:1189 \t loss: 0.100863\n",
      "Epoch:1190 \t loss: 0.037121\n",
      "Epoch:1191 \t loss: 0.091312\n",
      "Epoch:1192 \t loss: 0.088511\n",
      "Epoch:1193 \t loss: 0.066260\n",
      "Epoch:1194 \t loss: 0.023884\n",
      "Epoch:1195 \t loss: 0.039873\n",
      "Epoch:1196 \t loss: 0.082737\n",
      "Epoch:1197 \t loss: 0.086368\n",
      "Epoch:1198 \t loss: 0.071663\n",
      "Epoch:1199 \t loss: 0.069960\n",
      "Epoch:1200 \t loss: 0.029359\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = [], []\n",
    "\n",
    "if DO_NSP_TEST :\n",
    "    train_samples, test_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "else :\n",
    "    train_samples = all_samples\n",
    "\n",
    "    # the maximum of length of extents\n",
    "object_max_len = 91 # longest extents is 8\n",
    "# the number of tokens objects\n",
    "object_max_vocab = 473    \n",
    "\n",
    "batch_data = make_data(extent_token_train, train_samples, object2idx, object_max_vocab )\n",
    "\n",
    "batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)]\n",
    "dataset = BERTDataset(*batch_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = BERT(n_layers,object_max_vocab,object_max_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bat = 0\n",
    "    for one_batch in dataloader:\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch]\n",
    "\n",
    "        logits_cls, logits_lm, _ = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        loss_cls = criterion(logits_cls, is_next)\n",
    "        loss_lm = criterion(logits_lm.view(-1, object_max_vocab), masked_tokens.view(-1))\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        bat += 1\n",
    "        if bat % 500 == 0 :\n",
    "            print(f'Epoch:{epoch} Batch:{bat}\\t loss: {loss:.6f}')\n",
    "            torch.save(model.state_dict(), 'oo_no_pos_pretrained.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
