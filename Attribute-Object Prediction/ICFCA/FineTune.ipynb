{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the maximum number of masked tokens\n",
    "max_pred = 4\n",
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 64\n",
    "# dimension of embedding\n",
    "d_model = 768  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 12\n",
    "# number of encoders\n",
    "n_layers = 6\n",
    "# the number of input setences\n",
    "n_segs = 2\n",
    "\n",
    "p_dropout = .1\n",
    "\n",
    "#80% the chosen token is replaced by [mask], 10% is replaced by a random token, 10% do nothing\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in word2idx\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    # The size of pad_mask is [batch, seq_len, seq_len]\n",
    "    # The resulting tensor has True where padding tokens are located and False elsewhere.\n",
    "    \n",
    "    # print(f'the shape of pad_mask is {pad_mask.shape}')\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, max_vocab, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        word_enc = self.word_emb(x)\n",
    "        \n",
    "        '''\n",
    "        maybe positional embedding can be deleted\n",
    "        '''\n",
    "        # positional embedding\n",
    "        # pos = torch.arange(x.shape[1], dtype=torch.long, device=device) # .long: round down\n",
    "        # pos = pos.unsqueeze(0).expand_as(x) # the shape is [1, seq_len]\n",
    "        # pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# pre-LN is easier to train than post-LN, but if fully training, post_LN have better result than pre-LN. \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
    "\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The fullly connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, max_vocab, max_len):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embeddings(max_vocab, max_len)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "        \n",
    "        # next sentence prediction. output is 0 or 1.\n",
    "        self.next_cls = nn.Linear(d_model, 2)\n",
    "        self.gelu = gelu\n",
    "        \n",
    "        # Sharing weight between some fully connect layer, this will make training easier.\n",
    "        shared_weight = self.pooler.fc.weight\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.fc.weight = shared_weight\n",
    "\n",
    "        shared_weight = self.embedding.word_emb.weight\n",
    "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
    "        self.word_classifier.weight = shared_weight\n",
    "\n",
    "    def forward(self, tokens, segments, masked_pos):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "\n",
    "        # NSP Task\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        logits_cls = self.next_cls(hidden_pool)\n",
    "\n",
    "        # Masked Language Model Task\n",
    "        '''\n",
    "        extracting representations of masked positions, \n",
    "        passing them through a fully connected layer, \n",
    "        applying the GELU activation function, \n",
    "        and making predictions using the word classifier\n",
    "        '''\n",
    "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
    "\n",
    "        # h_masked: [batch, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
    "        h_masked = self.gelu(self.fc(h_masked))\n",
    "        logits_lm = self.word_classifier(h_masked)\n",
    "        # logits_lm: [batch, max_pred, max_vocab]\n",
    "        # logits_cls: [batch, 2]\n",
    "\n",
    "        return logits_cls, logits_lm, hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9177c2-dcb1-4500-bf67-e0f7ee0d286a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14012f5d-fc34-42ba-a496-1bd61287973d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n"
     ]
    }
   ],
   "source": [
    " '''\n",
    "Extract all extents, modify the form of extents as \"o1,o2,...\" named as modified_extents\n",
    "Change objects to indices in extents, named as extent_token_list. It is a list of INDICES not objects!\n",
    "Indices of objects and special tokens are from 1 to 338\n",
    "'''\n",
    "def process_train_extents_from_file(filename, max_vocab) :\n",
    "    extents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) >= 2:\n",
    "                extent = parts[1].strip()\n",
    "                extents.append(extent)\n",
    "    # print(\"The number of concepts is\",len(extents))\n",
    "    object_list = list(set(\" \".join(extents).split()))\n",
    "    sorted_object_list = sorted(map(int, object_list))\n",
    "    # print(\"The number of objects is \",len(sorted_object_list))\n",
    "    \n",
    "    # Create the object2idx dictionary\n",
    "    object2idx = {'o' + str(obj): int(obj)  for  obj in sorted_object_list}\n",
    "    sorted_object_list = list(map(str, sorted_object_list ))\n",
    "    # print(sorted_object_list)\n",
    "    special_tokens = {'[PAD]': max_vocab - 4, '[CLS]': max_vocab - 3, '[SEP]': max_vocab - 2, '[MASK]': max_vocab - 1}\n",
    "\n",
    "    object2idx.update(special_tokens)\n",
    "    # print(object2idx) \n",
    "\n",
    "    idx2object = {idx: object for object, idx in object2idx.items()}\n",
    "    vocab_size = len(object2idx)\n",
    "    assert len(object2idx) == len(idx2object)\n",
    "    \n",
    "    modified_extents = [' '.join(['o' + token for token in item.split()]) for item in extents]\n",
    "\n",
    "    # print(len(modified_extents))\n",
    "    \n",
    "    extent_token_list = []\n",
    "    for extent in modified_extents:\n",
    "        extent_token_list.append([\n",
    "            object2idx[s] for s in extent.split(' ')\n",
    "        ])\n",
    "\n",
    "    # print(len(extent_token_list))\n",
    "    return extent_token_list, object2idx, modified_extents, sorted_object_list\n",
    "\n",
    "max_vocab_object = 356\n",
    "# extent_token_test , object2idx , modified_extents , test_object_list  = process_train_extents_from_file('icfca-context_concepts.txt')\n",
    "extent_token_train, object2idx , modified_extents_train, train_object_list  = process_train_extents_from_file('icfca-context-with-missing-part_concepts.txt', max_vocab_object)\n",
    "print(len(extent_token_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6622ee2-899f-4f3f-86d8-442913d0d499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " '''\n",
    "Extract all intents, modify the form of intents as \"a1,a2,...\" named as modified_intents\n",
    "Change attributes to indices in intents , named as intent_token_list. It is a list of INDICES not attributes!\n",
    "Indices of attributes are from 399  (int(attribute)+398)\n",
    "\n",
    "Notice that index will not be continuous if some attributes are not in the reduced formal context!\n",
    "'''\n",
    "def process_train_intents_from_file(filename) :\n",
    "    intents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) <= 2:\n",
    "                intent = parts[0].strip()\n",
    "                intents.append(intent)\n",
    "    intents = intents[1:]\n",
    "    attribute_list = list(set(\" \".join(intents).split()))\n",
    "    sorted_attribute_list = sorted(map(int, attribute_list))\n",
    "    # print(\"The number of attributes is\",len(sorted_attribute_list))\n",
    "    \n",
    "    # Create the object2idx dictionary\n",
    "    attribute2idx = {'a' + str(attri): int(attri) + 351  for  attri in sorted_attribute_list}\n",
    "    sorted_attribute_list = list(map(str, sorted_attribute_list ))\n",
    "    # print(sorted_attribute_list)\n",
    "    # print(attribute2idx)\n",
    "\n",
    "\n",
    "    idx2attribute = {idx: attribute for attribute, idx in attribute2idx.items()}\n",
    "    vocab_size = len(attribute2idx)\n",
    "    assert len(attribute2idx) == len(idx2attribute)\n",
    "\n",
    "    modified_intents = [' '.join(['a' + token for token in item.split()]) for item in intents]\n",
    "    # print(intents)\n",
    "    # print(modified_intents)\n",
    "    \n",
    "    intent_token_list = []\n",
    "    for intent in modified_intents:\n",
    "        intent_token_list.append([\n",
    "            attribute2idx[s] for s in intent.split()\n",
    "        ])\n",
    "        \n",
    "    return intent_token_list, attribute2idx, modified_intents, sorted_attribute_list\n",
    "\n",
    "# intent_token_test, attribute2idx, modified_intents = process_test_intents_from_file('icfca-context_concepts.txt') \n",
    "intent_token_train, attribute2idx, modified_intents_train, train_attribute_list = process_train_intents_from_file('icfca-context-with-missing-part_concepts.txt')\n",
    "\n",
    "concept2idx = object2idx | attribute2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d56dd2-3618-450c-9225-04674cfc2eb7",
   "metadata": {},
   "source": [
    "# Object-Attribute Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156cfdc-170a-464f-ad84-cd7576577dcc",
   "metadata": {},
   "source": [
    "# Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b4c1467-729a-4afd-a9e7-4827056a4454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extents = []\n",
    "intents = []\n",
    "extent_token_new = []\n",
    "intent_token_new = []\n",
    "\n",
    "with open('icfca-context_concepts.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Split the line based on four blank spaces\n",
    "        parts = line.split('    ')\n",
    "\n",
    "        # Extract the right sequence (assuming it's the second part after splitting)\n",
    "        if len(parts) >= 2:\n",
    "            extent = parts[1].strip()\n",
    "            extents.append(extent)\n",
    "            intent = parts[0].strip()\n",
    "            intents.append(intent)\n",
    "            \n",
    "# print(object2idx.keys())\n",
    "for extent_str, intent_str in zip(extents, intents) :\n",
    "    extent_list = extent_str.split(' ')\n",
    "    intent_list = intent_str.split(' ')\n",
    "    \n",
    "    extent_tokens = []\n",
    "    intent_tokens = []\n",
    "    \n",
    "    for obj in extent_list :\n",
    "        if 'o' + obj in object2idx.keys() :\n",
    "            extent_tokens.append(object2idx['o' + obj])\n",
    "        \n",
    "    for attr in intent_list :\n",
    "        if 'a' + attr in attribute2idx.keys() :\n",
    "            intent_tokens.append(attribute2idx['a' + attr])\n",
    "        \n",
    "    extent_token_new.append(extent_tokens)\n",
    "    intent_token_new.append(intent_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256c64-5c39-4e0d-8c3f-7e9e294f2bcc",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7ec4a-ee80-4db1-8e40-e57e1e780117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe06b24e-1cd3-4a63-879c-eec2a1fc44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0. 0. 0. 1.]\n",
      "4\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def get_true_permute_pairs(extent_token_list, intent_token_list, tup_len = 3) :\n",
    "    true_permute_pairs = []\n",
    "    object_permutes = []\n",
    "    attribute_permutes = []\n",
    "    \n",
    "    distribution = np.zeros(shape = (tup_len + 1, tup_len + 1), dtype = np.float32)\n",
    "    \n",
    "    for extent, intent in zip(extent_token_list, intent_token_list) :\n",
    "        extent_len = len(extent)\n",
    "        intent_len = len(intent)\n",
    "        \n",
    "        for obj_len in range(1, tup_len + 1) :\n",
    "            if extent_len >= obj_len :\n",
    "                obj_pmts = [' '.join([str(ele) for ele in list(p)] + ['0' for _ in range(tup_len - obj_len)]) for p in itertools.permutations(extent, obj_len)]\n",
    "            else :\n",
    "                obj_pmts = []\n",
    "                \n",
    "            for obj_pmt in obj_pmts :\n",
    "                for attr_len in range(1, tup_len + 1) :\n",
    "                    if intent_len >= attr_len :\n",
    "                        attr_pmts = [' '.join([str(ele) for ele in list(p)] + ['0' for _ in range(tup_len - attr_len)]) for p in itertools.permutations(intent, attr_len)]\n",
    "                    else :\n",
    "                        attr_pmts\n",
    "                \n",
    "                    for attr_pmt in attr_pmts :\n",
    "                        true_permute_pairs.append(obj_pmt + '-' + attr_pmt)\n",
    "                        distribution[obj_len][attr_len] += 1\n",
    "\n",
    "    true_permute_pairs = set(true_permute_pairs)\n",
    "    \n",
    "    return true_permute_pairs, distribution\n",
    "\n",
    "def pad_negative_samples(object2idx, attribute2idx, true_permute_pairs, length_distribution, number) :\n",
    "    tup_len = length_distribution.shape[0] - 1\n",
    "    lengths = length_distribution.shape[0] * length_distribution.shape[1]\n",
    "    length_distribution = length_distribution.reshape(-1)\n",
    "\n",
    "    print(lengths)\n",
    "    print(length_distribution)\n",
    "    selected_permute_pairs = set([])\n",
    "    \n",
    "    object_full_list = []\n",
    "    for obj in object2idx :\n",
    "        if not '[' in obj :\n",
    "            object_full_list.append(object2idx[obj])\n",
    "    attribute_full_list = []\n",
    "    for attr in attribute2idx :\n",
    "        attribute_full_list.append(attribute2idx[attr])\n",
    "    \n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < number :\n",
    "        length_id = np.random.choice(lengths, p=length_distribution)\n",
    "        obj_length = int(length_id / (tup_len + 1))\n",
    "        attr_length = length_id % (tup_len + 1)\n",
    "\n",
    "        obj_list = random.sample(object_full_list, obj_length)\n",
    "        attr_list = random.sample(attribute_full_list, attr_length)\n",
    "        \n",
    "        if obj_length < tup_len :\n",
    "            obj_list.extend([0 for _ in range(tup_len - obj_length)])\n",
    "        if attr_length < tup_len :\n",
    "            attr_list.extend([0 for _ in range(tup_len - attr_length)])\n",
    "        \n",
    "        tmp_str = ' '.join([str(x) for x in obj_list]) + '-' + ' '.join([str(y) for y in attr_list])\n",
    "        \n",
    "        if tmp_str in true_permute_pairs :\n",
    "            continue\n",
    "            \n",
    "        selected_permute_pairs.add(tmp_str) #  防止抽到重复的负样本\n",
    "        \n",
    "        negative_samples.append((obj_list, attr_list, False))\n",
    "    return negative_samples, selected_permute_pairs\n",
    "\n",
    "def prepare_list_data(object2idx, attribute2idx, extent_token_list, extent_token_list_new, intent_token_list, intent_token_list_new, tup_len = 3) :\n",
    "    old_true_permute_pairs, old_distribution = get_true_permute_pairs(extent_token_list, intent_token_list, tup_len)\n",
    "    new_true_permute_pairs, new_distribution = get_true_permute_pairs(extent_token_list_new, intent_token_list_new, tup_len)\n",
    "    \n",
    "    added_true_permute_pairs = new_true_permute_pairs - old_true_permute_pairs\n",
    "    added_distribution = new_distribution - old_distribution\n",
    "    \n",
    "    old_distribution /= np.sum(old_distribution)\n",
    "    added_distribution /= np.sum(added_distribution)\n",
    "    \n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "    \n",
    "    for perm_str in old_true_permute_pairs :\n",
    "        perm_substrs = perm_str.split('-')\n",
    "        obj_list = [int(x) for x in perm_substrs[0].split(' ')]\n",
    "        attr_list = [int(x) for x in perm_substrs[1].split(' ')]\n",
    "        train_samples.append((obj_list, attr_list, True))\n",
    "        \n",
    "    for perm_str in added_true_permute_pairs :\n",
    "        perm_substrs = perm_str.split('-')\n",
    "        obj_list = [int(x) for x in perm_substrs[0].split(' ')]\n",
    "        attr_list = [int(x) for x in perm_substrs[1].split(' ')]\n",
    "        test_samples.append((obj_list, attr_list, True))\n",
    "    \n",
    "    train_len = len(train_samples)\n",
    "    test_len = len(test_samples)\n",
    "    \n",
    "    train_negative_samples, selected_permute_pairs = pad_negative_samples(object2idx, attribute2idx, old_true_permute_pairs, old_distribution, train_len)\n",
    "    test_negative_samples, selected_permute_pairs_train = pad_negative_samples(object2idx, attribute2idx, old_true_permute_pairs.union(selected_permute_pairs), added_distribution, test_len)\n",
    "\n",
    "    train_samples.extend(train_negative_samples)\n",
    "    test_samples.extend(test_negative_samples)\n",
    "    \n",
    "    random.shuffle(train_samples)\n",
    "    random.shuffle(test_samples)\n",
    "    \n",
    "    return train_samples, test_samples\n",
    "\n",
    "tup_len = 1\n",
    "\n",
    "train_labeled_lists, test_labeled_lists = prepare_list_data(object2idx, attribute2idx, extent_token_train, extent_token_new, intent_token_train, intent_token_new, tup_len = tup_len)\n",
    "# print(len(train_labeled_lists))\n",
    "# print(len(test_labeled_lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44006f7b-b58f-43b5-ba7b-32dc2ce5e780",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef9943-2c2e-4e63-9d0e-4d98973caddb",
   "metadata": {},
   "source": [
    "##  MLP for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1afb7f61-7896-43af-bcab-e39ed0914367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design a MLP for classification task\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, object_pretrained_model, attribute_pretrained_model, embedding_size, hidden_size, output_size, dropout_rate = .1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.bert_object = object_pretrained_model\n",
    "        self.bert_attribute = attribute_pretrained_model\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs_object, segments_object, masked_poses_object,inputs_attribute, segments_attribute, masked_poses_attribute):\n",
    "        _, __, x1 = self.bert_object(inputs_object, segments_object, masked_poses_object)\n",
    "        _, __, x2 = self.bert_attribute(inputs_attribute, segments_attribute, masked_poses_attribute)\n",
    "        x = self.fc1(torch.cat((x1,x2), dim=1))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49621787-09c0-42bb-a2a5-49168298ac72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for extent, intent, label in samples:\n",
    "        inputs.append(extent + intent)\n",
    "        labels.append(label)\n",
    "    return torch.tensor(inputs), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d50d109-a6e5-41c6-a451-43391b326a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_size = 2 * 768\n",
    "hidden_size = 262\n",
    "output_size = 1\n",
    "learning_rate = 1.4e-6\n",
    "num_epochs = 120\n",
    "batch_size = 128\n",
    "\n",
    "# 加载旧的模型\n",
    "# the maximum of length of extents\n",
    "object_max_len = 19 # longest extents is 8\n",
    "# the number of tokens objects\n",
    "object_max_vocab = 356    \n",
    "object_pretrained_model = BERT(n_layers, object_max_vocab, object_max_len)\n",
    "\n",
    "# the maximum of length of sequences\n",
    "attribute_max_len = 1067\n",
    "# the number of tokens (objects or attributes)\n",
    "attribute_max_vocab = 12974\n",
    "attribute_pretrained_model = BERT(n_layers, attribute_max_vocab, attribute_max_len)\n",
    "\n",
    "object_pretrained_model.load_state_dict(torch.load('no_pos_object_pretrained.dat'))\n",
    "attribute_pretrained_model.load_state_dict(torch.load('no_pos_attribute_pretrained.dat'))\n",
    "\n",
    "object_pretrained_model.train()\n",
    "attribute_pretrained_model.train()\n",
    "\n",
    "#object_pretrained_model.eval()\n",
    "#attribute_pretrained_model.eval()\n",
    "\n",
    "object_pretrained_model.to(device)\n",
    "attribute_pretrained_model.to(device)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MLP_model = MLP(object_pretrained_model, attribute_pretrained_model, d_model, hidden_size, output_size, dropout_rate=0.1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to device\n",
    "MLP_model = MLP_model.to(device)\n",
    "\n",
    "# Prepare the data\n",
    "train_inputs, train_labels = prepare_data(train_labeled_lists)\n",
    "test_inputs, test_labels = prepare_data(test_labeled_lists)\n",
    "\n",
    "train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fa74-69ff-4214-b7da-94c646012c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.36it/s, loss=0.645]\n",
      "Epoch 2/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.20it/s, loss=0.531]\n",
      "Epoch 3/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.78it/s, loss=0.548]\n",
      "Epoch 4/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.53it/s, loss=0.433]\n",
      "Epoch 5/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.91it/s, loss=0.535]\n",
      "Epoch 6/120: 100%|████████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.61it/s, loss=0.49]\n",
      "Epoch 7/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.03it/s, loss=0.513]\n",
      "Epoch 8/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.12it/s, loss=0.455]\n",
      "Epoch 9/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.20it/s, loss=0.539]\n",
      "Epoch 10/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.97it/s, loss=0.567]\n",
      "Epoch 11/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.15it/s, loss=0.56]\n",
      "Epoch 12/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.07it/s, loss=0.511]\n",
      "Epoch 13/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.95it/s, loss=0.416]\n",
      "Epoch 14/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.83it/s, loss=0.356]\n",
      "Epoch 15/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.93it/s, loss=0.467]\n",
      "Epoch 16/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.14it/s, loss=0.515]\n",
      "Epoch 17/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.31it/s, loss=0.512]\n",
      "Epoch 18/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 28.53it/s, loss=0.516]\n",
      "Epoch 19/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.06it/s, loss=0.516]\n",
      "Epoch 20/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.97it/s, loss=0.485]\n",
      "Epoch 21/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.41it/s, loss=0.55]\n",
      "Epoch 22/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.29it/s, loss=0.408]\n",
      "Epoch 23/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.77it/s, loss=0.408]\n",
      "Epoch 24/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.82it/s, loss=0.525]\n",
      "Epoch 25/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.21it/s, loss=0.468]\n",
      "Epoch 26/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.33it/s, loss=0.394]\n",
      "Epoch 27/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.54it/s, loss=0.453]\n",
      "Epoch 28/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.20it/s, loss=0.46]\n",
      "Epoch 29/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.15it/s, loss=0.366]\n",
      "Epoch 30/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.38it/s, loss=0.451]\n",
      "Epoch 31/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.37it/s, loss=0.421]\n",
      "Epoch 32/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.61it/s, loss=0.414]\n",
      "Epoch 33/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:08<00:00, 28.20it/s, loss=0.485]\n",
      "Epoch 34/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.30it/s, loss=0.427]\n",
      "Epoch 35/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.29it/s, loss=0.453]\n",
      "Epoch 36/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.08it/s, loss=0.316]\n",
      "Epoch 37/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.28it/s, loss=0.332]\n",
      "Epoch 38/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.30it/s, loss=0.392]\n",
      "Epoch 39/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.92it/s, loss=0.395]\n",
      "Epoch 40/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.96it/s, loss=0.484]\n",
      "Epoch 41/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.55it/s, loss=0.415]\n",
      "Epoch 42/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.37it/s, loss=0.365]\n",
      "Epoch 43/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.36it/s, loss=0.405]\n",
      "Epoch 44/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 34.16it/s, loss=0.334]\n",
      "Epoch 45/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.97it/s, loss=0.343]\n",
      "Epoch 46/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.76it/s, loss=0.244]\n",
      "Epoch 47/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.74it/s, loss=0.414]\n",
      "Epoch 48/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.95it/s, loss=0.435]\n",
      "Epoch 49/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.96it/s, loss=0.383]\n",
      "Epoch 50/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.20it/s, loss=0.33]\n",
      "Epoch 51/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.38it/s, loss=0.338]\n",
      "Epoch 52/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.77it/s, loss=0.36]\n",
      "Epoch 53/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.37it/s, loss=0.304]\n",
      "Epoch 54/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.99it/s, loss=0.375]\n",
      "Epoch 55/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.57it/s, loss=0.418]\n",
      "Epoch 56/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.22it/s, loss=0.365]\n",
      "Epoch 57/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.29it/s, loss=0.334]\n",
      "Epoch 58/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.07it/s, loss=0.347]\n",
      "Epoch 59/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.66it/s, loss=0.349]\n",
      "Epoch 60/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.73it/s, loss=0.309]\n",
      "Epoch 61/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.56it/s, loss=0.329]\n",
      "Epoch 62/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.48it/s, loss=0.367]\n",
      "Epoch 63/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.41it/s, loss=0.278]\n",
      "Epoch 64/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.61it/s, loss=0.242]\n",
      "Epoch 65/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.61it/s, loss=0.338]\n",
      "Epoch 66/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.07it/s, loss=0.335]\n",
      "Epoch 67/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.67it/s, loss=0.309]\n",
      "Epoch 68/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.32it/s, loss=0.273]\n",
      "Epoch 69/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.69it/s, loss=0.284]\n",
      "Epoch 70/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.81it/s, loss=0.32]\n",
      "Epoch 71/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.88it/s, loss=0.374]\n",
      "Epoch 72/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.85it/s, loss=0.342]\n",
      "Epoch 73/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.33it/s, loss=0.297]\n",
      "Epoch 74/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 34.01it/s, loss=0.285]\n",
      "Epoch 75/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.91it/s, loss=0.414]\n",
      "Epoch 76/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.21it/s, loss=0.267]\n",
      "Epoch 77/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.07it/s, loss=0.274]\n",
      "Epoch 78/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.12it/s, loss=0.235]\n",
      "Epoch 79/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.67it/s, loss=0.283]\n",
      "Epoch 80/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.48it/s, loss=0.228]\n",
      "Epoch 81/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.94it/s, loss=0.288]\n",
      "Epoch 82/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.74it/s, loss=0.246]\n",
      "Epoch 83/120: 100%|███████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.97it/s, loss=0.33]\n",
      "Epoch 84/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.77it/s, loss=0.246]\n",
      "Epoch 85/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.07it/s, loss=0.255]\n",
      "Epoch 86/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.72it/s, loss=0.256]\n",
      "Epoch 87/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.05it/s, loss=0.256]\n",
      "Epoch 88/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.97it/s, loss=0.242]\n",
      "Epoch 89/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.96it/s, loss=0.282]\n",
      "Epoch 90/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 28.93it/s, loss=0.275]\n",
      "Epoch 91/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 32.27it/s, loss=0.339]\n",
      "Epoch 92/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.50it/s, loss=0.311]\n",
      "Epoch 93/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.44it/s, loss=0.376]\n",
      "Epoch 94/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.20it/s, loss=0.233]\n",
      "Epoch 95/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.62it/s, loss=0.183]\n",
      "Epoch 96/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.64it/s, loss=0.247]\n",
      "Epoch 97/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.60it/s, loss=0.287]\n",
      "Epoch 98/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.20it/s, loss=0.233]\n",
      "Epoch 99/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.33it/s, loss=0.213]\n",
      "Epoch 100/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.42it/s, loss=0.25]\n",
      "Epoch 101/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:06<00:00, 32.37it/s, loss=0.286]\n",
      "Epoch 102/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.97it/s, loss=0.237]\n",
      "Epoch 103/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 29.85it/s, loss=0.216]\n",
      "Epoch 104/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:06<00:00, 33.11it/s, loss=0.278]\n",
      "Epoch 105/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 28.87it/s, loss=0.277]\n",
      "Epoch 106/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.93it/s, loss=0.189]\n",
      "Epoch 107/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.26it/s, loss=0.155]\n",
      "Epoch 108/120: 100%|██████████████████████████████████████████████████████| 226/226 [00:07<00:00, 30.54it/s, loss=0.24]\n",
      "Epoch 109/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.34it/s, loss=0.271]\n",
      "Epoch 110/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 28.68it/s, loss=0.287]\n",
      "Epoch 111/120: 100%|█████████████████████████████████████████████████████| 226/226 [00:07<00:00, 31.92it/s, loss=0.239]\n",
      "Epoch 112/120:  78%|█████████████████████████████████████████▎           | 176/226 [00:06<00:01, 35.05it/s, loss=0.284]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n",
    "\n",
    "    for inputs, labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        extents, intents = torch.tensor_split(inputs, [tup_len], dim=1)\n",
    "        \n",
    "        object_segments = torch.tensor([[0 for _ in i] for i in extents])\n",
    "        object_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in extents])\n",
    "        \n",
    "        attribute_segments = torch.tensor([[0 for _ in i] for i in intents])\n",
    "        attribute_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in intents])\n",
    "        \n",
    "        extents, intents, labels = extents.to(device), intents.to(device), labels.to(device)\n",
    "        object_segments, object_masked_poses = object_segments.to(device), object_masked_poses.to(device)\n",
    "        attribute_segments, attribute_masked_poses = attribute_segments.to(device), attribute_masked_poses.to(device)\n",
    "        \n",
    "        outputs = MLP_model(extents, object_segments, object_masked_poses, intents, attribute_segments, attribute_masked_poses)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm with the current loss\n",
    "        pbar.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3956915-e016-4c94-8b83-552bf63ad6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.897\n",
      "Precision: 0.917\n",
      "Recall: 0.873\n",
      "F1 Score: 0.894\n",
      "AUC: 0.971\n"
     ]
    }
   ],
   "source": [
    "MLP_model.eval()\n",
    "train_extents, train_intents = torch.tensor_split(train_inputs, [tup_len], dim=1)\n",
    "\n",
    "object_segments = torch.tensor([[0 for _ in i] for i in train_extents])\n",
    "object_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in train_extents])\n",
    "\n",
    "attribute_segments = torch.tensor([[0 for _ in i] for i in train_intents])\n",
    "attribute_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in train_intents])\n",
    "\n",
    "object_segments = object_segments.to(device)\n",
    "attribute_segments = attribute_segments.to(device)\n",
    "\n",
    "object_masked_poses = object_masked_poses.to(device)\n",
    "attribute_masked_poses = attribute_masked_poses.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = MLP_model(train_extents, object_segments, object_masked_poses, train_intents, attribute_segments, attribute_masked_poses)\n",
    "    predictions = (train_outputs > 0.5).float().cpu().numpy()\n",
    "    train_labels_numpy = train_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(train_labels_numpy, predictions_binary)\n",
    "precision = precision_score(train_labels_numpy, predictions_binary)\n",
    "recall = recall_score(train_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(train_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(train_labels_numpy, train_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3576624d-bdf2-4f5a-bf23-d3a1e5d82564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7021\n",
      "Precision: 0.6877\n",
      "Recall: 0.7406\n",
      "F1 Score: 0.7132\n",
      "AUC: 0.7735\n",
      "AUPR: 0.7380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "# ... (previous code)\n",
    "MLP_model.eval()\n",
    "\n",
    "test_extents, test_intents = torch.tensor_split(test_inputs, [tup_len], dim=1)\n",
    "\n",
    "object_segments = torch.tensor([[0 for _ in i] for i in test_extents])\n",
    "object_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in test_extents])\n",
    "\n",
    "attribute_segments = torch.tensor([[0 for _ in i] for i in test_intents])\n",
    "attribute_masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in test_intents])\n",
    "\n",
    "object_segments = object_segments.to(device)\n",
    "attribute_segments = attribute_segments.to(device)\n",
    "\n",
    "object_masked_poses = object_masked_poses.to(device)\n",
    "attribute_masked_poses = attribute_masked_poses.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = MLP_model(test_extents, object_segments, object_masked_poses, test_intents, attribute_segments, attribute_masked_poses)\n",
    "    predictions = (test_outputs > 0.46).float().cpu().numpy()\n",
    "    test_labels_numpy = test_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.46).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(test_labels_numpy, predictions_binary)\n",
    "precision = precision_score(test_labels_numpy, predictions_binary)\n",
    "recall = recall_score(test_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(test_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(test_labels_numpy, test_outputs.cpu().numpy())\n",
    "aupr = average_precision_score(test_labels_numpy, test_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'AUPR: {aupr:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec81a6-29f6-450a-8a7e-cc3ac74755b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
