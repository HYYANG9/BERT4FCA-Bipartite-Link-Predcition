{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the maximum number of masked tokens\n",
    "max_pred = 4\n",
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 64\n",
    "# dimension of embedding\n",
    "d_model = 768  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 12\n",
    "# number of encoders\n",
    "n_layers = 6\n",
    "# the number of input setences\n",
    "n_segs = 2\n",
    "\n",
    "p_dropout = .1\n",
    "\n",
    "#80% the chosen token is replaced by [mask], 10% is replaced by a random token, 10% do nothing\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in word2idx\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    # The size of pad_mask is [batch, seq_len, seq_len]\n",
    "    # The resulting tensor has True where padding tokens are located and False elsewhere.\n",
    "    \n",
    "    # print(f'the shape of pad_mask is {pad_mask.shape}')\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,max_vocab, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        # print(\"Input to Embeddings.forward - x:\", x.size())\n",
    "        word_enc = self.word_emb(x)\n",
    "        # print(\"Output from Embeddings.forward - word_enc:\", word_enc.size())\n",
    "        '''\n",
    "        maybe positional embedding can be deleted\n",
    "        '''\n",
    "        \n",
    "        # positional embedding\n",
    "        # pos = torch.arange(x.shape[1], dtype=torch.long, device=device) # .long: round down\n",
    "        # pos = pos.unsqueeze(0).expand_as(x) # the shape is [1, seq_len]\n",
    "        # pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# pre-LN is easier to train than post-LN, but if fullly training, post_LN have better result than pre-LN. \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
    "\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The full connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, max_vocab, max_len):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embeddings(max_vocab, max_len)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "        \n",
    "        # next sentence prediction. output is 0 or 1.\n",
    "        self.next_cls = nn.Linear(d_model, 2)\n",
    "        self.gelu = gelu\n",
    "        \n",
    "        # Sharing weight between some fully connect layer, this will make training easier.\n",
    "        shared_weight = self.pooler.fc.weight\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.fc.weight = shared_weight\n",
    "\n",
    "        shared_weight = self.embedding.word_emb.weight\n",
    "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
    "        self.word_classifier.weight = shared_weight\n",
    "\n",
    "    def forward(self, tokens, segments, masked_pos):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "\n",
    "        # NSP Task\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        logits_cls = self.next_cls(hidden_pool)\n",
    "\n",
    "        # Masked Language Model Task\n",
    "        '''\n",
    "        extracting representations of masked positions, \n",
    "        passing them through a fully connected layer, \n",
    "        applying the GELU activation function, \n",
    "        and making predictions using the word classifier\n",
    "        '''\n",
    "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
    "\n",
    "        # h_masked: [batch, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
    "        h_masked = self.gelu(self.fc(h_masked))\n",
    "        logits_lm = self.word_classifier(h_masked)\n",
    "        # logits_lm: [batch, max_pred, max_vocab]\n",
    "        # logits_cls: [batch, 2]\n",
    "\n",
    "        return logits_cls, logits_lm, hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9177c2-dcb1-4500-bf67-e0f7ee0d286a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "这个函数process_concepts_from_file负责从filename这个文件读入所有concepts并抽取所有extents\n",
    "该函数返回extent_token_list，其类型为list，每一行代表一个extent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14012f5d-fc34-42ba-a496-1bd61287973d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " '''\n",
    "Extract all extents, modify the form of extents as \"o1,o2,...\" named as modified_extents\n",
    "Change objects to indices in extents, named as extent_token_list. It is a list of INDICES not objects!\n",
    "Indices of objects and special tokens are from 1 to 338\n",
    "'[PAD]': 12966, '[CLS]': 12967, '[SEP]': 12968, '[MASK]': 12969\n",
    "'''\n",
    "def process_train_extents_from_file(filename, max_vocab) :\n",
    "    extents = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            # Extract the right sequence (assuming it's the second part after splitting)\n",
    "            if len(parts) >= 2:\n",
    "                extent = parts[1].strip()\n",
    "                extents.append(extent)\n",
    "    # print(\"The number of concepts is\",len(extents))\n",
    "    object_list = list(set(\" \".join(extents).split()))\n",
    "    sorted_object_list = sorted(map(int, object_list))\n",
    "    # print(\"The number of objects is \",len(sorted_object_list))\n",
    "    \n",
    "    # Create the object2idx dictionary\n",
    "    object2idx = {'o' + str(obj): int(obj)  for  obj in sorted_object_list}\n",
    "    sorted_object_list = list(map(str, sorted_object_list ))\n",
    "    # print(sorted_object_list)\n",
    "    special_tokens = {'[PAD]': max_vocab-4, '[CLS]': max_vocab-3, '[SEP]': max_vocab-2, '[MASK]': max_vocab-1 }\n",
    "\n",
    "    object2idx.update(special_tokens)\n",
    "    # print(object2idx) \n",
    "\n",
    "    idx2object = {idx: object for object, idx in object2idx.items()}\n",
    "    vocab_size = len(object2idx)\n",
    "    assert len(object2idx) == len(idx2object)\n",
    "    \n",
    "    modified_extents = [' '.join(['o' + token for token in item.split()]) for item in extents]\n",
    "\n",
    "    # print(len(modified_extents))\n",
    "    \n",
    "    extent_token_list = []\n",
    "    for extent in modified_extents:\n",
    "        extent_token_list.append([\n",
    "            object2idx[s] for s in extent.split()\n",
    "        ])\n",
    "    # print(len(extent_token_list))\n",
    "    return extent_token_list, object2idx, modified_extents, sorted_object_list\n",
    "\n",
    "extent_token_train, object2idx , modified_extents_train, train_object_list  = process_train_extents_from_file('icfca-context-with-missing-part_concepts.txt',  356)\n",
    "# print(object2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f03f47b-2af5-4b2f-90fa-eb9939803580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# padding the token lists to have the same length.\n",
    "def padding(ids, n_pads, pad_symb=0):\n",
    "    return ids.extend([pad_symb for _ in range(n_pads)])\n",
    "\n",
    "def masking_procedure(cand_pos, input_ids, max_vocab, masked_symb='[MASK]'):\n",
    "    masked_pos = []\n",
    "    masked_tokens = []\n",
    "    for pos in cand_pos:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random.random() < p_mask:\n",
    "            input_ids[pos] = masked_symb\n",
    "        elif random.random() > (p_mask + p_replace):\n",
    "            rand_word_idx = random.randint(0, max_vocab-4)\n",
    "            input_ids[pos] = rand_word_idx\n",
    "\n",
    "    return masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a28666-9619-4be9-b41d-287b585b9d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n"
     ]
    }
   ],
   "source": [
    "def get_neighbor_samples(extents) :\n",
    "    n = len(extents)\n",
    "    samples = []\n",
    "\n",
    "    dep = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "    neighbor = np.zeros(shape = (n, n), dtype = np.int32)\n",
    "\n",
    "    for i in range(n) :\n",
    "        for j in range(i + 1, n) :\n",
    "            if set(extents[i]).issubset(set(extents[j])) :\n",
    "                dep[i][j] = 1\n",
    "            if set(extents[j]).issubset(set(extents[i])) :\n",
    "                dep[j][i] = 1\n",
    "\n",
    "    for i in range(n) :\n",
    "        se = set([])\n",
    "        for j in range(n) :\n",
    "            if j != i :\n",
    "                if dep[j][i] == 1 :\n",
    "                    rep = False\n",
    "                    lst = list(se)\n",
    "                    for idk, k in enumerate(lst) :\n",
    "                        if dep[k][j] :\n",
    "                            se.remove(k)\n",
    "                            se.add(j)\n",
    "                            rep = True\n",
    "                        if dep[j][k] :\n",
    "                            rep = True\n",
    "                    if not rep :\n",
    "                        se.add(j)\n",
    "\n",
    "        for j in range(n) :\n",
    "            if j in se :\n",
    "                samples.append([i, j, True])\n",
    "            elif random.random() < 0.0018 :\n",
    "                samples.append([i, j, False])\n",
    "        \n",
    "    return samples\n",
    "\n",
    "all_samples = get_neighbor_samples(extent_token_train)\n",
    "print(len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afe0534-d54a-4aa2-9aaa-384813468cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 将tokens（某个extent）排列组合后生成一个列表。\n",
    "# # 如果tokens的长度不超过thres，那么就生成全排列；\n",
    "# # 如果长度超过thres，就随机打乱顺序thres!次。\n",
    "# def get_permuted_token_list(tokens, thres = 4) :\n",
    "#     tokens_list = []\n",
    "#     if len(tokens) <= thres :\n",
    "#         permutations = itertools.permutations(tokens)\n",
    "#         tokens_list = [list(p) for p in permutations]\n",
    "#     else :\n",
    "#         for i in range(math.comb(thres, thres)) :\n",
    "#             random.shuffle(tokens)\n",
    "#             tokens_list.append(tokens.copy())\n",
    "#     return tokens_list\n",
    "\n",
    "# A list of sentences and the desired number of data samples as input.\n",
    "def make_data(extents, all_samples, word2idx, max_vocab , num_per_sample = 120):\n",
    "    batch_data = []\n",
    "    max_len = 0\n",
    "    # len_sentences = len(extents)\n",
    "    for extent in extents :\n",
    "        max_len = max(max_len, len(extent))\n",
    "    max_len = max_len * 2 + 3\n",
    "    print(max_len)\n",
    "    for sample in all_samples :\n",
    "        \n",
    "        tokens_a_idx = sample[0]\n",
    "        tokens_b_idx = sample[1]\n",
    "        tokens_a = extent_token_train[tokens_a_idx]\n",
    "        tokens_b = extent_token_train[tokens_b_idx]\n",
    "             \n",
    "\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0 for i in range(\n",
    "            1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))]\n",
    "\n",
    "        # Determines the number of positions to mask (n_pred) based on the input sequence length.\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * .15)))\n",
    "        cand_pos = [i for i, token in enumerate(input_ids)\n",
    "                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] #exclude special tokens.\n",
    "\n",
    "        # shuffle all candidate position index, to sampling maksed position from first n_pred\n",
    "        masked_pos, masked_tokens = masking_procedure(\n",
    "            cand_pos[:n_pred], input_ids, max_vocab, word2idx['[MASK]'])\n",
    "\n",
    "        # zero padding for tokens to ensure that the input sequences and segment IDs have the maximum sequence length\n",
    "        padding(input_ids, max_len - len(input_ids))\n",
    "        # print(\"the size of input_ids is \" ,len(input_ids))\n",
    "        padding(segment_ids, max_len - len(segment_ids))\n",
    "        # print(\"the size of segment_ids is \" ,len(segment_ids))\n",
    "\n",
    "        # zero padding for mask\n",
    "        if max_pred > n_pred:\n",
    "            n_pads = max_pred - n_pred\n",
    "            padding(masked_pos, n_pads)\n",
    "            padding(masked_tokens, n_pads)\n",
    "\n",
    "        # Creating Batch Data:\n",
    "        batch_data.append(\n",
    "            [input_ids, segment_ids, masked_tokens, masked_pos, sample[2]])\n",
    "\n",
    "    random.shuffle(batch_data)\n",
    "    print(len(batch_data))\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.is_next = is_next\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.is_next[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b5fd-3776-4641-baa3-d82a8c496007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-Train BERT\n",
    "\n",
    "请先在下面设置是否进行下个句子预测的任务。\n",
    "\n",
    "如果不进行该任务，那么将会用到全部的extent pair训练数据，并且后面会跳过NSP那段程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7abc56-ac8f-449d-9709-e60d720dace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NSP_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f8ec1-06d2-443c-a93f-5390449c0b55",
   "metadata": {},
   "source": [
    "请在下面设置训练的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a030f9-caff-4a5e-8763-9a061fbabe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 54 # 必须为偶数\n",
    "lr = 2e-5\n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82700c5-db66-477a-baac-abd77f9171f9",
   "metadata": {},
   "source": [
    "下面是训练的主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40521dbb-258b-4a9e-a94f-859e4a6836a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "2870\n",
      "Epoch:1 \t loss: 3.467556\n",
      "Epoch:2 \t loss: 2.584857\n",
      "Epoch:3 \t loss: 2.661785\n",
      "Epoch:4 \t loss: 3.129277\n",
      "Epoch:5 \t loss: 1.866102\n",
      "Epoch:6 \t loss: 1.374606\n",
      "Epoch:7 \t loss: 1.237872\n",
      "Epoch:8 \t loss: 1.542521\n",
      "Epoch:9 \t loss: 1.521482\n",
      "Epoch:10 \t loss: 1.751760\n",
      "Epoch:11 \t loss: 1.257751\n",
      "Epoch:12 \t loss: 2.228470\n",
      "Epoch:13 \t loss: 0.710652\n",
      "Epoch:14 \t loss: 1.352659\n",
      "Epoch:15 \t loss: 1.186248\n",
      "Epoch:16 \t loss: 1.224095\n",
      "Epoch:17 \t loss: 1.084981\n",
      "Epoch:18 \t loss: 1.042567\n",
      "Epoch:19 \t loss: 0.844449\n",
      "Epoch:20 \t loss: 0.987576\n",
      "Epoch:21 \t loss: 0.778382\n",
      "Epoch:22 \t loss: 0.665857\n",
      "Epoch:23 \t loss: 0.659755\n",
      "Epoch:24 \t loss: 0.833743\n",
      "Epoch:25 \t loss: 0.727627\n",
      "Epoch:26 \t loss: 1.107923\n",
      "Epoch:27 \t loss: 0.576774\n",
      "Epoch:28 \t loss: 0.462181\n",
      "Epoch:29 \t loss: 0.732054\n",
      "Epoch:30 \t loss: 0.311920\n",
      "Epoch:31 \t loss: 0.894807\n",
      "Epoch:32 \t loss: 0.208055\n",
      "Epoch:33 \t loss: 0.534245\n",
      "Epoch:34 \t loss: 0.208643\n",
      "Epoch:35 \t loss: 0.410355\n",
      "Epoch:36 \t loss: 0.418258\n",
      "Epoch:37 \t loss: 0.354610\n",
      "Epoch:38 \t loss: 0.200065\n",
      "Epoch:39 \t loss: 0.195856\n",
      "Epoch:40 \t loss: 0.775757\n",
      "Epoch:41 \t loss: 0.099918\n",
      "Epoch:42 \t loss: 0.110954\n",
      "Epoch:43 \t loss: 0.533370\n",
      "Epoch:44 \t loss: 0.593948\n",
      "Epoch:45 \t loss: 0.237640\n",
      "Epoch:46 \t loss: 0.392419\n",
      "Epoch:47 \t loss: 0.067498\n",
      "Epoch:48 \t loss: 0.057382\n",
      "Epoch:49 \t loss: 0.343976\n",
      "Epoch:50 \t loss: 0.043054\n",
      "Epoch:51 \t loss: 0.007112\n",
      "Epoch:52 \t loss: 0.016637\n",
      "Epoch:53 \t loss: 0.194101\n",
      "Epoch:54 \t loss: 0.500839\n",
      "Epoch:55 \t loss: 0.003319\n",
      "Epoch:56 \t loss: 0.743696\n",
      "Epoch:57 \t loss: 0.038355\n",
      "Epoch:58 \t loss: 0.032160\n",
      "Epoch:59 \t loss: 0.449352\n",
      "Epoch:60 \t loss: 0.004310\n",
      "Epoch:61 \t loss: 0.038833\n",
      "Epoch:62 \t loss: 0.246569\n",
      "Epoch:63 \t loss: 0.836792\n",
      "Epoch:64 \t loss: 0.043492\n",
      "Epoch:65 \t loss: 0.051976\n",
      "Epoch:66 \t loss: 0.200742\n",
      "Epoch:67 \t loss: 0.378970\n",
      "Epoch:68 \t loss: 0.339751\n",
      "Epoch:69 \t loss: 0.004618\n",
      "Epoch:70 \t loss: 0.020006\n",
      "Epoch:71 \t loss: 0.361541\n",
      "Epoch:72 \t loss: 0.022219\n",
      "Epoch:73 \t loss: 0.296971\n",
      "Epoch:74 \t loss: 1.144279\n",
      "Epoch:75 \t loss: 0.265540\n",
      "Epoch:76 \t loss: 0.301440\n",
      "Epoch:77 \t loss: 0.205341\n",
      "Epoch:78 \t loss: 0.072123\n",
      "Epoch:79 \t loss: 0.280365\n",
      "Epoch:80 \t loss: 0.448881\n",
      "Epoch:81 \t loss: 0.083986\n",
      "Epoch:82 \t loss: 0.235908\n",
      "Epoch:83 \t loss: 0.177910\n",
      "Epoch:84 \t loss: 0.115511\n",
      "Epoch:85 \t loss: 0.328111\n",
      "Epoch:86 \t loss: 0.167268\n",
      "Epoch:87 \t loss: 0.061064\n",
      "Epoch:88 \t loss: 0.008960\n",
      "Epoch:89 \t loss: 0.004701\n",
      "Epoch:90 \t loss: 0.010453\n",
      "Epoch:91 \t loss: 0.238506\n",
      "Epoch:92 \t loss: 0.060051\n",
      "Epoch:93 \t loss: 0.084622\n",
      "Epoch:94 \t loss: 0.182357\n",
      "Epoch:95 \t loss: 0.506881\n",
      "Epoch:96 \t loss: 0.011324\n",
      "Epoch:97 \t loss: 0.001272\n",
      "Epoch:98 \t loss: 0.061214\n",
      "Epoch:99 \t loss: 0.147661\n",
      "Epoch:100 \t loss: 0.518254\n",
      "Epoch:101 \t loss: 0.059908\n",
      "Epoch:102 \t loss: 0.001189\n",
      "Epoch:103 \t loss: 0.241632\n",
      "Epoch:104 \t loss: 0.632435\n",
      "Epoch:105 \t loss: 0.121418\n",
      "Epoch:106 \t loss: 0.122181\n",
      "Epoch:107 \t loss: 0.072213\n",
      "Epoch:108 \t loss: 0.202236\n",
      "Epoch:109 \t loss: 0.026446\n",
      "Epoch:110 \t loss: 0.106836\n",
      "Epoch:111 \t loss: 0.000895\n",
      "Epoch:112 \t loss: 0.004992\n",
      "Epoch:113 \t loss: 0.274746\n",
      "Epoch:114 \t loss: 0.307817\n",
      "Epoch:115 \t loss: 0.025700\n",
      "Epoch:116 \t loss: 0.135183\n",
      "Epoch:117 \t loss: 0.009344\n",
      "Epoch:118 \t loss: 0.010744\n",
      "Epoch:119 \t loss: 0.067975\n",
      "Epoch:120 \t loss: 0.000397\n",
      "Epoch:121 \t loss: 0.165522\n",
      "Epoch:122 \t loss: 0.189029\n",
      "Epoch:123 \t loss: 0.001494\n",
      "Epoch:124 \t loss: 0.031150\n",
      "Epoch:125 \t loss: 0.537960\n",
      "Epoch:126 \t loss: 0.256054\n",
      "Epoch:127 \t loss: 0.236279\n",
      "Epoch:128 \t loss: 0.196582\n",
      "Epoch:129 \t loss: 0.256800\n",
      "Epoch:130 \t loss: 0.508173\n",
      "Epoch:131 \t loss: 0.000431\n",
      "Epoch:132 \t loss: 0.412254\n",
      "Epoch:133 \t loss: 0.006182\n",
      "Epoch:134 \t loss: 0.001938\n",
      "Epoch:135 \t loss: 0.000443\n",
      "Epoch:136 \t loss: 0.064997\n",
      "Epoch:137 \t loss: 0.001817\n",
      "Epoch:138 \t loss: 0.260462\n",
      "Epoch:139 \t loss: 0.314394\n",
      "Epoch:140 \t loss: 0.029758\n",
      "Epoch:141 \t loss: 0.135395\n",
      "Epoch:142 \t loss: 0.072259\n",
      "Epoch:143 \t loss: 0.111574\n",
      "Epoch:144 \t loss: 0.001220\n",
      "Epoch:145 \t loss: 0.074937\n",
      "Epoch:146 \t loss: 0.166187\n",
      "Epoch:147 \t loss: 0.002016\n",
      "Epoch:148 \t loss: 0.193575\n",
      "Epoch:149 \t loss: 0.000202\n",
      "Epoch:150 \t loss: 0.003011\n",
      "Epoch:151 \t loss: 0.202491\n",
      "Epoch:152 \t loss: 0.001219\n",
      "Epoch:153 \t loss: 0.314868\n",
      "Epoch:154 \t loss: 0.153769\n",
      "Epoch:155 \t loss: 0.199021\n",
      "Epoch:156 \t loss: 0.002516\n",
      "Epoch:157 \t loss: 0.082481\n",
      "Epoch:158 \t loss: 0.133856\n",
      "Epoch:159 \t loss: 0.209924\n",
      "Epoch:160 \t loss: 0.180822\n",
      "Epoch:161 \t loss: 0.102691\n",
      "Epoch:162 \t loss: 0.114688\n",
      "Epoch:163 \t loss: 0.053269\n",
      "Epoch:164 \t loss: 0.111186\n",
      "Epoch:165 \t loss: 0.106114\n",
      "Epoch:166 \t loss: 0.074016\n",
      "Epoch:167 \t loss: 0.135548\n",
      "Epoch:168 \t loss: 0.214561\n",
      "Epoch:169 \t loss: 0.216561\n",
      "Epoch:170 \t loss: 0.064514\n",
      "Epoch:171 \t loss: 0.097215\n",
      "Epoch:172 \t loss: 0.000423\n",
      "Epoch:173 \t loss: 0.168510\n",
      "Epoch:174 \t loss: 0.240215\n",
      "Epoch:175 \t loss: 0.166488\n",
      "Epoch:176 \t loss: 0.013455\n",
      "Epoch:177 \t loss: 0.045467\n",
      "Epoch:178 \t loss: 0.056397\n",
      "Epoch:179 \t loss: 0.038772\n",
      "Epoch:180 \t loss: 0.002521\n",
      "Epoch:181 \t loss: 0.360837\n",
      "Epoch:182 \t loss: 0.167382\n",
      "Epoch:183 \t loss: 0.103224\n",
      "Epoch:184 \t loss: 0.244784\n",
      "Epoch:185 \t loss: 0.257413\n",
      "Epoch:186 \t loss: 0.071278\n",
      "Epoch:187 \t loss: 0.005425\n",
      "Epoch:188 \t loss: 0.005227\n",
      "Epoch:189 \t loss: 0.118728\n",
      "Epoch:190 \t loss: 0.057092\n",
      "Epoch:191 \t loss: 0.004887\n",
      "Epoch:192 \t loss: 0.028768\n",
      "Epoch:193 \t loss: 0.220770\n",
      "Epoch:194 \t loss: 0.053543\n",
      "Epoch:195 \t loss: 0.106866\n",
      "Epoch:196 \t loss: 0.071503\n",
      "Epoch:197 \t loss: 0.227999\n",
      "Epoch:198 \t loss: 0.020504\n",
      "Epoch:199 \t loss: 0.000276\n",
      "Epoch:200 \t loss: 0.148253\n",
      "Epoch:201 \t loss: 0.262009\n",
      "Epoch:202 \t loss: 0.070584\n",
      "Epoch:203 \t loss: 0.089746\n",
      "Epoch:204 \t loss: 0.025309\n",
      "Epoch:205 \t loss: 0.052222\n",
      "Epoch:206 \t loss: 0.255683\n",
      "Epoch:207 \t loss: 0.003084\n",
      "Epoch:208 \t loss: 0.056949\n",
      "Epoch:209 \t loss: 0.142664\n",
      "Epoch:210 \t loss: 0.115154\n",
      "Epoch:211 \t loss: 0.106182\n",
      "Epoch:212 \t loss: 0.038396\n",
      "Epoch:213 \t loss: 0.058440\n",
      "Epoch:214 \t loss: 0.001258\n",
      "Epoch:215 \t loss: 0.000498\n",
      "Epoch:216 \t loss: 0.127543\n",
      "Epoch:217 \t loss: 0.116780\n",
      "Epoch:218 \t loss: 0.000113\n",
      "Epoch:219 \t loss: 0.005825\n",
      "Epoch:220 \t loss: 0.005355\n",
      "Epoch:221 \t loss: 0.000224\n",
      "Epoch:222 \t loss: 0.162852\n",
      "Epoch:223 \t loss: 0.099732\n",
      "Epoch:224 \t loss: 0.108211\n",
      "Epoch:225 \t loss: 0.104635\n",
      "Epoch:226 \t loss: 0.060298\n",
      "Epoch:227 \t loss: 0.232411\n",
      "Epoch:228 \t loss: 0.493092\n",
      "Epoch:229 \t loss: 0.040952\n",
      "Epoch:230 \t loss: 0.271692\n",
      "Epoch:231 \t loss: 0.000036\n",
      "Epoch:232 \t loss: 0.165715\n",
      "Epoch:233 \t loss: 0.053706\n",
      "Epoch:234 \t loss: 0.131394\n",
      "Epoch:235 \t loss: 0.045413\n",
      "Epoch:236 \t loss: 0.172798\n",
      "Epoch:237 \t loss: 0.024956\n",
      "Epoch:238 \t loss: 0.116342\n",
      "Epoch:239 \t loss: 0.212472\n",
      "Epoch:240 \t loss: 0.001129\n",
      "Epoch:241 \t loss: 0.013450\n",
      "Epoch:242 \t loss: 0.356794\n",
      "Epoch:243 \t loss: 0.322373\n",
      "Epoch:244 \t loss: 0.054698\n",
      "Epoch:245 \t loss: 0.036323\n",
      "Epoch:246 \t loss: 0.148446\n",
      "Epoch:247 \t loss: 0.195270\n",
      "Epoch:248 \t loss: 0.234814\n",
      "Epoch:249 \t loss: 0.092498\n",
      "Epoch:250 \t loss: 0.088343\n",
      "Epoch:251 \t loss: 0.000270\n",
      "Epoch:252 \t loss: 0.000060\n",
      "Epoch:253 \t loss: 0.203624\n",
      "Epoch:254 \t loss: 0.125486\n",
      "Epoch:255 \t loss: 0.007604\n",
      "Epoch:256 \t loss: 0.113375\n",
      "Epoch:257 \t loss: 0.613237\n",
      "Epoch:258 \t loss: 0.000135\n",
      "Epoch:259 \t loss: 0.227262\n",
      "Epoch:260 \t loss: 0.230203\n",
      "Epoch:261 \t loss: 0.122821\n",
      "Epoch:262 \t loss: 0.025703\n",
      "Epoch:263 \t loss: 0.002533\n",
      "Epoch:264 \t loss: 0.143046\n",
      "Epoch:265 \t loss: 0.062792\n",
      "Epoch:266 \t loss: 0.048191\n",
      "Epoch:267 \t loss: 0.118610\n",
      "Epoch:268 \t loss: 0.071345\n",
      "Epoch:269 \t loss: 0.059494\n",
      "Epoch:270 \t loss: 0.001999\n",
      "Epoch:271 \t loss: 0.092159\n",
      "Epoch:272 \t loss: 0.000834\n",
      "Epoch:273 \t loss: 0.000086\n",
      "Epoch:274 \t loss: 0.036813\n",
      "Epoch:275 \t loss: 0.001667\n",
      "Epoch:276 \t loss: 0.011382\n",
      "Epoch:277 \t loss: 0.080198\n",
      "Epoch:278 \t loss: 0.018153\n",
      "Epoch:279 \t loss: 0.014127\n",
      "Epoch:280 \t loss: 0.049265\n",
      "Epoch:281 \t loss: 0.224508\n",
      "Epoch:282 \t loss: 0.087855\n",
      "Epoch:283 \t loss: 0.009585\n",
      "Epoch:284 \t loss: 0.000076\n",
      "Epoch:285 \t loss: 0.054617\n",
      "Epoch:286 \t loss: 0.222081\n",
      "Epoch:287 \t loss: 0.001700\n",
      "Epoch:288 \t loss: 0.095513\n",
      "Epoch:289 \t loss: 0.002889\n",
      "Epoch:290 \t loss: 0.004283\n",
      "Epoch:291 \t loss: 0.000150\n",
      "Epoch:292 \t loss: 0.002133\n",
      "Epoch:293 \t loss: 0.098303\n",
      "Epoch:294 \t loss: 0.361926\n",
      "Epoch:295 \t loss: 0.075985\n",
      "Epoch:296 \t loss: 0.084512\n",
      "Epoch:297 \t loss: 0.000014\n",
      "Epoch:298 \t loss: 0.055621\n",
      "Epoch:299 \t loss: 0.019080\n",
      "Epoch:300 \t loss: 0.102384\n",
      "Epoch:301 \t loss: 0.160956\n",
      "Epoch:302 \t loss: 0.093092\n",
      "Epoch:303 \t loss: 0.002013\n",
      "Epoch:304 \t loss: 0.001279\n",
      "Epoch:305 \t loss: 0.006973\n",
      "Epoch:306 \t loss: 0.063832\n",
      "Epoch:307 \t loss: 0.001239\n",
      "Epoch:308 \t loss: 0.020246\n",
      "Epoch:309 \t loss: 0.225363\n",
      "Epoch:310 \t loss: 0.003270\n",
      "Epoch:311 \t loss: 0.174077\n",
      "Epoch:312 \t loss: 0.125873\n",
      "Epoch:313 \t loss: 0.001391\n",
      "Epoch:314 \t loss: 0.014332\n",
      "Epoch:315 \t loss: 0.135495\n",
      "Epoch:316 \t loss: 0.066121\n",
      "Epoch:317 \t loss: 0.237774\n",
      "Epoch:318 \t loss: 0.002135\n",
      "Epoch:319 \t loss: 0.125952\n",
      "Epoch:320 \t loss: 0.002043\n",
      "Epoch:321 \t loss: 0.108638\n",
      "Epoch:322 \t loss: 0.067868\n",
      "Epoch:323 \t loss: 0.057661\n",
      "Epoch:324 \t loss: 0.025203\n",
      "Epoch:325 \t loss: 0.054935\n",
      "Epoch:326 \t loss: 0.069798\n",
      "Epoch:327 \t loss: 0.002900\n",
      "Epoch:328 \t loss: 0.093373\n",
      "Epoch:329 \t loss: 0.144857\n",
      "Epoch:330 \t loss: 0.014221\n",
      "Epoch:331 \t loss: 0.000259\n",
      "Epoch:332 \t loss: 0.217699\n",
      "Epoch:333 \t loss: 0.534309\n",
      "Epoch:334 \t loss: 0.055023\n",
      "Epoch:335 \t loss: 0.126127\n",
      "Epoch:336 \t loss: 0.102338\n",
      "Epoch:337 \t loss: 0.081645\n",
      "Epoch:338 \t loss: 0.172846\n",
      "Epoch:339 \t loss: 0.000295\n",
      "Epoch:340 \t loss: 0.050203\n",
      "Epoch:341 \t loss: 0.000172\n",
      "Epoch:342 \t loss: 0.000094\n",
      "Epoch:343 \t loss: 0.271264\n",
      "Epoch:344 \t loss: 0.072682\n",
      "Epoch:345 \t loss: 0.052941\n",
      "Epoch:346 \t loss: 0.036971\n",
      "Epoch:347 \t loss: 0.022149\n",
      "Epoch:348 \t loss: 0.000042\n",
      "Epoch:349 \t loss: 0.046977\n",
      "Epoch:350 \t loss: 0.030310\n",
      "Epoch:351 \t loss: 0.077354\n",
      "Epoch:352 \t loss: 0.085359\n",
      "Epoch:353 \t loss: 0.201268\n",
      "Epoch:354 \t loss: 0.068104\n",
      "Epoch:355 \t loss: 0.001069\n",
      "Epoch:356 \t loss: 0.006748\n",
      "Epoch:357 \t loss: 0.000064\n",
      "Epoch:358 \t loss: 0.144003\n",
      "Epoch:359 \t loss: 0.008774\n",
      "Epoch:360 \t loss: 0.196098\n",
      "Epoch:361 \t loss: 0.029692\n",
      "Epoch:362 \t loss: 0.160903\n",
      "Epoch:363 \t loss: 0.115264\n",
      "Epoch:364 \t loss: 0.250917\n",
      "Epoch:365 \t loss: 0.141887\n",
      "Epoch:366 \t loss: 0.120646\n",
      "Epoch:367 \t loss: 0.110818\n",
      "Epoch:368 \t loss: 0.047388\n",
      "Epoch:369 \t loss: 0.004937\n",
      "Epoch:370 \t loss: 0.000242\n",
      "Epoch:371 \t loss: 0.090089\n",
      "Epoch:372 \t loss: 0.087848\n",
      "Epoch:373 \t loss: 0.000025\n",
      "Epoch:374 \t loss: 0.072222\n",
      "Epoch:375 \t loss: 0.081133\n",
      "Epoch:376 \t loss: 0.111939\n",
      "Epoch:377 \t loss: 0.142763\n",
      "Epoch:378 \t loss: 0.000258\n",
      "Epoch:379 \t loss: 0.114058\n",
      "Epoch:380 \t loss: 0.000137\n",
      "Epoch:381 \t loss: 0.131583\n",
      "Epoch:382 \t loss: 0.120420\n",
      "Epoch:383 \t loss: 0.000104\n",
      "Epoch:384 \t loss: 0.198511\n",
      "Epoch:385 \t loss: 0.169625\n",
      "Epoch:386 \t loss: 0.043011\n",
      "Epoch:387 \t loss: 0.001359\n",
      "Epoch:388 \t loss: 0.000159\n",
      "Epoch:389 \t loss: 0.027661\n",
      "Epoch:390 \t loss: 0.073249\n",
      "Epoch:391 \t loss: 0.002577\n",
      "Epoch:392 \t loss: 0.000007\n",
      "Epoch:393 \t loss: 0.000164\n",
      "Epoch:394 \t loss: 0.178509\n",
      "Epoch:395 \t loss: 0.117036\n",
      "Epoch:396 \t loss: 0.026105\n",
      "Epoch:397 \t loss: 0.158393\n",
      "Epoch:398 \t loss: 0.156722\n",
      "Epoch:399 \t loss: 0.000164\n",
      "Epoch:400 \t loss: 0.127912\n",
      "Epoch:401 \t loss: 0.269491\n",
      "Epoch:402 \t loss: 0.122767\n",
      "Epoch:403 \t loss: 0.095455\n",
      "Epoch:404 \t loss: 0.114304\n",
      "Epoch:405 \t loss: 0.190974\n",
      "Epoch:406 \t loss: 0.225448\n",
      "Epoch:407 \t loss: 0.052025\n",
      "Epoch:408 \t loss: 0.068173\n",
      "Epoch:409 \t loss: 0.267641\n",
      "Epoch:410 \t loss: 0.000752\n",
      "Epoch:411 \t loss: 0.000085\n",
      "Epoch:412 \t loss: 0.073088\n",
      "Epoch:413 \t loss: 0.157213\n",
      "Epoch:414 \t loss: 0.142341\n",
      "Epoch:415 \t loss: 0.053078\n",
      "Epoch:416 \t loss: 0.050305\n",
      "Epoch:417 \t loss: 0.206587\n",
      "Epoch:418 \t loss: 0.372736\n",
      "Epoch:419 \t loss: 0.000038\n",
      "Epoch:420 \t loss: 0.124402\n",
      "Epoch:421 \t loss: 0.145176\n",
      "Epoch:422 \t loss: 0.125368\n",
      "Epoch:423 \t loss: 0.129831\n",
      "Epoch:424 \t loss: 0.150664\n",
      "Epoch:425 \t loss: 0.055012\n",
      "Epoch:426 \t loss: 0.116406\n",
      "Epoch:427 \t loss: 0.128480\n",
      "Epoch:428 \t loss: 0.175316\n",
      "Epoch:429 \t loss: 0.221725\n",
      "Epoch:430 \t loss: 0.147958\n",
      "Epoch:431 \t loss: 0.015258\n",
      "Epoch:432 \t loss: 0.033252\n",
      "Epoch:433 \t loss: 0.206113\n",
      "Epoch:434 \t loss: 0.222316\n",
      "Epoch:435 \t loss: 0.128095\n",
      "Epoch:436 \t loss: 0.058446\n",
      "Epoch:437 \t loss: 0.031160\n",
      "Epoch:438 \t loss: 0.203285\n",
      "Epoch:439 \t loss: 0.219218\n",
      "Epoch:440 \t loss: 0.000788\n",
      "Epoch:441 \t loss: 0.110032\n",
      "Epoch:442 \t loss: 0.141676\n",
      "Epoch:443 \t loss: 0.033967\n",
      "Epoch:444 \t loss: 0.012656\n",
      "Epoch:445 \t loss: 0.145248\n",
      "Epoch:446 \t loss: 0.000052\n",
      "Epoch:447 \t loss: 0.017502\n",
      "Epoch:448 \t loss: 0.130106\n",
      "Epoch:449 \t loss: 0.054197\n",
      "Epoch:450 \t loss: 0.000285\n",
      "Epoch:451 \t loss: 0.059407\n",
      "Epoch:452 \t loss: 0.116188\n",
      "Epoch:453 \t loss: 0.060169\n",
      "Epoch:454 \t loss: 0.133284\n",
      "Epoch:455 \t loss: 0.158568\n",
      "Epoch:456 \t loss: 0.078718\n",
      "Epoch:457 \t loss: 0.000247\n",
      "Epoch:458 \t loss: 0.080643\n",
      "Epoch:459 \t loss: 0.093222\n",
      "Epoch:460 \t loss: 0.103967\n",
      "Epoch:461 \t loss: 0.000045\n",
      "Epoch:462 \t loss: 0.129097\n",
      "Epoch:463 \t loss: 0.079776\n",
      "Epoch:464 \t loss: 0.073486\n",
      "Epoch:465 \t loss: 0.139510\n",
      "Epoch:466 \t loss: 0.000051\n",
      "Epoch:467 \t loss: 0.107434\n",
      "Epoch:468 \t loss: 0.035697\n",
      "Epoch:469 \t loss: 0.031740\n",
      "Epoch:470 \t loss: 0.130372\n",
      "Epoch:471 \t loss: 0.182944\n",
      "Epoch:472 \t loss: 0.000008\n",
      "Epoch:473 \t loss: 0.000626\n",
      "Epoch:474 \t loss: 0.135678\n",
      "Epoch:475 \t loss: 0.117483\n",
      "Epoch:476 \t loss: 0.000101\n",
      "Epoch:477 \t loss: 0.000296\n",
      "Epoch:478 \t loss: 0.142160\n",
      "Epoch:479 \t loss: 0.066601\n",
      "Epoch:480 \t loss: 0.155883\n",
      "Epoch:481 \t loss: 0.046256\n",
      "Epoch:482 \t loss: 0.215205\n",
      "Epoch:483 \t loss: 0.197286\n",
      "Epoch:484 \t loss: 0.000184\n",
      "Epoch:485 \t loss: 0.025649\n",
      "Epoch:486 \t loss: 0.000054\n",
      "Epoch:487 \t loss: 0.190774\n",
      "Epoch:488 \t loss: 0.000098\n",
      "Epoch:489 \t loss: 0.194334\n",
      "Epoch:490 \t loss: 0.000509\n",
      "Epoch:491 \t loss: 0.000063\n",
      "Epoch:492 \t loss: 0.000019\n",
      "Epoch:493 \t loss: 0.194705\n",
      "Epoch:494 \t loss: 0.311361\n",
      "Epoch:495 \t loss: 0.115517\n",
      "Epoch:496 \t loss: 0.245469\n",
      "Epoch:497 \t loss: 0.000807\n",
      "Epoch:498 \t loss: 0.125442\n",
      "Epoch:499 \t loss: 0.131951\n",
      "Epoch:500 \t loss: 0.211443\n",
      "Epoch:501 \t loss: 0.075540\n",
      "Epoch:502 \t loss: 0.159855\n",
      "Epoch:503 \t loss: 0.151294\n",
      "Epoch:504 \t loss: 0.233648\n",
      "Epoch:505 \t loss: 0.077546\n",
      "Epoch:506 \t loss: 0.232099\n",
      "Epoch:507 \t loss: 0.000115\n",
      "Epoch:508 \t loss: 0.222329\n",
      "Epoch:509 \t loss: 0.266900\n",
      "Epoch:510 \t loss: 0.079341\n",
      "Epoch:511 \t loss: 0.004610\n",
      "Epoch:512 \t loss: 0.015059\n",
      "Epoch:513 \t loss: 0.014380\n",
      "Epoch:514 \t loss: 0.061549\n",
      "Epoch:515 \t loss: 0.116174\n",
      "Epoch:516 \t loss: 0.013320\n",
      "Epoch:517 \t loss: 0.090792\n",
      "Epoch:518 \t loss: 0.013798\n",
      "Epoch:519 \t loss: 0.089142\n",
      "Epoch:520 \t loss: 0.000017\n",
      "Epoch:521 \t loss: 0.099160\n",
      "Epoch:522 \t loss: 0.069210\n",
      "Epoch:523 \t loss: 0.030564\n",
      "Epoch:524 \t loss: 0.000144\n",
      "Epoch:525 \t loss: 0.066197\n",
      "Epoch:526 \t loss: 0.192670\n",
      "Epoch:527 \t loss: 0.000043\n",
      "Epoch:528 \t loss: 0.185439\n",
      "Epoch:529 \t loss: 0.000455\n",
      "Epoch:530 \t loss: 0.003270\n",
      "Epoch:531 \t loss: 0.093887\n",
      "Epoch:532 \t loss: 0.096014\n",
      "Epoch:533 \t loss: 0.090774\n",
      "Epoch:534 \t loss: 0.160889\n",
      "Epoch:535 \t loss: 0.154612\n",
      "Epoch:536 \t loss: 0.031294\n",
      "Epoch:537 \t loss: 0.052952\n",
      "Epoch:538 \t loss: 0.399489\n",
      "Epoch:539 \t loss: 0.192553\n",
      "Epoch:540 \t loss: 0.054426\n",
      "Epoch:541 \t loss: 0.193940\n",
      "Epoch:542 \t loss: 0.118804\n",
      "Epoch:543 \t loss: 0.140004\n",
      "Epoch:544 \t loss: 0.601896\n",
      "Epoch:545 \t loss: 0.012161\n",
      "Epoch:546 \t loss: 0.052371\n",
      "Epoch:547 \t loss: 0.004046\n",
      "Epoch:548 \t loss: 0.080271\n",
      "Epoch:549 \t loss: 0.198763\n",
      "Epoch:550 \t loss: 0.000252\n",
      "Epoch:551 \t loss: 0.164075\n",
      "Epoch:552 \t loss: 0.014282\n",
      "Epoch:553 \t loss: 0.000080\n",
      "Epoch:554 \t loss: 0.129757\n",
      "Epoch:555 \t loss: 0.060009\n",
      "Epoch:556 \t loss: 0.136074\n",
      "Epoch:557 \t loss: 0.094539\n",
      "Epoch:558 \t loss: 0.053334\n",
      "Epoch:559 \t loss: 0.096953\n",
      "Epoch:560 \t loss: 0.001448\n",
      "Epoch:561 \t loss: 0.117647\n",
      "Epoch:562 \t loss: 0.137314\n",
      "Epoch:563 \t loss: 0.146111\n",
      "Epoch:564 \t loss: 0.143479\n",
      "Epoch:565 \t loss: 0.003930\n",
      "Epoch:566 \t loss: 0.000281\n",
      "Epoch:567 \t loss: 0.000590\n",
      "Epoch:568 \t loss: 0.069763\n",
      "Epoch:569 \t loss: 0.020948\n",
      "Epoch:570 \t loss: 0.151450\n",
      "Epoch:571 \t loss: 0.071892\n",
      "Epoch:572 \t loss: 0.171995\n",
      "Epoch:573 \t loss: 0.001191\n",
      "Epoch:574 \t loss: 0.098731\n",
      "Epoch:575 \t loss: 0.000034\n",
      "Epoch:576 \t loss: 0.043541\n",
      "Epoch:577 \t loss: 0.101300\n",
      "Epoch:578 \t loss: 0.072295\n",
      "Epoch:579 \t loss: 0.096175\n",
      "Epoch:580 \t loss: 0.067039\n",
      "Epoch:581 \t loss: 0.013694\n",
      "Epoch:582 \t loss: 0.124227\n",
      "Epoch:583 \t loss: 0.241736\n",
      "Epoch:584 \t loss: 0.147848\n",
      "Epoch:585 \t loss: 0.145056\n",
      "Epoch:586 \t loss: 0.074208\n",
      "Epoch:587 \t loss: 0.028198\n",
      "Epoch:588 \t loss: 0.010099\n",
      "Epoch:589 \t loss: 0.216918\n",
      "Epoch:590 \t loss: 0.068250\n",
      "Epoch:591 \t loss: 0.225402\n",
      "Epoch:592 \t loss: 0.000010\n",
      "Epoch:593 \t loss: 0.000318\n",
      "Epoch:594 \t loss: 0.029109\n",
      "Epoch:595 \t loss: 0.019554\n",
      "Epoch:596 \t loss: 0.036303\n",
      "Epoch:597 \t loss: 0.024329\n",
      "Epoch:598 \t loss: 0.047905\n",
      "Epoch:599 \t loss: 0.114462\n",
      "Epoch:600 \t loss: 0.000430\n",
      "Epoch:601 \t loss: 0.145081\n",
      "Epoch:602 \t loss: 0.155284\n",
      "Epoch:603 \t loss: 0.000024\n",
      "Epoch:604 \t loss: 0.074532\n",
      "Epoch:605 \t loss: 0.062136\n",
      "Epoch:606 \t loss: 0.065921\n",
      "Epoch:607 \t loss: 0.069433\n",
      "Epoch:608 \t loss: 0.000024\n",
      "Epoch:609 \t loss: 0.145388\n",
      "Epoch:610 \t loss: 0.018235\n",
      "Epoch:611 \t loss: 0.108750\n",
      "Epoch:612 \t loss: 0.047060\n",
      "Epoch:613 \t loss: 0.100938\n",
      "Epoch:614 \t loss: 0.100919\n",
      "Epoch:615 \t loss: 0.141934\n",
      "Epoch:616 \t loss: 0.074007\n",
      "Epoch:617 \t loss: 0.000134\n",
      "Epoch:618 \t loss: 0.150137\n",
      "Epoch:619 \t loss: 0.000074\n",
      "Epoch:620 \t loss: 0.055563\n",
      "Epoch:621 \t loss: 0.000049\n",
      "Epoch:622 \t loss: 0.112695\n",
      "Epoch:623 \t loss: 0.000022\n",
      "Epoch:624 \t loss: 0.068369\n",
      "Epoch:625 \t loss: 0.000312\n",
      "Epoch:626 \t loss: 0.099956\n",
      "Epoch:627 \t loss: 0.081535\n",
      "Epoch:628 \t loss: 0.002850\n",
      "Epoch:629 \t loss: 0.010447\n",
      "Epoch:630 \t loss: 0.047523\n",
      "Epoch:631 \t loss: 0.079494\n",
      "Epoch:632 \t loss: 0.040148\n",
      "Epoch:633 \t loss: 0.220308\n",
      "Epoch:634 \t loss: 0.098342\n",
      "Epoch:635 \t loss: 0.100007\n",
      "Epoch:636 \t loss: 0.035724\n",
      "Epoch:637 \t loss: 0.129408\n",
      "Epoch:638 \t loss: 0.060902\n",
      "Epoch:639 \t loss: 0.000112\n",
      "Epoch:640 \t loss: 0.016783\n",
      "Epoch:641 \t loss: 0.131308\n",
      "Epoch:642 \t loss: 0.028033\n",
      "Epoch:643 \t loss: 0.000044\n",
      "Epoch:644 \t loss: 0.000047\n",
      "Epoch:645 \t loss: 0.031036\n",
      "Epoch:646 \t loss: 0.038544\n",
      "Epoch:647 \t loss: 0.056473\n",
      "Epoch:648 \t loss: 0.078351\n",
      "Epoch:649 \t loss: 0.011571\n",
      "Epoch:650 \t loss: 0.127880\n",
      "Epoch:651 \t loss: 0.005737\n",
      "Epoch:652 \t loss: 0.044021\n",
      "Epoch:653 \t loss: 0.117730\n",
      "Epoch:654 \t loss: 0.078645\n",
      "Epoch:655 \t loss: 0.082149\n",
      "Epoch:656 \t loss: 0.109888\n",
      "Epoch:657 \t loss: 0.000632\n",
      "Epoch:658 \t loss: 0.074141\n",
      "Epoch:659 \t loss: 0.104986\n",
      "Epoch:660 \t loss: 0.000037\n",
      "Epoch:661 \t loss: 0.047412\n",
      "Epoch:662 \t loss: 0.062241\n",
      "Epoch:663 \t loss: 0.008196\n",
      "Epoch:664 \t loss: 0.062180\n",
      "Epoch:665 \t loss: 0.000002\n",
      "Epoch:666 \t loss: 0.078160\n",
      "Epoch:667 \t loss: 0.000029\n",
      "Epoch:668 \t loss: 0.015414\n",
      "Epoch:669 \t loss: 0.057097\n",
      "Epoch:670 \t loss: 0.042937\n",
      "Epoch:671 \t loss: 0.001117\n",
      "Epoch:672 \t loss: 0.000875\n",
      "Epoch:673 \t loss: 0.000028\n",
      "Epoch:674 \t loss: 0.000043\n",
      "Epoch:675 \t loss: 0.233591\n",
      "Epoch:676 \t loss: 0.051210\n",
      "Epoch:677 \t loss: 0.001413\n",
      "Epoch:678 \t loss: 0.022336\n",
      "Epoch:679 \t loss: 0.081908\n",
      "Epoch:680 \t loss: 0.056414\n",
      "Epoch:681 \t loss: 0.000024\n",
      "Epoch:682 \t loss: 0.051771\n",
      "Epoch:683 \t loss: 0.041515\n",
      "Epoch:684 \t loss: 0.058718\n",
      "Epoch:685 \t loss: 0.000613\n",
      "Epoch:686 \t loss: 0.000182\n",
      "Epoch:687 \t loss: 0.211389\n",
      "Epoch:688 \t loss: 0.000011\n",
      "Epoch:689 \t loss: 0.198814\n",
      "Epoch:690 \t loss: 0.000006\n",
      "Epoch:691 \t loss: 0.157118\n",
      "Epoch:692 \t loss: 0.076463\n",
      "Epoch:693 \t loss: 0.151126\n",
      "Epoch:694 \t loss: 0.081935\n",
      "Epoch:695 \t loss: 0.197577\n",
      "Epoch:696 \t loss: 0.066328\n",
      "Epoch:697 \t loss: 0.051152\n",
      "Epoch:698 \t loss: 0.046699\n",
      "Epoch:699 \t loss: 0.000892\n",
      "Epoch:700 \t loss: 0.096426\n",
      "Epoch:701 \t loss: 0.057638\n",
      "Epoch:702 \t loss: 0.187621\n",
      "Epoch:703 \t loss: 0.218626\n",
      "Epoch:704 \t loss: 0.164770\n",
      "Epoch:705 \t loss: 0.215895\n",
      "Epoch:706 \t loss: 0.061958\n",
      "Epoch:707 \t loss: 0.061614\n",
      "Epoch:708 \t loss: 0.182490\n",
      "Epoch:709 \t loss: 0.191308\n",
      "Epoch:710 \t loss: 0.000067\n",
      "Epoch:711 \t loss: 0.078808\n",
      "Epoch:712 \t loss: 0.118520\n",
      "Epoch:713 \t loss: 0.028722\n",
      "Epoch:714 \t loss: 0.045413\n",
      "Epoch:715 \t loss: 0.032804\n",
      "Epoch:716 \t loss: 0.044485\n",
      "Epoch:717 \t loss: 0.077512\n",
      "Epoch:718 \t loss: 0.000127\n",
      "Epoch:719 \t loss: 0.000217\n",
      "Epoch:720 \t loss: 0.035860\n",
      "Epoch:721 \t loss: 0.051927\n",
      "Epoch:722 \t loss: 0.048507\n",
      "Epoch:723 \t loss: 0.115173\n",
      "Epoch:724 \t loss: 0.056517\n",
      "Epoch:725 \t loss: 0.000042\n",
      "Epoch:726 \t loss: 0.149239\n",
      "Epoch:727 \t loss: 0.085987\n",
      "Epoch:728 \t loss: 0.031074\n",
      "Epoch:729 \t loss: 0.143850\n",
      "Epoch:730 \t loss: 0.031334\n",
      "Epoch:731 \t loss: 0.002494\n",
      "Epoch:732 \t loss: 0.000064\n",
      "Epoch:733 \t loss: 0.084539\n",
      "Epoch:734 \t loss: 0.093555\n",
      "Epoch:735 \t loss: 0.083539\n",
      "Epoch:736 \t loss: 0.152873\n",
      "Epoch:737 \t loss: 0.157115\n",
      "Epoch:738 \t loss: 0.165804\n",
      "Epoch:739 \t loss: 0.023923\n",
      "Epoch:740 \t loss: 0.104879\n",
      "Epoch:741 \t loss: 0.013688\n",
      "Epoch:742 \t loss: 0.000004\n",
      "Epoch:743 \t loss: 0.067203\n",
      "Epoch:744 \t loss: 0.076476\n",
      "Epoch:745 \t loss: 0.042076\n",
      "Epoch:746 \t loss: 0.046466\n",
      "Epoch:747 \t loss: 0.001120\n",
      "Epoch:748 \t loss: 0.173088\n",
      "Epoch:749 \t loss: 0.000108\n",
      "Epoch:750 \t loss: 0.112123\n",
      "Epoch:751 \t loss: 0.020171\n",
      "Epoch:752 \t loss: 0.083294\n",
      "Epoch:753 \t loss: 0.048085\n",
      "Epoch:754 \t loss: 0.000005\n",
      "Epoch:755 \t loss: 0.095599\n",
      "Epoch:756 \t loss: 0.000014\n",
      "Epoch:757 \t loss: 0.083871\n",
      "Epoch:758 \t loss: 0.052376\n",
      "Epoch:759 \t loss: 0.079714\n",
      "Epoch:760 \t loss: 0.000434\n",
      "Epoch:761 \t loss: 0.126247\n",
      "Epoch:762 \t loss: 0.122542\n",
      "Epoch:763 \t loss: 0.014972\n",
      "Epoch:764 \t loss: 0.083352\n",
      "Epoch:765 \t loss: 0.000071\n",
      "Epoch:766 \t loss: 0.001288\n",
      "Epoch:767 \t loss: 0.116939\n",
      "Epoch:768 \t loss: 0.039507\n",
      "Epoch:769 \t loss: 0.088049\n",
      "Epoch:770 \t loss: 0.015529\n",
      "Epoch:771 \t loss: 0.000039\n",
      "Epoch:772 \t loss: 0.000027\n",
      "Epoch:773 \t loss: 0.048483\n",
      "Epoch:774 \t loss: 0.001595\n",
      "Epoch:775 \t loss: 0.086811\n",
      "Epoch:776 \t loss: 0.058826\n",
      "Epoch:777 \t loss: 0.076895\n",
      "Epoch:778 \t loss: 0.029414\n",
      "Epoch:779 \t loss: 0.009627\n",
      "Epoch:780 \t loss: 0.010674\n",
      "Epoch:781 \t loss: 0.000028\n",
      "Epoch:782 \t loss: 0.169649\n",
      "Epoch:783 \t loss: 0.112521\n",
      "Epoch:784 \t loss: 0.041240\n",
      "Epoch:785 \t loss: 0.011901\n",
      "Epoch:786 \t loss: 0.000037\n",
      "Epoch:787 \t loss: 0.034019\n",
      "Epoch:788 \t loss: 0.000050\n",
      "Epoch:789 \t loss: 0.107568\n",
      "Epoch:790 \t loss: 0.086422\n",
      "Epoch:791 \t loss: 0.045599\n",
      "Epoch:792 \t loss: 0.000024\n",
      "Epoch:793 \t loss: 0.050045\n",
      "Epoch:794 \t loss: 0.119847\n",
      "Epoch:795 \t loss: 0.108017\n",
      "Epoch:796 \t loss: 0.056127\n",
      "Epoch:797 \t loss: 0.106232\n",
      "Epoch:798 \t loss: 0.000093\n",
      "Epoch:799 \t loss: 0.100091\n",
      "Epoch:800 \t loss: 0.000340\n",
      "Epoch:801 \t loss: 0.100116\n",
      "Epoch:802 \t loss: 0.040555\n",
      "Epoch:803 \t loss: 0.128422\n",
      "Epoch:804 \t loss: 0.104208\n",
      "Epoch:805 \t loss: 0.043244\n",
      "Epoch:806 \t loss: 0.037988\n",
      "Epoch:807 \t loss: 0.004620\n",
      "Epoch:808 \t loss: 0.064439\n",
      "Epoch:809 \t loss: 0.261555\n",
      "Epoch:810 \t loss: 0.189695\n",
      "Epoch:811 \t loss: 0.031171\n",
      "Epoch:812 \t loss: 0.107139\n",
      "Epoch:813 \t loss: 0.085687\n",
      "Epoch:814 \t loss: 0.096308\n",
      "Epoch:815 \t loss: 0.000445\n",
      "Epoch:816 \t loss: 0.129931\n",
      "Epoch:817 \t loss: 0.105270\n",
      "Epoch:818 \t loss: 0.129838\n",
      "Epoch:819 \t loss: 0.135572\n",
      "Epoch:820 \t loss: 0.026816\n",
      "Epoch:821 \t loss: 0.077879\n",
      "Epoch:822 \t loss: 0.048858\n",
      "Epoch:823 \t loss: 0.045000\n",
      "Epoch:824 \t loss: 0.062893\n",
      "Epoch:825 \t loss: 0.023683\n",
      "Epoch:826 \t loss: 0.064588\n",
      "Epoch:827 \t loss: 0.078960\n",
      "Epoch:828 \t loss: 0.106654\n",
      "Epoch:829 \t loss: 0.048709\n",
      "Epoch:830 \t loss: 0.093801\n",
      "Epoch:831 \t loss: 0.082012\n",
      "Epoch:832 \t loss: 0.000012\n",
      "Epoch:833 \t loss: 0.070358\n",
      "Epoch:834 \t loss: 0.049256\n",
      "Epoch:835 \t loss: 0.129076\n",
      "Epoch:836 \t loss: 0.000010\n",
      "Epoch:837 \t loss: 0.100462\n",
      "Epoch:838 \t loss: 0.000085\n",
      "Epoch:839 \t loss: 0.000158\n",
      "Epoch:840 \t loss: 0.050539\n",
      "Epoch:841 \t loss: 0.000066\n",
      "Epoch:842 \t loss: 0.000405\n",
      "Epoch:843 \t loss: 0.194020\n",
      "Epoch:844 \t loss: 0.000011\n",
      "Epoch:845 \t loss: 0.035339\n",
      "Epoch:846 \t loss: 0.139928\n",
      "Epoch:847 \t loss: 0.000073\n",
      "Epoch:848 \t loss: 0.056988\n",
      "Epoch:849 \t loss: 0.012705\n",
      "Epoch:850 \t loss: 0.025613\n",
      "Epoch:851 \t loss: 0.188627\n",
      "Epoch:852 \t loss: 0.000008\n",
      "Epoch:853 \t loss: 0.119543\n",
      "Epoch:854 \t loss: 0.068563\n",
      "Epoch:855 \t loss: 0.135026\n",
      "Epoch:856 \t loss: 0.068062\n",
      "Epoch:857 \t loss: 0.154080\n",
      "Epoch:858 \t loss: 0.000071\n",
      "Epoch:859 \t loss: 0.000063\n",
      "Epoch:860 \t loss: 0.119781\n",
      "Epoch:861 \t loss: 0.071565\n",
      "Epoch:862 \t loss: 0.041064\n",
      "Epoch:863 \t loss: 0.000523\n",
      "Epoch:864 \t loss: 0.127374\n",
      "Epoch:865 \t loss: 0.139983\n",
      "Epoch:866 \t loss: 0.000186\n",
      "Epoch:867 \t loss: 0.038438\n",
      "Epoch:868 \t loss: 0.133353\n",
      "Epoch:869 \t loss: 0.122335\n",
      "Epoch:870 \t loss: 0.000026\n",
      "Epoch:871 \t loss: 0.032203\n",
      "Epoch:872 \t loss: 0.045070\n",
      "Epoch:873 \t loss: 0.039322\n",
      "Epoch:874 \t loss: 0.096580\n",
      "Epoch:875 \t loss: 0.100204\n",
      "Epoch:876 \t loss: 0.080494\n",
      "Epoch:877 \t loss: 0.109884\n",
      "Epoch:878 \t loss: 0.245497\n",
      "Epoch:879 \t loss: 0.043818\n",
      "Epoch:880 \t loss: 0.083628\n",
      "Epoch:881 \t loss: 0.071180\n",
      "Epoch:882 \t loss: 0.060612\n",
      "Epoch:883 \t loss: 0.155327\n",
      "Epoch:884 \t loss: 0.000134\n",
      "Epoch:885 \t loss: 0.001753\n",
      "Epoch:886 \t loss: 0.161519\n",
      "Epoch:887 \t loss: 0.088336\n",
      "Epoch:888 \t loss: 0.000054\n",
      "Epoch:889 \t loss: 0.058409\n",
      "Epoch:890 \t loss: 0.000081\n",
      "Epoch:891 \t loss: 0.000119\n",
      "Epoch:892 \t loss: 0.045141\n",
      "Epoch:893 \t loss: 0.183126\n",
      "Epoch:894 \t loss: 0.113666\n",
      "Epoch:895 \t loss: 0.058681\n",
      "Epoch:896 \t loss: 0.059698\n",
      "Epoch:897 \t loss: 0.000500\n",
      "Epoch:898 \t loss: 0.025599\n",
      "Epoch:899 \t loss: 0.050608\n",
      "Epoch:900 \t loss: 0.042197\n",
      "Epoch:901 \t loss: 0.146071\n",
      "Epoch:902 \t loss: 0.158620\n",
      "Epoch:903 \t loss: 0.000027\n",
      "Epoch:904 \t loss: 0.058699\n",
      "Epoch:905 \t loss: 0.000016\n",
      "Epoch:906 \t loss: 0.078617\n",
      "Epoch:907 \t loss: 0.042146\n",
      "Epoch:908 \t loss: 0.184943\n",
      "Epoch:909 \t loss: 0.099213\n",
      "Epoch:910 \t loss: 0.108225\n",
      "Epoch:911 \t loss: 0.066921\n",
      "Epoch:912 \t loss: 0.000041\n",
      "Epoch:913 \t loss: 0.000011\n",
      "Epoch:914 \t loss: 0.058144\n",
      "Epoch:915 \t loss: 0.054013\n",
      "Epoch:916 \t loss: 0.000023\n",
      "Epoch:917 \t loss: 0.000017\n",
      "Epoch:918 \t loss: 0.057327\n",
      "Epoch:919 \t loss: 0.100647\n",
      "Epoch:920 \t loss: 0.121381\n",
      "Epoch:921 \t loss: 0.226608\n",
      "Epoch:922 \t loss: 0.000522\n",
      "Epoch:923 \t loss: 0.061839\n",
      "Epoch:924 \t loss: 0.100761\n",
      "Epoch:925 \t loss: 0.002031\n",
      "Epoch:926 \t loss: 0.000026\n",
      "Epoch:927 \t loss: 0.058780\n",
      "Epoch:928 \t loss: 0.088267\n",
      "Epoch:929 \t loss: 0.097349\n",
      "Epoch:930 \t loss: 0.097964\n",
      "Epoch:931 \t loss: 0.001494\n",
      "Epoch:932 \t loss: 0.000015\n",
      "Epoch:933 \t loss: 0.057251\n",
      "Epoch:934 \t loss: 0.065343\n",
      "Epoch:935 \t loss: 0.154330\n",
      "Epoch:936 \t loss: 0.177764\n",
      "Epoch:937 \t loss: 0.176637\n",
      "Epoch:938 \t loss: 0.037421\n",
      "Epoch:939 \t loss: 0.124409\n",
      "Epoch:940 \t loss: 0.037454\n",
      "Epoch:941 \t loss: 0.000083\n",
      "Epoch:942 \t loss: 0.063524\n",
      "Epoch:943 \t loss: 0.093388\n",
      "Epoch:944 \t loss: 0.079745\n",
      "Epoch:945 \t loss: 0.132378\n",
      "Epoch:946 \t loss: 0.184731\n",
      "Epoch:947 \t loss: 0.038004\n",
      "Epoch:948 \t loss: 0.115711\n",
      "Epoch:949 \t loss: 0.040516\n",
      "Epoch:950 \t loss: 0.000055\n",
      "Epoch:951 \t loss: 0.113800\n",
      "Epoch:952 \t loss: 0.204631\n",
      "Epoch:953 \t loss: 0.051311\n",
      "Epoch:954 \t loss: 0.054401\n",
      "Epoch:955 \t loss: 0.047457\n",
      "Epoch:956 \t loss: 0.040781\n",
      "Epoch:957 \t loss: 0.032510\n",
      "Epoch:958 \t loss: 0.000030\n",
      "Epoch:959 \t loss: 0.103551\n",
      "Epoch:960 \t loss: 0.176281\n",
      "Epoch:961 \t loss: 0.069597\n",
      "Epoch:962 \t loss: 0.000851\n",
      "Epoch:963 \t loss: 0.000002\n",
      "Epoch:964 \t loss: 0.072658\n",
      "Epoch:965 \t loss: 0.000001\n",
      "Epoch:966 \t loss: 0.078838\n",
      "Epoch:967 \t loss: 0.037865\n",
      "Epoch:968 \t loss: 0.048431\n",
      "Epoch:969 \t loss: 0.000134\n",
      "Epoch:970 \t loss: 0.000387\n",
      "Epoch:971 \t loss: 0.000017\n",
      "Epoch:972 \t loss: 0.118621\n",
      "Epoch:973 \t loss: 0.000026\n",
      "Epoch:974 \t loss: 0.125297\n",
      "Epoch:975 \t loss: 0.176025\n",
      "Epoch:976 \t loss: 0.000151\n",
      "Epoch:977 \t loss: 0.024138\n",
      "Epoch:978 \t loss: 0.108869\n",
      "Epoch:979 \t loss: 0.127717\n",
      "Epoch:980 \t loss: 0.133649\n",
      "Epoch:981 \t loss: 0.081711\n",
      "Epoch:982 \t loss: 0.070750\n",
      "Epoch:983 \t loss: 0.048555\n",
      "Epoch:984 \t loss: 0.018262\n",
      "Epoch:985 \t loss: 0.000006\n",
      "Epoch:986 \t loss: 0.145391\n",
      "Epoch:987 \t loss: 0.227974\n",
      "Epoch:988 \t loss: 0.222900\n",
      "Epoch:989 \t loss: 0.158659\n",
      "Epoch:990 \t loss: 0.113198\n",
      "Epoch:991 \t loss: 0.066790\n",
      "Epoch:992 \t loss: 0.196721\n",
      "Epoch:993 \t loss: 0.000293\n",
      "Epoch:994 \t loss: 0.000007\n",
      "Epoch:995 \t loss: 0.071831\n",
      "Epoch:996 \t loss: 0.000053\n",
      "Epoch:997 \t loss: 0.045130\n",
      "Epoch:998 \t loss: 0.142822\n",
      "Epoch:999 \t loss: 0.087401\n",
      "Epoch:1000 \t loss: 0.049899\n",
      "Epoch:1001 \t loss: 0.000022\n",
      "Epoch:1002 \t loss: 0.000028\n",
      "Epoch:1003 \t loss: 0.000004\n",
      "Epoch:1004 \t loss: 0.188642\n",
      "Epoch:1005 \t loss: 0.026762\n",
      "Epoch:1006 \t loss: 0.000003\n",
      "Epoch:1007 \t loss: 0.023953\n",
      "Epoch:1008 \t loss: 0.000025\n",
      "Epoch:1009 \t loss: 0.033133\n",
      "Epoch:1010 \t loss: 0.000087\n",
      "Epoch:1011 \t loss: 0.083518\n",
      "Epoch:1012 \t loss: 0.122294\n",
      "Epoch:1013 \t loss: 0.212356\n",
      "Epoch:1014 \t loss: 0.000013\n",
      "Epoch:1015 \t loss: 0.066702\n",
      "Epoch:1016 \t loss: 0.104752\n",
      "Epoch:1017 \t loss: 0.000092\n",
      "Epoch:1018 \t loss: 0.051407\n",
      "Epoch:1019 \t loss: 0.000004\n",
      "Epoch:1020 \t loss: 0.092162\n",
      "Epoch:1021 \t loss: 0.101680\n",
      "Epoch:1022 \t loss: 0.041238\n",
      "Epoch:1023 \t loss: 0.048106\n",
      "Epoch:1024 \t loss: 0.064125\n",
      "Epoch:1025 \t loss: 0.023276\n",
      "Epoch:1026 \t loss: 0.000267\n",
      "Epoch:1027 \t loss: 0.030478\n",
      "Epoch:1028 \t loss: 0.001184\n",
      "Epoch:1029 \t loss: 0.033638\n",
      "Epoch:1030 \t loss: 0.125634\n",
      "Epoch:1031 \t loss: 0.083346\n",
      "Epoch:1032 \t loss: 0.032831\n",
      "Epoch:1033 \t loss: 0.037315\n",
      "Epoch:1034 \t loss: 0.000027\n",
      "Epoch:1035 \t loss: 0.033570\n",
      "Epoch:1036 \t loss: 0.040761\n",
      "Epoch:1037 \t loss: 0.000079\n",
      "Epoch:1038 \t loss: 0.061705\n",
      "Epoch:1039 \t loss: 0.056004\n",
      "Epoch:1040 \t loss: 0.089323\n",
      "Epoch:1041 \t loss: 0.000012\n",
      "Epoch:1042 \t loss: 0.106607\n",
      "Epoch:1043 \t loss: 0.000003\n",
      "Epoch:1044 \t loss: 0.047871\n",
      "Epoch:1045 \t loss: 0.069650\n",
      "Epoch:1046 \t loss: 0.168340\n",
      "Epoch:1047 \t loss: 0.065011\n",
      "Epoch:1048 \t loss: 0.096092\n",
      "Epoch:1049 \t loss: 0.048549\n",
      "Epoch:1050 \t loss: 0.105474\n",
      "Epoch:1051 \t loss: 0.000010\n",
      "Epoch:1052 \t loss: 0.137714\n",
      "Epoch:1053 \t loss: 0.186269\n",
      "Epoch:1054 \t loss: 0.038200\n",
      "Epoch:1055 \t loss: 0.027759\n",
      "Epoch:1056 \t loss: 0.000015\n",
      "Epoch:1057 \t loss: 0.137492\n",
      "Epoch:1058 \t loss: 0.029539\n",
      "Epoch:1059 \t loss: 0.111641\n",
      "Epoch:1060 \t loss: 0.000019\n",
      "Epoch:1061 \t loss: 0.022475\n",
      "Epoch:1062 \t loss: 0.163722\n",
      "Epoch:1063 \t loss: 0.135993\n",
      "Epoch:1064 \t loss: 0.081819\n",
      "Epoch:1065 \t loss: 0.054207\n",
      "Epoch:1066 \t loss: 0.041236\n",
      "Epoch:1067 \t loss: 0.000090\n",
      "Epoch:1068 \t loss: 0.000218\n",
      "Epoch:1069 \t loss: 0.055253\n",
      "Epoch:1070 \t loss: 0.025080\n",
      "Epoch:1071 \t loss: 0.000012\n",
      "Epoch:1072 \t loss: 0.135307\n",
      "Epoch:1073 \t loss: 0.000073\n",
      "Epoch:1074 \t loss: 0.000004\n",
      "Epoch:1075 \t loss: 0.000492\n",
      "Epoch:1076 \t loss: 0.000005\n",
      "Epoch:1077 \t loss: 0.066469\n",
      "Epoch:1078 \t loss: 0.046105\n",
      "Epoch:1079 \t loss: 0.124255\n",
      "Epoch:1080 \t loss: 0.001317\n",
      "Epoch:1081 \t loss: 0.077297\n",
      "Epoch:1082 \t loss: 0.103326\n",
      "Epoch:1083 \t loss: 0.109815\n",
      "Epoch:1084 \t loss: 0.094650\n",
      "Epoch:1085 \t loss: 0.096357\n",
      "Epoch:1086 \t loss: 0.000026\n",
      "Epoch:1087 \t loss: 0.086834\n",
      "Epoch:1088 \t loss: 0.060794\n",
      "Epoch:1089 \t loss: 0.149820\n",
      "Epoch:1090 \t loss: 0.104017\n",
      "Epoch:1091 \t loss: 0.000356\n",
      "Epoch:1092 \t loss: 0.044375\n",
      "Epoch:1093 \t loss: 0.072441\n",
      "Epoch:1094 \t loss: 0.048458\n",
      "Epoch:1095 \t loss: 0.173666\n",
      "Epoch:1096 \t loss: 0.198238\n",
      "Epoch:1097 \t loss: 0.061286\n",
      "Epoch:1098 \t loss: 0.000026\n",
      "Epoch:1099 \t loss: 0.038960\n",
      "Epoch:1100 \t loss: 0.101878\n",
      "Epoch:1101 \t loss: 0.069274\n",
      "Epoch:1102 \t loss: 0.049726\n",
      "Epoch:1103 \t loss: 0.000011\n",
      "Epoch:1104 \t loss: 0.173148\n",
      "Epoch:1105 \t loss: 0.092374\n",
      "Epoch:1106 \t loss: 0.000014\n",
      "Epoch:1107 \t loss: 0.044022\n",
      "Epoch:1108 \t loss: 0.059250\n",
      "Epoch:1109 \t loss: 0.092245\n",
      "Epoch:1110 \t loss: 0.047552\n",
      "Epoch:1111 \t loss: 0.000112\n",
      "Epoch:1112 \t loss: 0.000046\n",
      "Epoch:1113 \t loss: 0.001142\n",
      "Epoch:1114 \t loss: 0.108959\n",
      "Epoch:1115 \t loss: 0.000054\n",
      "Epoch:1116 \t loss: 0.077437\n",
      "Epoch:1117 \t loss: 0.040617\n",
      "Epoch:1118 \t loss: 0.000028\n",
      "Epoch:1119 \t loss: 0.030547\n",
      "Epoch:1120 \t loss: 0.195022\n",
      "Epoch:1121 \t loss: 0.022857\n",
      "Epoch:1122 \t loss: 0.023013\n",
      "Epoch:1123 \t loss: 0.072117\n",
      "Epoch:1124 \t loss: 0.000011\n",
      "Epoch:1125 \t loss: 0.000073\n",
      "Epoch:1126 \t loss: 0.000026\n",
      "Epoch:1127 \t loss: 0.106888\n",
      "Epoch:1128 \t loss: 0.027336\n",
      "Epoch:1129 \t loss: 0.137037\n",
      "Epoch:1130 \t loss: 0.029088\n",
      "Epoch:1131 \t loss: 0.043071\n",
      "Epoch:1132 \t loss: 0.023875\n",
      "Epoch:1133 \t loss: 0.108525\n",
      "Epoch:1134 \t loss: 0.048881\n",
      "Epoch:1135 \t loss: 0.165751\n",
      "Epoch:1136 \t loss: 0.028626\n",
      "Epoch:1137 \t loss: 0.032734\n",
      "Epoch:1138 \t loss: 0.092950\n",
      "Epoch:1139 \t loss: 0.041718\n",
      "Epoch:1140 \t loss: 0.050581\n",
      "Epoch:1141 \t loss: 0.058275\n",
      "Epoch:1142 \t loss: 0.063240\n",
      "Epoch:1143 \t loss: 0.057575\n",
      "Epoch:1144 \t loss: 0.061777\n",
      "Epoch:1145 \t loss: 0.000029\n",
      "Epoch:1146 \t loss: 0.000819\n",
      "Epoch:1147 \t loss: 0.000197\n",
      "Epoch:1148 \t loss: 0.021438\n",
      "Epoch:1149 \t loss: 0.096937\n",
      "Epoch:1150 \t loss: 0.026695\n",
      "Epoch:1151 \t loss: 0.050332\n",
      "Epoch:1152 \t loss: 0.040984\n",
      "Epoch:1153 \t loss: 0.047046\n",
      "Epoch:1154 \t loss: 0.098422\n",
      "Epoch:1155 \t loss: 0.099897\n",
      "Epoch:1156 \t loss: 0.041465\n",
      "Epoch:1157 \t loss: 0.100470\n",
      "Epoch:1158 \t loss: 0.025861\n",
      "Epoch:1159 \t loss: 0.047732\n",
      "Epoch:1160 \t loss: 0.077090\n",
      "Epoch:1161 \t loss: 0.108413\n",
      "Epoch:1162 \t loss: 0.048303\n",
      "Epoch:1163 \t loss: 0.027842\n",
      "Epoch:1164 \t loss: 0.072139\n",
      "Epoch:1165 \t loss: 0.079948\n",
      "Epoch:1166 \t loss: 0.061956\n",
      "Epoch:1167 \t loss: 0.000039\n",
      "Epoch:1168 \t loss: 0.000078\n",
      "Epoch:1169 \t loss: 0.085309\n",
      "Epoch:1170 \t loss: 0.105883\n",
      "Epoch:1171 \t loss: 0.076729\n",
      "Epoch:1172 \t loss: 0.029556\n",
      "Epoch:1173 \t loss: 0.000211\n",
      "Epoch:1174 \t loss: 0.000204\n",
      "Epoch:1175 \t loss: 0.000055\n",
      "Epoch:1176 \t loss: 0.050114\n",
      "Epoch:1177 \t loss: 0.042875\n",
      "Epoch:1178 \t loss: 0.000005\n",
      "Epoch:1179 \t loss: 0.000027\n",
      "Epoch:1180 \t loss: 0.084670\n",
      "Epoch:1181 \t loss: 0.064422\n",
      "Epoch:1182 \t loss: 0.000029\n",
      "Epoch:1183 \t loss: 0.132554\n",
      "Epoch:1184 \t loss: 0.075156\n",
      "Epoch:1185 \t loss: 0.021742\n",
      "Epoch:1186 \t loss: 0.085156\n",
      "Epoch:1187 \t loss: 0.021356\n",
      "Epoch:1188 \t loss: 0.065895\n",
      "Epoch:1189 \t loss: 0.000005\n",
      "Epoch:1190 \t loss: 0.000018\n",
      "Epoch:1191 \t loss: 0.020941\n",
      "Epoch:1192 \t loss: 0.066122\n",
      "Epoch:1193 \t loss: 0.056641\n",
      "Epoch:1194 \t loss: 0.051162\n",
      "Epoch:1195 \t loss: 0.026321\n",
      "Epoch:1196 \t loss: 0.000004\n",
      "Epoch:1197 \t loss: 0.098595\n",
      "Epoch:1198 \t loss: 0.041533\n",
      "Epoch:1199 \t loss: 0.074150\n",
      "Epoch:1200 \t loss: 0.000004\n",
      "Epoch:1201 \t loss: 0.000004\n",
      "Epoch:1202 \t loss: 0.063117\n",
      "Epoch:1203 \t loss: 0.080765\n",
      "Epoch:1204 \t loss: 0.035481\n",
      "Epoch:1205 \t loss: 0.000591\n",
      "Epoch:1206 \t loss: 0.000280\n",
      "Epoch:1207 \t loss: 0.000115\n",
      "Epoch:1208 \t loss: 0.042826\n",
      "Epoch:1209 \t loss: 0.094489\n",
      "Epoch:1210 \t loss: 0.070789\n",
      "Epoch:1211 \t loss: 0.030229\n",
      "Epoch:1212 \t loss: 0.048714\n",
      "Epoch:1213 \t loss: 0.096397\n",
      "Epoch:1214 \t loss: 0.000027\n",
      "Epoch:1215 \t loss: 0.064978\n",
      "Epoch:1216 \t loss: 0.052593\n",
      "Epoch:1217 \t loss: 0.000040\n",
      "Epoch:1218 \t loss: 0.115675\n",
      "Epoch:1219 \t loss: 0.065709\n",
      "Epoch:1220 \t loss: 0.000681\n",
      "Epoch:1221 \t loss: 0.000206\n",
      "Epoch:1222 \t loss: 0.009983\n",
      "Epoch:1223 \t loss: 0.042171\n",
      "Epoch:1224 \t loss: 0.049219\n",
      "Epoch:1225 \t loss: 0.023698\n",
      "Epoch:1226 \t loss: 0.087228\n",
      "Epoch:1227 \t loss: 0.073591\n",
      "Epoch:1228 \t loss: 0.064311\n",
      "Epoch:1229 \t loss: 0.019026\n",
      "Epoch:1230 \t loss: 0.032319\n",
      "Epoch:1231 \t loss: 0.124617\n",
      "Epoch:1232 \t loss: 0.063703\n",
      "Epoch:1233 \t loss: 0.000036\n",
      "Epoch:1234 \t loss: 0.096430\n",
      "Epoch:1235 \t loss: 0.000041\n",
      "Epoch:1236 \t loss: 0.142664\n",
      "Epoch:1237 \t loss: 0.000013\n",
      "Epoch:1238 \t loss: 0.193931\n",
      "Epoch:1239 \t loss: 0.074625\n",
      "Epoch:1240 \t loss: 0.000006\n",
      "Epoch:1241 \t loss: 0.000008\n",
      "Epoch:1242 \t loss: 0.071207\n",
      "Epoch:1243 \t loss: 0.113500\n",
      "Epoch:1244 \t loss: 0.000025\n",
      "Epoch:1245 \t loss: 0.050142\n",
      "Epoch:1246 \t loss: 0.000053\n",
      "Epoch:1247 \t loss: 0.015022\n",
      "Epoch:1248 \t loss: 0.105933\n",
      "Epoch:1249 \t loss: 0.070240\n",
      "Epoch:1250 \t loss: 0.081560\n",
      "Epoch:1251 \t loss: 0.000330\n",
      "Epoch:1252 \t loss: 0.000003\n",
      "Epoch:1253 \t loss: 0.061080\n",
      "Epoch:1254 \t loss: 0.000785\n",
      "Epoch:1255 \t loss: 0.157253\n",
      "Epoch:1256 \t loss: 0.074004\n",
      "Epoch:1257 \t loss: 0.000036\n",
      "Epoch:1258 \t loss: 0.099059\n",
      "Epoch:1259 \t loss: 0.090362\n",
      "Epoch:1260 \t loss: 0.050131\n",
      "Epoch:1261 \t loss: 0.019332\n",
      "Epoch:1262 \t loss: 0.000005\n",
      "Epoch:1263 \t loss: 0.000009\n",
      "Epoch:1264 \t loss: 0.191211\n",
      "Epoch:1265 \t loss: 0.000016\n",
      "Epoch:1266 \t loss: 0.116319\n",
      "Epoch:1267 \t loss: 0.138259\n",
      "Epoch:1268 \t loss: 0.022711\n",
      "Epoch:1269 \t loss: 0.083751\n",
      "Epoch:1270 \t loss: 0.000025\n",
      "Epoch:1271 \t loss: 0.064408\n",
      "Epoch:1272 \t loss: 0.000009\n",
      "Epoch:1273 \t loss: 0.047218\n",
      "Epoch:1274 \t loss: 0.050548\n",
      "Epoch:1275 \t loss: 0.028372\n",
      "Epoch:1276 \t loss: 0.040687\n",
      "Epoch:1277 \t loss: 0.068794\n",
      "Epoch:1278 \t loss: 0.044507\n",
      "Epoch:1279 \t loss: 0.080993\n",
      "Epoch:1280 \t loss: 0.143667\n",
      "Epoch:1281 \t loss: 0.000042\n",
      "Epoch:1282 \t loss: 0.057186\n",
      "Epoch:1283 \t loss: 0.000026\n",
      "Epoch:1284 \t loss: 0.032459\n",
      "Epoch:1285 \t loss: 0.000017\n",
      "Epoch:1286 \t loss: 0.173594\n",
      "Epoch:1287 \t loss: 0.000018\n",
      "Epoch:1288 \t loss: 0.000317\n",
      "Epoch:1289 \t loss: 0.000004\n",
      "Epoch:1290 \t loss: 0.102914\n",
      "Epoch:1291 \t loss: 0.104051\n",
      "Epoch:1292 \t loss: 0.000062\n",
      "Epoch:1293 \t loss: 0.091427\n",
      "Epoch:1294 \t loss: 0.065411\n",
      "Epoch:1295 \t loss: 0.090407\n",
      "Epoch:1296 \t loss: 0.090560\n",
      "Epoch:1297 \t loss: 0.029936\n",
      "Epoch:1298 \t loss: 0.043554\n",
      "Epoch:1299 \t loss: 0.000067\n",
      "Epoch:1300 \t loss: 0.000540\n",
      "Epoch:1301 \t loss: 0.090449\n",
      "Epoch:1302 \t loss: 0.000001\n",
      "Epoch:1303 \t loss: 0.000010\n",
      "Epoch:1304 \t loss: 0.091659\n",
      "Epoch:1305 \t loss: 0.096319\n",
      "Epoch:1306 \t loss: 0.114016\n",
      "Epoch:1307 \t loss: 0.000018\n",
      "Epoch:1308 \t loss: 0.000001\n",
      "Epoch:1309 \t loss: 0.000034\n",
      "Epoch:1310 \t loss: 0.039116\n",
      "Epoch:1311 \t loss: 0.054591\n",
      "Epoch:1312 \t loss: 0.083022\n",
      "Epoch:1313 \t loss: 0.070362\n",
      "Epoch:1314 \t loss: 0.149591\n",
      "Epoch:1315 \t loss: 0.080367\n",
      "Epoch:1316 \t loss: 0.179745\n",
      "Epoch:1317 \t loss: 0.119762\n",
      "Epoch:1318 \t loss: 0.040498\n",
      "Epoch:1319 \t loss: 0.107110\n",
      "Epoch:1320 \t loss: 0.091310\n",
      "Epoch:1321 \t loss: 0.096912\n",
      "Epoch:1322 \t loss: 0.033160\n",
      "Epoch:1323 \t loss: 0.094672\n",
      "Epoch:1324 \t loss: 0.032130\n",
      "Epoch:1325 \t loss: 0.046310\n",
      "Epoch:1326 \t loss: 0.096798\n",
      "Epoch:1327 \t loss: 0.087284\n",
      "Epoch:1328 \t loss: 0.095660\n",
      "Epoch:1329 \t loss: 0.142426\n",
      "Epoch:1330 \t loss: 0.000003\n",
      "Epoch:1331 \t loss: 0.048946\n",
      "Epoch:1332 \t loss: 0.160428\n",
      "Epoch:1333 \t loss: 0.049871\n",
      "Epoch:1334 \t loss: 0.025941\n",
      "Epoch:1335 \t loss: 0.078190\n",
      "Epoch:1336 \t loss: 0.000012\n",
      "Epoch:1337 \t loss: 0.000317\n",
      "Epoch:1338 \t loss: 0.064702\n",
      "Epoch:1339 \t loss: 0.138951\n",
      "Epoch:1340 \t loss: 0.000001\n",
      "Epoch:1341 \t loss: 0.055490\n",
      "Epoch:1342 \t loss: 0.035363\n",
      "Epoch:1343 \t loss: 0.000043\n",
      "Epoch:1344 \t loss: 0.090485\n",
      "Epoch:1345 \t loss: 0.085917\n",
      "Epoch:1346 \t loss: 0.038416\n",
      "Epoch:1347 \t loss: 0.082228\n",
      "Epoch:1348 \t loss: 0.121787\n",
      "Epoch:1349 \t loss: 0.027943\n",
      "Epoch:1350 \t loss: 0.062430\n",
      "Epoch:1351 \t loss: 0.040349\n",
      "Epoch:1352 \t loss: 0.045489\n",
      "Epoch:1353 \t loss: 0.000067\n",
      "Epoch:1354 \t loss: 0.168402\n",
      "Epoch:1355 \t loss: 0.000001\n",
      "Epoch:1356 \t loss: 0.066238\n",
      "Epoch:1357 \t loss: 0.153430\n",
      "Epoch:1358 \t loss: 0.000096\n",
      "Epoch:1359 \t loss: 0.000574\n",
      "Epoch:1360 \t loss: 0.000005\n",
      "Epoch:1361 \t loss: 0.019168\n",
      "Epoch:1362 \t loss: 0.220954\n",
      "Epoch:1363 \t loss: 0.118057\n",
      "Epoch:1364 \t loss: 0.001188\n",
      "Epoch:1365 \t loss: 0.095039\n",
      "Epoch:1366 \t loss: 0.065370\n",
      "Epoch:1367 \t loss: 0.085031\n",
      "Epoch:1368 \t loss: 0.000001\n",
      "Epoch:1369 \t loss: 0.168465\n",
      "Epoch:1370 \t loss: 0.046547\n",
      "Epoch:1371 \t loss: 0.029886\n",
      "Epoch:1372 \t loss: 0.000012\n",
      "Epoch:1373 \t loss: 0.163579\n",
      "Epoch:1374 \t loss: 0.053054\n",
      "Epoch:1375 \t loss: 0.035096\n",
      "Epoch:1376 \t loss: 0.000006\n",
      "Epoch:1377 \t loss: 0.000084\n",
      "Epoch:1378 \t loss: 0.000007\n",
      "Epoch:1379 \t loss: 0.118312\n",
      "Epoch:1380 \t loss: 0.092238\n",
      "Epoch:1381 \t loss: 0.000169\n",
      "Epoch:1382 \t loss: 0.084634\n",
      "Epoch:1383 \t loss: 0.191816\n",
      "Epoch:1384 \t loss: 0.200687\n",
      "Epoch:1385 \t loss: 0.000106\n",
      "Epoch:1386 \t loss: 0.050345\n",
      "Epoch:1387 \t loss: 0.068911\n",
      "Epoch:1388 \t loss: 0.023571\n",
      "Epoch:1389 \t loss: 0.000007\n",
      "Epoch:1390 \t loss: 0.036914\n",
      "Epoch:1391 \t loss: 0.067877\n",
      "Epoch:1392 \t loss: 0.026438\n",
      "Epoch:1393 \t loss: 0.029604\n",
      "Epoch:1394 \t loss: 0.107004\n",
      "Epoch:1395 \t loss: 0.110116\n",
      "Epoch:1396 \t loss: 0.000004\n",
      "Epoch:1397 \t loss: 0.000197\n",
      "Epoch:1398 \t loss: 0.117106\n",
      "Epoch:1399 \t loss: 0.058072\n",
      "Epoch:1400 \t loss: 0.073076\n",
      "Epoch:1401 \t loss: 0.034578\n",
      "Epoch:1402 \t loss: 0.049521\n",
      "Epoch:1403 \t loss: 0.120338\n",
      "Epoch:1404 \t loss: 0.032229\n",
      "Epoch:1405 \t loss: 0.002333\n",
      "Epoch:1406 \t loss: 0.090606\n",
      "Epoch:1407 \t loss: 0.035654\n",
      "Epoch:1408 \t loss: 0.042896\n",
      "Epoch:1409 \t loss: 0.132060\n",
      "Epoch:1410 \t loss: 0.165898\n",
      "Epoch:1411 \t loss: 0.084775\n",
      "Epoch:1412 \t loss: 0.108869\n",
      "Epoch:1413 \t loss: 0.064431\n",
      "Epoch:1414 \t loss: 0.049651\n",
      "Epoch:1415 \t loss: 0.000004\n",
      "Epoch:1416 \t loss: 0.000005\n",
      "Epoch:1417 \t loss: 0.034398\n",
      "Epoch:1418 \t loss: 0.000005\n",
      "Epoch:1419 \t loss: 0.026476\n",
      "Epoch:1420 \t loss: 0.000021\n",
      "Epoch:1421 \t loss: 0.123288\n",
      "Epoch:1422 \t loss: 0.067565\n",
      "Epoch:1423 \t loss: 0.023417\n",
      "Epoch:1424 \t loss: 0.065792\n",
      "Epoch:1425 \t loss: 0.000003\n",
      "Epoch:1426 \t loss: 0.081497\n",
      "Epoch:1427 \t loss: 0.068301\n",
      "Epoch:1428 \t loss: 0.030809\n",
      "Epoch:1429 \t loss: 0.000003\n",
      "Epoch:1430 \t loss: 0.039745\n",
      "Epoch:1431 \t loss: 0.030337\n",
      "Epoch:1432 \t loss: 0.000010\n",
      "Epoch:1433 \t loss: 0.077042\n",
      "Epoch:1434 \t loss: 0.040276\n",
      "Epoch:1435 \t loss: 0.051907\n",
      "Epoch:1436 \t loss: 0.043867\n",
      "Epoch:1437 \t loss: 0.000002\n",
      "Epoch:1438 \t loss: 0.200386\n",
      "Epoch:1439 \t loss: 0.113413\n",
      "Epoch:1440 \t loss: 0.000050\n",
      "Epoch:1441 \t loss: 0.094401\n",
      "Epoch:1442 \t loss: 0.035719\n",
      "Epoch:1443 \t loss: 0.000378\n",
      "Epoch:1444 \t loss: 0.000194\n",
      "Epoch:1445 \t loss: 0.080970\n",
      "Epoch:1446 \t loss: 0.075656\n",
      "Epoch:1447 \t loss: 0.000002\n",
      "Epoch:1448 \t loss: 0.057109\n",
      "Epoch:1449 \t loss: 0.091542\n",
      "Epoch:1450 \t loss: 0.102163\n",
      "Epoch:1451 \t loss: 0.032389\n",
      "Epoch:1452 \t loss: 0.141849\n",
      "Epoch:1453 \t loss: 0.000006\n",
      "Epoch:1454 \t loss: 0.000004\n",
      "Epoch:1455 \t loss: 0.072402\n",
      "Epoch:1456 \t loss: 0.000002\n",
      "Epoch:1457 \t loss: 0.020527\n",
      "Epoch:1458 \t loss: 0.064337\n",
      "Epoch:1459 \t loss: 0.000078\n",
      "Epoch:1460 \t loss: 0.000002\n",
      "Epoch:1461 \t loss: 0.072460\n",
      "Epoch:1462 \t loss: 0.112741\n",
      "Epoch:1463 \t loss: 0.000002\n",
      "Epoch:1464 \t loss: 0.000006\n",
      "Epoch:1465 \t loss: 0.122325\n",
      "Epoch:1466 \t loss: 0.060437\n",
      "Epoch:1467 \t loss: 0.031245\n",
      "Epoch:1468 \t loss: 0.025425\n",
      "Epoch:1469 \t loss: 0.046324\n",
      "Epoch:1470 \t loss: 0.060971\n",
      "Epoch:1471 \t loss: 0.093983\n",
      "Epoch:1472 \t loss: 0.068636\n",
      "Epoch:1473 \t loss: 0.035466\n",
      "Epoch:1474 \t loss: 0.008717\n",
      "Epoch:1475 \t loss: 0.157830\n",
      "Epoch:1476 \t loss: 0.000160\n",
      "Epoch:1477 \t loss: 0.030916\n",
      "Epoch:1478 \t loss: 0.064251\n",
      "Epoch:1479 \t loss: 0.085460\n",
      "Epoch:1480 \t loss: 0.000013\n",
      "Epoch:1481 \t loss: 0.000002\n",
      "Epoch:1482 \t loss: 0.072892\n",
      "Epoch:1483 \t loss: 0.085562\n",
      "Epoch:1484 \t loss: 0.228365\n",
      "Epoch:1485 \t loss: 0.086955\n",
      "Epoch:1486 \t loss: 0.040739\n",
      "Epoch:1487 \t loss: 0.077750\n",
      "Epoch:1488 \t loss: 0.051034\n",
      "Epoch:1489 \t loss: 0.000054\n",
      "Epoch:1490 \t loss: 0.000384\n",
      "Epoch:1491 \t loss: 0.095245\n",
      "Epoch:1492 \t loss: 0.000004\n",
      "Epoch:1493 \t loss: 0.146083\n",
      "Epoch:1494 \t loss: 0.024501\n",
      "Epoch:1495 \t loss: 0.087057\n",
      "Epoch:1496 \t loss: 0.033444\n",
      "Epoch:1497 \t loss: 0.034322\n",
      "Epoch:1498 \t loss: 0.051667\n",
      "Epoch:1499 \t loss: 0.141032\n",
      "Epoch:1500 \t loss: 0.145196\n",
      "Epoch:1501 \t loss: 0.051985\n",
      "Epoch:1502 \t loss: 0.000009\n",
      "Epoch:1503 \t loss: 0.103873\n",
      "Epoch:1504 \t loss: 0.021236\n",
      "Epoch:1505 \t loss: 0.057747\n",
      "Epoch:1506 \t loss: 0.000002\n",
      "Epoch:1507 \t loss: 0.060047\n",
      "Epoch:1508 \t loss: 0.063285\n",
      "Epoch:1509 \t loss: 0.051431\n",
      "Epoch:1510 \t loss: 0.081187\n",
      "Epoch:1511 \t loss: 0.000002\n",
      "Epoch:1512 \t loss: 0.120554\n",
      "Epoch:1513 \t loss: 0.069832\n",
      "Epoch:1514 \t loss: 0.000023\n",
      "Epoch:1515 \t loss: 0.126112\n",
      "Epoch:1516 \t loss: 0.062188\n",
      "Epoch:1517 \t loss: 0.065646\n",
      "Epoch:1518 \t loss: 0.024539\n",
      "Epoch:1519 \t loss: 0.000032\n",
      "Epoch:1520 \t loss: 0.000004\n",
      "Epoch:1521 \t loss: 0.112838\n",
      "Epoch:1522 \t loss: 0.000008\n",
      "Epoch:1523 \t loss: 0.047787\n",
      "Epoch:1524 \t loss: 0.000006\n",
      "Epoch:1525 \t loss: 0.000002\n",
      "Epoch:1526 \t loss: 0.043876\n",
      "Epoch:1527 \t loss: 0.084035\n",
      "Epoch:1528 \t loss: 0.103720\n",
      "Epoch:1529 \t loss: 0.075096\n",
      "Epoch:1530 \t loss: 0.027292\n",
      "Epoch:1531 \t loss: 0.154000\n",
      "Epoch:1532 \t loss: 0.063492\n",
      "Epoch:1533 \t loss: 0.071063\n",
      "Epoch:1534 \t loss: 0.047912\n",
      "Epoch:1535 \t loss: 0.047183\n",
      "Epoch:1536 \t loss: 0.038797\n",
      "Epoch:1537 \t loss: 0.000009\n",
      "Epoch:1538 \t loss: 0.057177\n",
      "Epoch:1539 \t loss: 0.155654\n",
      "Epoch:1540 \t loss: 0.147438\n",
      "Epoch:1541 \t loss: 0.039447\n",
      "Epoch:1542 \t loss: 0.024952\n",
      "Epoch:1543 \t loss: 0.092718\n",
      "Epoch:1544 \t loss: 0.099649\n",
      "Epoch:1545 \t loss: 0.000030\n",
      "Epoch:1546 \t loss: 0.089146\n",
      "Epoch:1547 \t loss: 0.048341\n",
      "Epoch:1548 \t loss: 0.094125\n",
      "Epoch:1549 \t loss: 0.000012\n",
      "Epoch:1550 \t loss: 0.000019\n",
      "Epoch:1551 \t loss: 0.093440\n",
      "Epoch:1552 \t loss: 0.076017\n",
      "Epoch:1553 \t loss: 0.130874\n",
      "Epoch:1554 \t loss: 0.050705\n",
      "Epoch:1555 \t loss: 0.052737\n",
      "Epoch:1556 \t loss: 0.066596\n",
      "Epoch:1557 \t loss: 0.071390\n",
      "Epoch:1558 \t loss: 0.086390\n",
      "Epoch:1559 \t loss: 0.108046\n",
      "Epoch:1560 \t loss: 0.062224\n",
      "Epoch:1561 \t loss: 0.000008\n",
      "Epoch:1562 \t loss: 0.000028\n",
      "Epoch:1563 \t loss: 0.096337\n",
      "Epoch:1564 \t loss: 0.045390\n",
      "Epoch:1565 \t loss: 0.059328\n",
      "Epoch:1566 \t loss: 0.164956\n",
      "Epoch:1567 \t loss: 0.083346\n",
      "Epoch:1568 \t loss: 0.035364\n",
      "Epoch:1569 \t loss: 0.000011\n",
      "Epoch:1570 \t loss: 0.000017\n",
      "Epoch:1571 \t loss: 0.018683\n",
      "Epoch:1572 \t loss: 0.060534\n",
      "Epoch:1573 \t loss: 0.036609\n",
      "Epoch:1574 \t loss: 0.077687\n",
      "Epoch:1575 \t loss: 0.036507\n",
      "Epoch:1576 \t loss: 0.145356\n",
      "Epoch:1577 \t loss: 0.000004\n",
      "Epoch:1578 \t loss: 0.000013\n",
      "Epoch:1579 \t loss: 0.000055\n",
      "Epoch:1580 \t loss: 0.000001\n",
      "Epoch:1581 \t loss: 0.065172\n",
      "Epoch:1582 \t loss: 0.000660\n",
      "Epoch:1583 \t loss: 0.000004\n",
      "Epoch:1584 \t loss: 0.041527\n",
      "Epoch:1585 \t loss: 0.062706\n",
      "Epoch:1586 \t loss: 0.000794\n",
      "Epoch:1587 \t loss: 0.136684\n",
      "Epoch:1588 \t loss: 0.062797\n",
      "Epoch:1589 \t loss: 0.115275\n",
      "Epoch:1590 \t loss: 0.073789\n",
      "Epoch:1591 \t loss: 0.061093\n",
      "Epoch:1592 \t loss: 0.039095\n",
      "Epoch:1593 \t loss: 0.034586\n",
      "Epoch:1594 \t loss: 0.075590\n",
      "Epoch:1595 \t loss: 0.013312\n",
      "Epoch:1596 \t loss: 0.099183\n",
      "Epoch:1597 \t loss: 0.000054\n",
      "Epoch:1598 \t loss: 0.000004\n",
      "Epoch:1599 \t loss: 0.000004\n",
      "Epoch:1600 \t loss: 0.035504\n",
      "Epoch:1601 \t loss: 0.120320\n",
      "Epoch:1602 \t loss: 0.000588\n",
      "Epoch:1603 \t loss: 0.000026\n",
      "Epoch:1604 \t loss: 0.078209\n",
      "Epoch:1605 \t loss: 0.000022\n",
      "Epoch:1606 \t loss: 0.004688\n",
      "Epoch:1607 \t loss: 0.086129\n",
      "Epoch:1608 \t loss: 0.094408\n",
      "Epoch:1609 \t loss: 0.102341\n",
      "Epoch:1610 \t loss: 0.094874\n",
      "Epoch:1611 \t loss: 0.024069\n",
      "Epoch:1612 \t loss: 0.068158\n",
      "Epoch:1613 \t loss: 0.104228\n",
      "Epoch:1614 \t loss: 0.028373\n",
      "Epoch:1615 \t loss: 0.000377\n",
      "Epoch:1616 \t loss: 0.000003\n",
      "Epoch:1617 \t loss: 0.031916\n",
      "Epoch:1618 \t loss: 0.028095\n",
      "Epoch:1619 \t loss: 0.046263\n",
      "Epoch:1620 \t loss: 0.000375\n",
      "Epoch:1621 \t loss: 0.065339\n",
      "Epoch:1622 \t loss: 0.000006\n",
      "Epoch:1623 \t loss: 0.000014\n",
      "Epoch:1624 \t loss: 0.018548\n",
      "Epoch:1625 \t loss: 0.000002\n",
      "Epoch:1626 \t loss: 0.080171\n",
      "Epoch:1627 \t loss: 0.122818\n",
      "Epoch:1628 \t loss: 0.035159\n",
      "Epoch:1629 \t loss: 0.000164\n",
      "Epoch:1630 \t loss: 0.131748\n",
      "Epoch:1631 \t loss: 0.000064\n",
      "Epoch:1632 \t loss: 0.000011\n",
      "Epoch:1633 \t loss: 0.103300\n",
      "Epoch:1634 \t loss: 0.039143\n",
      "Epoch:1635 \t loss: 0.018582\n",
      "Epoch:1636 \t loss: 0.020284\n",
      "Epoch:1637 \t loss: 0.000031\n",
      "Epoch:1638 \t loss: 0.052188\n",
      "Epoch:1639 \t loss: 0.000052\n",
      "Epoch:1640 \t loss: 0.119027\n",
      "Epoch:1641 \t loss: 0.000003\n",
      "Epoch:1642 \t loss: 0.089209\n",
      "Epoch:1643 \t loss: 0.000306\n",
      "Epoch:1644 \t loss: 0.070679\n",
      "Epoch:1645 \t loss: 0.000021\n",
      "Epoch:1646 \t loss: 0.028697\n",
      "Epoch:1647 \t loss: 0.000010\n",
      "Epoch:1648 \t loss: 0.081754\n",
      "Epoch:1649 \t loss: 0.117716\n",
      "Epoch:1650 \t loss: 0.000001\n",
      "Epoch:1651 \t loss: 0.065383\n",
      "Epoch:1652 \t loss: 0.094527\n",
      "Epoch:1653 \t loss: 0.000014\n",
      "Epoch:1654 \t loss: 0.093240\n",
      "Epoch:1655 \t loss: 0.023092\n",
      "Epoch:1656 \t loss: 0.064210\n",
      "Epoch:1657 \t loss: 0.000008\n",
      "Epoch:1658 \t loss: 0.000007\n",
      "Epoch:1659 \t loss: 0.096763\n",
      "Epoch:1660 \t loss: 0.079092\n",
      "Epoch:1661 \t loss: 0.025913\n",
      "Epoch:1662 \t loss: 0.075354\n",
      "Epoch:1663 \t loss: 0.188266\n",
      "Epoch:1664 \t loss: 0.113397\n",
      "Epoch:1665 \t loss: 0.000014\n",
      "Epoch:1666 \t loss: 0.065959\n",
      "Epoch:1667 \t loss: 0.066922\n",
      "Epoch:1668 \t loss: 0.087173\n",
      "Epoch:1669 \t loss: 0.059411\n",
      "Epoch:1670 \t loss: 0.000039\n",
      "Epoch:1671 \t loss: 0.000004\n",
      "Epoch:1672 \t loss: 0.048168\n",
      "Epoch:1673 \t loss: 0.194350\n",
      "Epoch:1674 \t loss: 0.038525\n",
      "Epoch:1675 \t loss: 0.217821\n",
      "Epoch:1676 \t loss: 0.000054\n",
      "Epoch:1677 \t loss: 0.079540\n",
      "Epoch:1678 \t loss: 0.231001\n",
      "Epoch:1679 \t loss: 0.094524\n",
      "Epoch:1680 \t loss: 0.000013\n",
      "Epoch:1681 \t loss: 0.000003\n",
      "Epoch:1682 \t loss: 0.066040\n",
      "Epoch:1683 \t loss: 0.115184\n",
      "Epoch:1684 \t loss: 0.057188\n",
      "Epoch:1685 \t loss: 0.036253\n",
      "Epoch:1686 \t loss: 0.140017\n",
      "Epoch:1687 \t loss: 0.000001\n",
      "Epoch:1688 \t loss: 0.048682\n",
      "Epoch:1689 \t loss: 0.115342\n",
      "Epoch:1690 \t loss: 0.075505\n",
      "Epoch:1691 \t loss: 0.000003\n",
      "Epoch:1692 \t loss: 0.000017\n",
      "Epoch:1693 \t loss: 0.000006\n",
      "Epoch:1694 \t loss: 0.101259\n",
      "Epoch:1695 \t loss: 0.008089\n",
      "Epoch:1696 \t loss: 0.057893\n",
      "Epoch:1697 \t loss: 0.000004\n",
      "Epoch:1698 \t loss: 0.117384\n",
      "Epoch:1699 \t loss: 0.023265\n",
      "Epoch:1700 \t loss: 0.021624\n",
      "Epoch:1701 \t loss: 0.173591\n",
      "Epoch:1702 \t loss: 0.065608\n",
      "Epoch:1703 \t loss: 0.105230\n",
      "Epoch:1704 \t loss: 0.000012\n",
      "Epoch:1705 \t loss: 0.000001\n",
      "Epoch:1706 \t loss: 0.081734\n",
      "Epoch:1707 \t loss: 0.000003\n",
      "Epoch:1708 \t loss: 0.029347\n",
      "Epoch:1709 \t loss: 0.000034\n",
      "Epoch:1710 \t loss: 0.037203\n",
      "Epoch:1711 \t loss: 0.000007\n",
      "Epoch:1712 \t loss: 0.109153\n",
      "Epoch:1713 \t loss: 0.000058\n",
      "Epoch:1714 \t loss: 0.084756\n",
      "Epoch:1715 \t loss: 0.063058\n",
      "Epoch:1716 \t loss: 0.000014\n",
      "Epoch:1717 \t loss: 0.122143\n",
      "Epoch:1718 \t loss: 0.000070\n",
      "Epoch:1719 \t loss: 0.062359\n",
      "Epoch:1720 \t loss: 0.031779\n",
      "Epoch:1721 \t loss: 0.000039\n",
      "Epoch:1722 \t loss: 0.071464\n",
      "Epoch:1723 \t loss: 0.025087\n",
      "Epoch:1724 \t loss: 0.034988\n",
      "Epoch:1725 \t loss: 0.000017\n",
      "Epoch:1726 \t loss: 0.000071\n",
      "Epoch:1727 \t loss: 0.000005\n",
      "Epoch:1728 \t loss: 0.000002\n",
      "Epoch:1729 \t loss: 0.100799\n",
      "Epoch:1730 \t loss: 0.048041\n",
      "Epoch:1731 \t loss: 0.079182\n",
      "Epoch:1732 \t loss: 0.000015\n",
      "Epoch:1733 \t loss: 0.000005\n",
      "Epoch:1734 \t loss: 0.066084\n",
      "Epoch:1735 \t loss: 0.042889\n",
      "Epoch:1736 \t loss: 0.104545\n",
      "Epoch:1737 \t loss: 0.000030\n",
      "Epoch:1738 \t loss: 0.018991\n",
      "Epoch:1739 \t loss: 0.123727\n",
      "Epoch:1740 \t loss: 0.000016\n",
      "Epoch:1741 \t loss: 0.027230\n",
      "Epoch:1742 \t loss: 0.000004\n",
      "Epoch:1743 \t loss: 0.132288\n",
      "Epoch:1744 \t loss: 0.041271\n",
      "Epoch:1745 \t loss: 0.034362\n",
      "Epoch:1746 \t loss: 0.000002\n",
      "Epoch:1747 \t loss: 0.158449\n",
      "Epoch:1748 \t loss: 0.051146\n",
      "Epoch:1749 \t loss: 0.070465\n",
      "Epoch:1750 \t loss: 0.000039\n",
      "Epoch:1751 \t loss: 0.000010\n",
      "Epoch:1752 \t loss: 0.036260\n",
      "Epoch:1753 \t loss: 0.108072\n",
      "Epoch:1754 \t loss: 0.154245\n",
      "Epoch:1755 \t loss: 0.028562\n",
      "Epoch:1756 \t loss: 0.029255\n",
      "Epoch:1757 \t loss: 0.000022\n",
      "Epoch:1758 \t loss: 0.027950\n",
      "Epoch:1759 \t loss: 0.000005\n",
      "Epoch:1760 \t loss: 0.093147\n",
      "Epoch:1761 \t loss: 0.036127\n",
      "Epoch:1762 \t loss: 0.045325\n",
      "Epoch:1763 \t loss: 0.028630\n",
      "Epoch:1764 \t loss: 0.160125\n",
      "Epoch:1765 \t loss: 0.057941\n",
      "Epoch:1766 \t loss: 0.061794\n",
      "Epoch:1767 \t loss: 0.125172\n",
      "Epoch:1768 \t loss: 0.000016\n",
      "Epoch:1769 \t loss: 0.000002\n",
      "Epoch:1770 \t loss: 0.147819\n",
      "Epoch:1771 \t loss: 0.078116\n",
      "Epoch:1772 \t loss: 0.123114\n",
      "Epoch:1773 \t loss: 0.000001\n",
      "Epoch:1774 \t loss: 0.117041\n",
      "Epoch:1775 \t loss: 0.055326\n",
      "Epoch:1776 \t loss: 0.253601\n",
      "Epoch:1777 \t loss: 0.116572\n",
      "Epoch:1778 \t loss: 0.068737\n",
      "Epoch:1779 \t loss: 0.000005\n",
      "Epoch:1780 \t loss: 0.045800\n",
      "Epoch:1781 \t loss: 0.118005\n",
      "Epoch:1782 \t loss: 0.000011\n",
      "Epoch:1783 \t loss: 0.030255\n",
      "Epoch:1784 \t loss: 0.047591\n",
      "Epoch:1785 \t loss: 0.000007\n",
      "Epoch:1786 \t loss: 0.000039\n",
      "Epoch:1787 \t loss: 0.103486\n",
      "Epoch:1788 \t loss: 0.000017\n",
      "Epoch:1789 \t loss: 0.031249\n",
      "Epoch:1790 \t loss: 0.531541\n",
      "Epoch:1791 \t loss: 0.000014\n",
      "Epoch:1792 \t loss: 0.032781\n",
      "Epoch:1793 \t loss: 0.000001\n",
      "Epoch:1794 \t loss: 0.157818\n",
      "Epoch:1795 \t loss: 0.000017\n",
      "Epoch:1796 \t loss: 0.000030\n",
      "Epoch:1797 \t loss: 0.147470\n",
      "Epoch:1798 \t loss: 0.052369\n",
      "Epoch:1799 \t loss: 0.037579\n",
      "Epoch:1800 \t loss: 0.192096\n",
      "Epoch:1801 \t loss: 0.059947\n",
      "Epoch:1802 \t loss: 0.000003\n",
      "Epoch:1803 \t loss: 0.000166\n",
      "Epoch:1804 \t loss: 0.100971\n",
      "Epoch:1805 \t loss: 0.000035\n",
      "Epoch:1806 \t loss: 0.020656\n",
      "Epoch:1807 \t loss: 0.085230\n",
      "Epoch:1808 \t loss: 0.115791\n",
      "Epoch:1809 \t loss: 0.000006\n",
      "Epoch:1810 \t loss: 0.077373\n",
      "Epoch:1811 \t loss: 0.097560\n",
      "Epoch:1812 \t loss: 0.089346\n",
      "Epoch:1813 \t loss: 0.034465\n",
      "Epoch:1814 \t loss: 0.000001\n",
      "Epoch:1815 \t loss: 0.104318\n",
      "Epoch:1816 \t loss: 0.000002\n",
      "Epoch:1817 \t loss: 0.183151\n",
      "Epoch:1818 \t loss: 0.000002\n",
      "Epoch:1819 \t loss: 0.041368\n",
      "Epoch:1820 \t loss: 0.049538\n",
      "Epoch:1821 \t loss: 0.097555\n",
      "Epoch:1822 \t loss: 0.275705\n",
      "Epoch:1823 \t loss: 0.000005\n",
      "Epoch:1824 \t loss: 0.000053\n",
      "Epoch:1825 \t loss: 0.034189\n",
      "Epoch:1826 \t loss: 0.047140\n",
      "Epoch:1827 \t loss: 0.000020\n",
      "Epoch:1828 \t loss: 0.059546\n",
      "Epoch:1829 \t loss: 0.025533\n",
      "Epoch:1830 \t loss: 0.055416\n",
      "Epoch:1831 \t loss: 0.000030\n",
      "Epoch:1832 \t loss: 0.077634\n",
      "Epoch:1833 \t loss: 0.060223\n",
      "Epoch:1834 \t loss: 0.000032\n",
      "Epoch:1835 \t loss: 0.000008\n",
      "Epoch:1836 \t loss: 0.098362\n",
      "Epoch:1837 \t loss: 0.000002\n",
      "Epoch:1838 \t loss: 0.000003\n",
      "Epoch:1839 \t loss: 0.062857\n",
      "Epoch:1840 \t loss: 0.000007\n",
      "Epoch:1841 \t loss: 0.091995\n",
      "Epoch:1842 \t loss: 0.038623\n",
      "Epoch:1843 \t loss: 0.000007\n",
      "Epoch:1844 \t loss: 0.067838\n",
      "Epoch:1845 \t loss: 0.117089\n",
      "Epoch:1846 \t loss: 0.022739\n",
      "Epoch:1847 \t loss: 0.101734\n",
      "Epoch:1848 \t loss: 0.074849\n",
      "Epoch:1849 \t loss: 0.000007\n",
      "Epoch:1850 \t loss: 0.050403\n",
      "Epoch:1851 \t loss: 0.000004\n",
      "Epoch:1852 \t loss: 0.113609\n",
      "Epoch:1853 \t loss: 0.000004\n",
      "Epoch:1854 \t loss: 0.053985\n",
      "Epoch:1855 \t loss: 0.019897\n",
      "Epoch:1856 \t loss: 0.122835\n",
      "Epoch:1857 \t loss: 0.055984\n",
      "Epoch:1858 \t loss: 0.000024\n",
      "Epoch:1859 \t loss: 0.122421\n",
      "Epoch:1860 \t loss: 0.101253\n",
      "Epoch:1861 \t loss: 0.118687\n",
      "Epoch:1862 \t loss: 0.058981\n",
      "Epoch:1863 \t loss: 0.000015\n",
      "Epoch:1864 \t loss: 0.091390\n",
      "Epoch:1865 \t loss: 0.120020\n",
      "Epoch:1866 \t loss: 0.071994\n",
      "Epoch:1867 \t loss: 0.138372\n",
      "Epoch:1868 \t loss: 0.072237\n",
      "Epoch:1869 \t loss: 0.034284\n",
      "Epoch:1870 \t loss: 0.000001\n",
      "Epoch:1871 \t loss: 0.025679\n",
      "Epoch:1872 \t loss: 0.043042\n",
      "Epoch:1873 \t loss: 0.083774\n",
      "Epoch:1874 \t loss: 0.144381\n",
      "Epoch:1875 \t loss: 0.061741\n",
      "Epoch:1876 \t loss: 0.038917\n",
      "Epoch:1877 \t loss: 0.019049\n",
      "Epoch:1878 \t loss: 0.073103\n",
      "Epoch:1879 \t loss: 0.000003\n",
      "Epoch:1880 \t loss: 0.234179\n",
      "Epoch:1881 \t loss: 0.080251\n",
      "Epoch:1882 \t loss: 0.064462\n",
      "Epoch:1883 \t loss: 0.000004\n",
      "Epoch:1884 \t loss: 0.068876\n",
      "Epoch:1885 \t loss: 0.045987\n",
      "Epoch:1886 \t loss: 0.118971\n",
      "Epoch:1887 \t loss: 0.021072\n",
      "Epoch:1888 \t loss: 0.021633\n",
      "Epoch:1889 \t loss: 0.077018\n",
      "Epoch:1890 \t loss: 0.092415\n",
      "Epoch:1891 \t loss: 0.003852\n",
      "Epoch:1892 \t loss: 0.000018\n",
      "Epoch:1893 \t loss: 0.023635\n",
      "Epoch:1894 \t loss: 0.000019\n",
      "Epoch:1895 \t loss: 0.128256\n",
      "Epoch:1896 \t loss: 0.026282\n",
      "Epoch:1897 \t loss: 0.000007\n",
      "Epoch:1898 \t loss: 0.000013\n",
      "Epoch:1899 \t loss: 0.035575\n",
      "Epoch:1900 \t loss: 0.149699\n",
      "Epoch:1901 \t loss: 0.000040\n",
      "Epoch:1902 \t loss: 0.026713\n",
      "Epoch:1903 \t loss: 0.039470\n",
      "Epoch:1904 \t loss: 0.210843\n",
      "Epoch:1905 \t loss: 0.039028\n",
      "Epoch:1906 \t loss: 0.000006\n",
      "Epoch:1907 \t loss: 0.083837\n",
      "Epoch:1908 \t loss: 0.000009\n",
      "Epoch:1909 \t loss: 0.031637\n",
      "Epoch:1910 \t loss: 0.077322\n",
      "Epoch:1911 \t loss: 0.084323\n",
      "Epoch:1912 \t loss: 0.000003\n",
      "Epoch:1913 \t loss: 0.090594\n",
      "Epoch:1914 \t loss: 0.039314\n",
      "Epoch:1915 \t loss: 0.109738\n",
      "Epoch:1916 \t loss: 0.077451\n",
      "Epoch:1917 \t loss: 0.000001\n",
      "Epoch:1918 \t loss: 0.093134\n",
      "Epoch:1919 \t loss: 0.040265\n",
      "Epoch:1920 \t loss: 0.000263\n",
      "Epoch:1921 \t loss: 0.000002\n",
      "Epoch:1922 \t loss: 0.095370\n",
      "Epoch:1923 \t loss: 0.065230\n",
      "Epoch:1924 \t loss: 0.051353\n",
      "Epoch:1925 \t loss: 0.122965\n",
      "Epoch:1926 \t loss: 0.104697\n",
      "Epoch:1927 \t loss: 0.062573\n",
      "Epoch:1928 \t loss: 0.025774\n",
      "Epoch:1929 \t loss: 0.000019\n",
      "Epoch:1930 \t loss: 0.000001\n",
      "Epoch:1931 \t loss: 0.041626\n",
      "Epoch:1932 \t loss: 0.022905\n",
      "Epoch:1933 \t loss: 0.119564\n",
      "Epoch:1934 \t loss: 0.119417\n",
      "Epoch:1935 \t loss: 0.000000\n",
      "Epoch:1936 \t loss: 0.000002\n",
      "Epoch:1937 \t loss: 0.000136\n",
      "Epoch:1938 \t loss: 0.000005\n",
      "Epoch:1939 \t loss: 0.032161\n",
      "Epoch:1940 \t loss: 0.039998\n",
      "Epoch:1941 \t loss: 0.000056\n",
      "Epoch:1942 \t loss: 0.081841\n",
      "Epoch:1943 \t loss: 0.000646\n",
      "Epoch:1944 \t loss: 0.075492\n",
      "Epoch:1945 \t loss: 0.038918\n",
      "Epoch:1946 \t loss: 0.038873\n",
      "Epoch:1947 \t loss: 0.057732\n",
      "Epoch:1948 \t loss: 0.051594\n",
      "Epoch:1949 \t loss: 0.041262\n",
      "Epoch:1950 \t loss: 0.066563\n",
      "Epoch:1951 \t loss: 0.131976\n",
      "Epoch:1952 \t loss: 0.080041\n",
      "Epoch:1953 \t loss: 0.029475\n",
      "Epoch:1954 \t loss: 0.047853\n",
      "Epoch:1955 \t loss: 0.061467\n",
      "Epoch:1956 \t loss: 0.000023\n",
      "Epoch:1957 \t loss: 0.026870\n",
      "Epoch:1958 \t loss: 0.109446\n",
      "Epoch:1959 \t loss: 0.052750\n",
      "Epoch:1960 \t loss: 0.000001\n",
      "Epoch:1961 \t loss: 0.098138\n",
      "Epoch:1962 \t loss: 0.000003\n",
      "Epoch:1963 \t loss: 0.015618\n",
      "Epoch:1964 \t loss: 0.000003\n",
      "Epoch:1965 \t loss: 0.000011\n",
      "Epoch:1966 \t loss: 0.092464\n",
      "Epoch:1967 \t loss: 0.071420\n",
      "Epoch:1968 \t loss: 0.218938\n",
      "Epoch:1969 \t loss: 0.000002\n",
      "Epoch:1970 \t loss: 0.000002\n",
      "Epoch:1971 \t loss: 0.041488\n",
      "Epoch:1972 \t loss: 0.000024\n",
      "Epoch:1973 \t loss: 0.000015\n",
      "Epoch:1974 \t loss: 0.100703\n",
      "Epoch:1975 \t loss: 0.073934\n",
      "Epoch:1976 \t loss: 0.041009\n",
      "Epoch:1977 \t loss: 0.027152\n",
      "Epoch:1978 \t loss: 0.067540\n",
      "Epoch:1979 \t loss: 0.135984\n",
      "Epoch:1980 \t loss: 0.086882\n",
      "Epoch:1981 \t loss: 0.044431\n",
      "Epoch:1982 \t loss: 0.090632\n",
      "Epoch:1983 \t loss: 0.065303\n",
      "Epoch:1984 \t loss: 0.041010\n",
      "Epoch:1985 \t loss: 0.066328\n",
      "Epoch:1986 \t loss: 0.066140\n",
      "Epoch:1987 \t loss: 0.036649\n",
      "Epoch:1988 \t loss: 0.060085\n",
      "Epoch:1989 \t loss: 0.027345\n",
      "Epoch:1990 \t loss: 0.000078\n",
      "Epoch:1991 \t loss: 0.082836\n",
      "Epoch:1992 \t loss: 0.051787\n",
      "Epoch:1993 \t loss: 0.022837\n",
      "Epoch:1994 \t loss: 0.057509\n",
      "Epoch:1995 \t loss: 0.000004\n",
      "Epoch:1996 \t loss: 0.081255\n",
      "Epoch:1997 \t loss: 0.101805\n",
      "Epoch:1998 \t loss: 0.026658\n",
      "Epoch:1999 \t loss: 0.090061\n",
      "Epoch:2000 \t loss: 0.077016\n",
      "Epoch:2001 \t loss: 0.000010\n",
      "Epoch:2002 \t loss: 0.085581\n",
      "Epoch:2003 \t loss: 0.036474\n",
      "Epoch:2004 \t loss: 0.163353\n",
      "Epoch:2005 \t loss: 0.048862\n",
      "Epoch:2006 \t loss: 0.140394\n",
      "Epoch:2007 \t loss: 0.000001\n",
      "Epoch:2008 \t loss: 0.000024\n",
      "Epoch:2009 \t loss: 0.034130\n",
      "Epoch:2010 \t loss: 0.000029\n",
      "Epoch:2011 \t loss: 0.048303\n",
      "Epoch:2012 \t loss: 0.000001\n",
      "Epoch:2013 \t loss: 0.032845\n",
      "Epoch:2014 \t loss: 0.166641\n",
      "Epoch:2015 \t loss: 0.098229\n",
      "Epoch:2016 \t loss: 0.000004\n",
      "Epoch:2017 \t loss: 0.055151\n",
      "Epoch:2018 \t loss: 0.097166\n",
      "Epoch:2019 \t loss: 0.047910\n",
      "Epoch:2020 \t loss: 0.174956\n",
      "Epoch:2021 \t loss: 0.127259\n",
      "Epoch:2022 \t loss: 0.169580\n",
      "Epoch:2023 \t loss: 0.032263\n",
      "Epoch:2024 \t loss: 0.000035\n",
      "Epoch:2025 \t loss: 0.000002\n",
      "Epoch:2026 \t loss: 0.079287\n",
      "Epoch:2027 \t loss: 0.054769\n",
      "Epoch:2028 \t loss: 0.000002\n",
      "Epoch:2029 \t loss: 0.020576\n",
      "Epoch:2030 \t loss: 0.024008\n",
      "Epoch:2031 \t loss: 0.000015\n",
      "Epoch:2032 \t loss: 0.038478\n",
      "Epoch:2033 \t loss: 0.039007\n",
      "Epoch:2034 \t loss: 0.000002\n",
      "Epoch:2035 \t loss: 0.000002\n",
      "Epoch:2036 \t loss: 0.103329\n",
      "Epoch:2037 \t loss: 0.000001\n",
      "Epoch:2038 \t loss: 0.000003\n",
      "Epoch:2039 \t loss: 0.039118\n",
      "Epoch:2040 \t loss: 0.020813\n",
      "Epoch:2041 \t loss: 0.024029\n",
      "Epoch:2042 \t loss: 0.022435\n",
      "Epoch:2043 \t loss: 0.101803\n",
      "Epoch:2044 \t loss: 0.030939\n",
      "Epoch:2045 \t loss: 0.000026\n",
      "Epoch:2046 \t loss: 0.099011\n",
      "Epoch:2047 \t loss: 0.000001\n",
      "Epoch:2048 \t loss: 0.000001\n",
      "Epoch:2049 \t loss: 0.087468\n",
      "Epoch:2050 \t loss: 0.059150\n",
      "Epoch:2051 \t loss: 0.000001\n",
      "Epoch:2052 \t loss: 0.036049\n",
      "Epoch:2053 \t loss: 0.037164\n",
      "Epoch:2054 \t loss: 0.067825\n",
      "Epoch:2055 \t loss: 0.027576\n",
      "Epoch:2056 \t loss: 0.033265\n",
      "Epoch:2057 \t loss: 0.000002\n",
      "Epoch:2058 \t loss: 0.085478\n",
      "Epoch:2059 \t loss: 0.092456\n",
      "Epoch:2060 \t loss: 0.076064\n",
      "Epoch:2061 \t loss: 0.000001\n",
      "Epoch:2062 \t loss: 0.061677\n",
      "Epoch:2063 \t loss: 0.022872\n",
      "Epoch:2064 \t loss: 0.078528\n",
      "Epoch:2065 \t loss: 0.037266\n",
      "Epoch:2066 \t loss: 0.102494\n",
      "Epoch:2067 \t loss: 0.043121\n",
      "Epoch:2068 \t loss: 0.000001\n",
      "Epoch:2069 \t loss: 0.000002\n",
      "Epoch:2070 \t loss: 0.085790\n",
      "Epoch:2071 \t loss: 0.091922\n",
      "Epoch:2072 \t loss: 0.000001\n",
      "Epoch:2073 \t loss: 0.092596\n",
      "Epoch:2074 \t loss: 0.000013\n",
      "Epoch:2075 \t loss: 0.000001\n",
      "Epoch:2076 \t loss: 0.138077\n",
      "Epoch:2077 \t loss: 0.064550\n",
      "Epoch:2078 \t loss: 0.044830\n",
      "Epoch:2079 \t loss: 0.068652\n",
      "Epoch:2080 \t loss: 0.000002\n",
      "Epoch:2081 \t loss: 0.075729\n",
      "Epoch:2082 \t loss: 0.109894\n",
      "Epoch:2083 \t loss: 0.067286\n",
      "Epoch:2084 \t loss: 0.158174\n",
      "Epoch:2085 \t loss: 0.057858\n",
      "Epoch:2086 \t loss: 0.000001\n",
      "Epoch:2087 \t loss: 0.000009\n",
      "Epoch:2088 \t loss: 0.045452\n",
      "Epoch:2089 \t loss: 0.040151\n",
      "Epoch:2090 \t loss: 0.046320\n",
      "Epoch:2091 \t loss: 0.170431\n",
      "Epoch:2092 \t loss: 0.072638\n",
      "Epoch:2093 \t loss: 0.071133\n",
      "Epoch:2094 \t loss: 0.034110\n",
      "Epoch:2095 \t loss: 0.146301\n",
      "Epoch:2096 \t loss: 0.059690\n",
      "Epoch:2097 \t loss: 0.000003\n",
      "Epoch:2098 \t loss: 0.000006\n",
      "Epoch:2099 \t loss: 0.139213\n",
      "Epoch:2100 \t loss: 0.056932\n",
      "Epoch:2101 \t loss: 0.136265\n",
      "Epoch:2102 \t loss: 0.027155\n",
      "Epoch:2103 \t loss: 0.025982\n",
      "Epoch:2104 \t loss: 0.000001\n",
      "Epoch:2105 \t loss: 0.079147\n",
      "Epoch:2106 \t loss: 0.048271\n",
      "Epoch:2107 \t loss: 0.111018\n",
      "Epoch:2108 \t loss: 0.127522\n",
      "Epoch:2109 \t loss: 0.808891\n",
      "Epoch:2110 \t loss: 0.000122\n",
      "Epoch:2111 \t loss: 0.163858\n",
      "Epoch:2112 \t loss: 0.000016\n",
      "Epoch:2113 \t loss: 0.120551\n",
      "Epoch:2114 \t loss: 0.049920\n",
      "Epoch:2115 \t loss: 0.071281\n",
      "Epoch:2116 \t loss: 0.000003\n",
      "Epoch:2117 \t loss: 0.049410\n",
      "Epoch:2118 \t loss: 0.063980\n",
      "Epoch:2119 \t loss: 0.000001\n",
      "Epoch:2120 \t loss: 0.000031\n",
      "Epoch:2121 \t loss: 0.017008\n",
      "Epoch:2122 \t loss: 0.059666\n",
      "Epoch:2123 \t loss: 0.036684\n",
      "Epoch:2124 \t loss: 0.072440\n",
      "Epoch:2125 \t loss: 0.026777\n",
      "Epoch:2126 \t loss: 0.044830\n",
      "Epoch:2127 \t loss: 0.065722\n",
      "Epoch:2128 \t loss: 0.066733\n",
      "Epoch:2129 \t loss: 0.029604\n",
      "Epoch:2130 \t loss: 0.000007\n",
      "Epoch:2131 \t loss: 0.052869\n",
      "Epoch:2132 \t loss: 0.095052\n",
      "Epoch:2133 \t loss: 0.040154\n",
      "Epoch:2134 \t loss: 0.000001\n",
      "Epoch:2135 \t loss: 0.000003\n",
      "Epoch:2136 \t loss: 0.188532\n",
      "Epoch:2137 \t loss: 0.039307\n",
      "Epoch:2138 \t loss: 0.019048\n",
      "Epoch:2139 \t loss: 0.051402\n",
      "Epoch:2140 \t loss: 0.000036\n",
      "Epoch:2141 \t loss: 0.052447\n",
      "Epoch:2142 \t loss: 0.000003\n",
      "Epoch:2143 \t loss: 0.124396\n",
      "Epoch:2144 \t loss: 0.000006\n",
      "Epoch:2145 \t loss: 0.035014\n",
      "Epoch:2146 \t loss: 0.101501\n",
      "Epoch:2147 \t loss: 0.114305\n",
      "Epoch:2148 \t loss: 0.045429\n",
      "Epoch:2149 \t loss: 0.000006\n",
      "Epoch:2150 \t loss: 0.027083\n",
      "Epoch:2151 \t loss: 0.048886\n",
      "Epoch:2152 \t loss: 0.101038\n",
      "Epoch:2153 \t loss: 0.000002\n",
      "Epoch:2154 \t loss: 0.181289\n",
      "Epoch:2155 \t loss: 0.172439\n",
      "Epoch:2156 \t loss: 0.142455\n",
      "Epoch:2157 \t loss: 0.024468\n",
      "Epoch:2158 \t loss: 0.000002\n",
      "Epoch:2159 \t loss: 0.000002\n",
      "Epoch:2160 \t loss: 0.000002\n",
      "Epoch:2161 \t loss: 0.135074\n",
      "Epoch:2162 \t loss: 0.000001\n",
      "Epoch:2163 \t loss: 0.095559\n",
      "Epoch:2164 \t loss: 0.027960\n",
      "Epoch:2165 \t loss: 0.049421\n",
      "Epoch:2166 \t loss: 0.000001\n",
      "Epoch:2167 \t loss: 0.000001\n",
      "Epoch:2168 \t loss: 0.000002\n",
      "Epoch:2169 \t loss: 0.000001\n",
      "Epoch:2170 \t loss: 0.000001\n",
      "Epoch:2171 \t loss: 0.030093\n",
      "Epoch:2172 \t loss: 0.093883\n",
      "Epoch:2173 \t loss: 0.022913\n",
      "Epoch:2174 \t loss: 0.065233\n",
      "Epoch:2175 \t loss: 0.029259\n",
      "Epoch:2176 \t loss: 0.000000\n",
      "Epoch:2177 \t loss: 0.074859\n",
      "Epoch:2178 \t loss: 0.000002\n",
      "Epoch:2179 \t loss: 0.094555\n",
      "Epoch:2180 \t loss: 0.029341\n",
      "Epoch:2181 \t loss: 0.051419\n",
      "Epoch:2182 \t loss: 0.127493\n",
      "Epoch:2183 \t loss: 0.058374\n",
      "Epoch:2184 \t loss: 0.081615\n",
      "Epoch:2185 \t loss: 0.079010\n",
      "Epoch:2186 \t loss: 0.088358\n",
      "Epoch:2187 \t loss: 0.093080\n",
      "Epoch:2188 \t loss: 0.096739\n",
      "Epoch:2189 \t loss: 0.034709\n",
      "Epoch:2190 \t loss: 0.000081\n",
      "Epoch:2191 \t loss: 0.070934\n",
      "Epoch:2192 \t loss: 0.023809\n",
      "Epoch:2193 \t loss: 0.128379\n",
      "Epoch:2194 \t loss: 0.000415\n",
      "Epoch:2195 \t loss: 0.105099\n",
      "Epoch:2196 \t loss: 0.019241\n",
      "Epoch:2197 \t loss: 0.000007\n",
      "Epoch:2198 \t loss: 0.042186\n",
      "Epoch:2199 \t loss: 0.024524\n",
      "Epoch:2200 \t loss: 0.033238\n",
      "Epoch:2201 \t loss: 0.059366\n",
      "Epoch:2202 \t loss: 0.036928\n",
      "Epoch:2203 \t loss: 0.143987\n",
      "Epoch:2204 \t loss: 0.000011\n",
      "Epoch:2205 \t loss: 0.035630\n",
      "Epoch:2206 \t loss: 0.101017\n",
      "Epoch:2207 \t loss: 0.000007\n",
      "Epoch:2208 \t loss: 0.034923\n",
      "Epoch:2209 \t loss: 0.103739\n",
      "Epoch:2210 \t loss: 0.032587\n",
      "Epoch:2211 \t loss: 0.000075\n",
      "Epoch:2212 \t loss: 0.029293\n",
      "Epoch:2213 \t loss: 0.000009\n",
      "Epoch:2214 \t loss: 0.076357\n",
      "Epoch:2215 \t loss: 0.128981\n",
      "Epoch:2216 \t loss: 0.000028\n",
      "Epoch:2217 \t loss: 0.184361\n",
      "Epoch:2218 \t loss: 0.105852\n",
      "Epoch:2219 \t loss: 0.000005\n",
      "Epoch:2220 \t loss: 0.056921\n",
      "Epoch:2221 \t loss: 0.071323\n",
      "Epoch:2222 \t loss: 0.000003\n",
      "Epoch:2223 \t loss: 0.027849\n",
      "Epoch:2224 \t loss: 0.129819\n",
      "Epoch:2225 \t loss: 0.025116\n",
      "Epoch:2226 \t loss: 0.000009\n",
      "Epoch:2227 \t loss: 0.054409\n",
      "Epoch:2228 \t loss: 0.060599\n",
      "Epoch:2229 \t loss: 0.041534\n",
      "Epoch:2230 \t loss: 0.074318\n",
      "Epoch:2231 \t loss: 0.128087\n",
      "Epoch:2232 \t loss: 0.060160\n",
      "Epoch:2233 \t loss: 0.103331\n",
      "Epoch:2234 \t loss: 0.034263\n",
      "Epoch:2235 \t loss: 0.000003\n",
      "Epoch:2236 \t loss: 0.000003\n",
      "Epoch:2237 \t loss: 0.000301\n",
      "Epoch:2238 \t loss: 0.030385\n",
      "Epoch:2239 \t loss: 0.080329\n",
      "Epoch:2240 \t loss: 0.130890\n",
      "Epoch:2241 \t loss: 0.112961\n",
      "Epoch:2242 \t loss: 0.094084\n",
      "Epoch:2243 \t loss: 0.026763\n",
      "Epoch:2244 \t loss: 0.093223\n",
      "Epoch:2245 \t loss: 0.082101\n",
      "Epoch:2246 \t loss: 0.028532\n",
      "Epoch:2247 \t loss: 0.000001\n",
      "Epoch:2248 \t loss: 0.048073\n",
      "Epoch:2249 \t loss: 0.019798\n",
      "Epoch:2250 \t loss: 0.105217\n",
      "Epoch:2251 \t loss: 0.124319\n",
      "Epoch:2252 \t loss: 0.157702\n",
      "Epoch:2253 \t loss: 0.097720\n",
      "Epoch:2254 \t loss: 0.061085\n",
      "Epoch:2255 \t loss: 0.000025\n",
      "Epoch:2256 \t loss: 0.070671\n",
      "Epoch:2257 \t loss: 0.036673\n",
      "Epoch:2258 \t loss: 0.055417\n",
      "Epoch:2259 \t loss: 0.074809\n",
      "Epoch:2260 \t loss: 0.000000\n",
      "Epoch:2261 \t loss: 0.000000\n",
      "Epoch:2262 \t loss: 0.048993\n",
      "Epoch:2263 \t loss: 0.000010\n",
      "Epoch:2264 \t loss: 0.036389\n",
      "Epoch:2265 \t loss: 0.031145\n",
      "Epoch:2266 \t loss: 0.023869\n",
      "Epoch:2267 \t loss: 0.061779\n",
      "Epoch:2268 \t loss: 0.019298\n",
      "Epoch:2269 \t loss: 0.025060\n",
      "Epoch:2270 \t loss: 0.078215\n",
      "Epoch:2271 \t loss: 0.058023\n",
      "Epoch:2272 \t loss: 0.095034\n",
      "Epoch:2273 \t loss: 0.068769\n",
      "Epoch:2274 \t loss: 0.132944\n",
      "Epoch:2275 \t loss: 0.000007\n",
      "Epoch:2276 \t loss: 0.030560\n",
      "Epoch:2277 \t loss: 0.054791\n",
      "Epoch:2278 \t loss: 0.038424\n",
      "Epoch:2279 \t loss: 0.000001\n",
      "Epoch:2280 \t loss: 0.035503\n",
      "Epoch:2281 \t loss: 0.052581\n",
      "Epoch:2282 \t loss: 0.043687\n",
      "Epoch:2283 \t loss: 0.041341\n",
      "Epoch:2284 \t loss: 0.031531\n",
      "Epoch:2285 \t loss: 0.000011\n",
      "Epoch:2286 \t loss: 0.000007\n",
      "Epoch:2287 \t loss: 0.117164\n",
      "Epoch:2288 \t loss: 0.000003\n",
      "Epoch:2289 \t loss: 0.091013\n",
      "Epoch:2290 \t loss: 0.029259\n",
      "Epoch:2291 \t loss: 0.019284\n",
      "Epoch:2292 \t loss: 0.000001\n",
      "Epoch:2293 \t loss: 0.000000\n",
      "Epoch:2294 \t loss: 0.000028\n",
      "Epoch:2295 \t loss: 0.000108\n",
      "Epoch:2296 \t loss: 0.029834\n",
      "Epoch:2297 \t loss: 0.130902\n",
      "Epoch:2298 \t loss: 0.017469\n",
      "Epoch:2299 \t loss: 0.000001\n",
      "Epoch:2300 \t loss: 0.052450\n",
      "Epoch:2301 \t loss: 0.082282\n",
      "Epoch:2302 \t loss: 0.036433\n",
      "Epoch:2303 \t loss: 0.188320\n",
      "Epoch:2304 \t loss: 0.024163\n",
      "Epoch:2305 \t loss: 0.000001\n",
      "Epoch:2306 \t loss: 0.026081\n",
      "Epoch:2307 \t loss: 0.292794\n",
      "Epoch:2308 \t loss: 0.007894\n",
      "Epoch:2309 \t loss: 0.087854\n",
      "Epoch:2310 \t loss: 0.158759\n",
      "Epoch:2311 \t loss: 0.030812\n",
      "Epoch:2312 \t loss: 0.047616\n",
      "Epoch:2313 \t loss: 0.038651\n",
      "Epoch:2314 \t loss: 0.000021\n",
      "Epoch:2315 \t loss: 0.026832\n",
      "Epoch:2316 \t loss: 0.000084\n",
      "Epoch:2317 \t loss: 0.042703\n",
      "Epoch:2318 \t loss: 0.000009\n",
      "Epoch:2319 \t loss: 0.102062\n",
      "Epoch:2320 \t loss: 0.000669\n",
      "Epoch:2321 \t loss: 0.077872\n",
      "Epoch:2322 \t loss: 0.000003\n",
      "Epoch:2323 \t loss: 0.064502\n",
      "Epoch:2324 \t loss: 0.047661\n",
      "Epoch:2325 \t loss: 0.133647\n",
      "Epoch:2326 \t loss: 0.072021\n",
      "Epoch:2327 \t loss: 0.000002\n",
      "Epoch:2328 \t loss: 0.000012\n",
      "Epoch:2329 \t loss: 0.024720\n",
      "Epoch:2330 \t loss: 0.049385\n",
      "Epoch:2331 \t loss: 0.183343\n",
      "Epoch:2332 \t loss: 0.048422\n",
      "Epoch:2333 \t loss: 0.000012\n",
      "Epoch:2334 \t loss: 0.120380\n",
      "Epoch:2335 \t loss: 0.053971\n",
      "Epoch:2336 \t loss: 0.000037\n",
      "Epoch:2337 \t loss: 0.000046\n",
      "Epoch:2338 \t loss: 0.068834\n",
      "Epoch:2339 \t loss: 0.076698\n",
      "Epoch:2340 \t loss: 0.079149\n",
      "Epoch:2341 \t loss: 0.022597\n",
      "Epoch:2342 \t loss: 0.147045\n",
      "Epoch:2343 \t loss: 0.000005\n",
      "Epoch:2344 \t loss: 0.065462\n",
      "Epoch:2345 \t loss: 0.060891\n",
      "Epoch:2346 \t loss: 0.124924\n",
      "Epoch:2347 \t loss: 0.049145\n",
      "Epoch:2348 \t loss: 0.025086\n",
      "Epoch:2349 \t loss: 0.079139\n",
      "Epoch:2350 \t loss: 0.041878\n",
      "Epoch:2351 \t loss: 0.000005\n",
      "Epoch:2352 \t loss: 0.103626\n",
      "Epoch:2353 \t loss: 0.000003\n",
      "Epoch:2354 \t loss: 0.000003\n",
      "Epoch:2355 \t loss: 0.038057\n",
      "Epoch:2356 \t loss: 0.000011\n",
      "Epoch:2357 \t loss: 0.179144\n",
      "Epoch:2358 \t loss: 0.066957\n",
      "Epoch:2359 \t loss: 0.019535\n",
      "Epoch:2360 \t loss: 0.000014\n",
      "Epoch:2361 \t loss: 0.000007\n",
      "Epoch:2362 \t loss: 0.042699\n",
      "Epoch:2363 \t loss: 0.000009\n",
      "Epoch:2364 \t loss: 0.058252\n",
      "Epoch:2365 \t loss: 0.035921\n",
      "Epoch:2366 \t loss: 0.044959\n",
      "Epoch:2367 \t loss: 0.000003\n",
      "Epoch:2368 \t loss: 0.000013\n",
      "Epoch:2369 \t loss: 0.030183\n",
      "Epoch:2370 \t loss: 0.022407\n",
      "Epoch:2371 \t loss: 0.129377\n",
      "Epoch:2372 \t loss: 0.051416\n",
      "Epoch:2373 \t loss: 0.000001\n",
      "Epoch:2374 \t loss: 0.043992\n",
      "Epoch:2375 \t loss: 0.153838\n",
      "Epoch:2376 \t loss: 0.074461\n",
      "Epoch:2377 \t loss: 0.054154\n",
      "Epoch:2378 \t loss: 0.067816\n",
      "Epoch:2379 \t loss: 0.154713\n",
      "Epoch:2380 \t loss: 0.037471\n",
      "Epoch:2381 \t loss: 0.047915\n",
      "Epoch:2382 \t loss: 0.077169\n",
      "Epoch:2383 \t loss: 0.046799\n",
      "Epoch:2384 \t loss: 0.000002\n",
      "Epoch:2385 \t loss: 0.000002\n",
      "Epoch:2386 \t loss: 0.032253\n",
      "Epoch:2387 \t loss: 0.052704\n",
      "Epoch:2388 \t loss: 0.033572\n",
      "Epoch:2389 \t loss: 0.130485\n",
      "Epoch:2390 \t loss: 0.000008\n",
      "Epoch:2391 \t loss: 0.022824\n",
      "Epoch:2392 \t loss: 0.095486\n",
      "Epoch:2393 \t loss: 0.043438\n",
      "Epoch:2394 \t loss: 0.045277\n",
      "Epoch:2395 \t loss: 0.085379\n",
      "Epoch:2396 \t loss: 0.047652\n",
      "Epoch:2397 \t loss: 0.000005\n",
      "Epoch:2398 \t loss: 0.081701\n",
      "Epoch:2399 \t loss: 0.000067\n",
      "Epoch:2400 \t loss: 0.092062\n",
      "Epoch:2401 \t loss: 0.000017\n",
      "Epoch:2402 \t loss: 0.040882\n",
      "Epoch:2403 \t loss: 0.201534\n",
      "Epoch:2404 \t loss: 0.140554\n",
      "Epoch:2405 \t loss: 0.000001\n",
      "Epoch:2406 \t loss: 0.000134\n",
      "Epoch:2407 \t loss: 0.029621\n",
      "Epoch:2408 \t loss: 0.000000\n",
      "Epoch:2409 \t loss: 0.101340\n",
      "Epoch:2410 \t loss: 0.064141\n",
      "Epoch:2411 \t loss: 0.111065\n",
      "Epoch:2412 \t loss: 0.027283\n",
      "Epoch:2413 \t loss: 0.067632\n",
      "Epoch:2414 \t loss: 0.036170\n",
      "Epoch:2415 \t loss: 0.116529\n",
      "Epoch:2416 \t loss: 0.022730\n",
      "Epoch:2417 \t loss: 0.019889\n",
      "Epoch:2418 \t loss: 0.028612\n",
      "Epoch:2419 \t loss: 0.088253\n",
      "Epoch:2420 \t loss: 0.101299\n",
      "Epoch:2421 \t loss: 0.000011\n",
      "Epoch:2422 \t loss: 0.069417\n",
      "Epoch:2423 \t loss: 0.025917\n",
      "Epoch:2424 \t loss: 0.150191\n",
      "Epoch:2425 \t loss: 0.052455\n",
      "Epoch:2426 \t loss: 0.080219\n",
      "Epoch:2427 \t loss: 0.000003\n",
      "Epoch:2428 \t loss: 0.201315\n",
      "Epoch:2429 \t loss: 0.031577\n",
      "Epoch:2430 \t loss: 0.023583\n",
      "Epoch:2431 \t loss: 0.068293\n",
      "Epoch:2432 \t loss: 0.071643\n",
      "Epoch:2433 \t loss: 0.074624\n",
      "Epoch:2434 \t loss: 0.052887\n",
      "Epoch:2435 \t loss: 0.000001\n",
      "Epoch:2436 \t loss: 0.074917\n",
      "Epoch:2437 \t loss: 0.107158\n",
      "Epoch:2438 \t loss: 0.047882\n",
      "Epoch:2439 \t loss: 0.000002\n",
      "Epoch:2440 \t loss: 0.074099\n",
      "Epoch:2441 \t loss: 0.000010\n",
      "Epoch:2442 \t loss: 0.000209\n",
      "Epoch:2443 \t loss: 0.000007\n",
      "Epoch:2444 \t loss: 0.053610\n",
      "Epoch:2445 \t loss: 0.000002\n",
      "Epoch:2446 \t loss: 0.039036\n",
      "Epoch:2447 \t loss: 0.000049\n",
      "Epoch:2448 \t loss: 0.052721\n",
      "Epoch:2449 \t loss: 0.147067\n",
      "Epoch:2450 \t loss: 0.070087\n",
      "Epoch:2451 \t loss: 0.050413\n",
      "Epoch:2452 \t loss: 0.000002\n",
      "Epoch:2453 \t loss: 0.036049\n",
      "Epoch:2454 \t loss: 0.000002\n",
      "Epoch:2455 \t loss: 0.028772\n",
      "Epoch:2456 \t loss: 0.044891\n",
      "Epoch:2457 \t loss: 0.000011\n",
      "Epoch:2458 \t loss: 0.150949\n",
      "Epoch:2459 \t loss: 0.042012\n",
      "Epoch:2460 \t loss: 0.100696\n",
      "Epoch:2461 \t loss: 0.055566\n",
      "Epoch:2462 \t loss: 0.000018\n",
      "Epoch:2463 \t loss: 0.000005\n",
      "Epoch:2464 \t loss: 0.000001\n",
      "Epoch:2465 \t loss: 0.176468\n",
      "Epoch:2466 \t loss: 0.053610\n",
      "Epoch:2467 \t loss: 0.000004\n",
      "Epoch:2468 \t loss: 0.025365\n",
      "Epoch:2469 \t loss: 0.000001\n",
      "Epoch:2470 \t loss: 0.088943\n",
      "Epoch:2471 \t loss: 0.086472\n",
      "Epoch:2472 \t loss: 0.102367\n",
      "Epoch:2473 \t loss: 0.067941\n",
      "Epoch:2474 \t loss: 0.095287\n",
      "Epoch:2475 \t loss: 0.000004\n",
      "Epoch:2476 \t loss: 0.031696\n",
      "Epoch:2477 \t loss: 0.038722\n",
      "Epoch:2478 \t loss: 0.027015\n",
      "Epoch:2479 \t loss: 0.095507\n",
      "Epoch:2480 \t loss: 0.061760\n",
      "Epoch:2481 \t loss: 0.038597\n",
      "Epoch:2482 \t loss: 0.082372\n",
      "Epoch:2483 \t loss: 0.027596\n",
      "Epoch:2484 \t loss: 0.130803\n",
      "Epoch:2485 \t loss: 0.000001\n",
      "Epoch:2486 \t loss: 0.025306\n",
      "Epoch:2487 \t loss: 0.000006\n",
      "Epoch:2488 \t loss: 0.001310\n",
      "Epoch:2489 \t loss: 0.115291\n",
      "Epoch:2490 \t loss: 0.123127\n",
      "Epoch:2491 \t loss: 0.076902\n",
      "Epoch:2492 \t loss: 0.040000\n",
      "Epoch:2493 \t loss: 0.049355\n",
      "Epoch:2494 \t loss: 0.000002\n",
      "Epoch:2495 \t loss: 0.050870\n",
      "Epoch:2496 \t loss: 0.000004\n",
      "Epoch:2497 \t loss: 0.022980\n",
      "Epoch:2498 \t loss: 0.044168\n",
      "Epoch:2499 \t loss: 0.037723\n",
      "Epoch:2500 \t loss: 0.022113\n",
      "Epoch:2501 \t loss: 0.000007\n",
      "Epoch:2502 \t loss: 0.020743\n",
      "Epoch:2503 \t loss: 0.000009\n",
      "Epoch:2504 \t loss: 0.000020\n",
      "Epoch:2505 \t loss: 0.047839\n",
      "Epoch:2506 \t loss: 0.053333\n",
      "Epoch:2507 \t loss: 0.000015\n",
      "Epoch:2508 \t loss: 0.028485\n",
      "Epoch:2509 \t loss: 0.000002\n",
      "Epoch:2510 \t loss: 0.000001\n",
      "Epoch:2511 \t loss: 0.000009\n",
      "Epoch:2512 \t loss: 0.123849\n",
      "Epoch:2513 \t loss: 0.000001\n",
      "Epoch:2514 \t loss: 0.028827\n",
      "Epoch:2515 \t loss: 0.017748\n",
      "Epoch:2516 \t loss: 0.056774\n",
      "Epoch:2517 \t loss: 0.026313\n",
      "Epoch:2518 \t loss: 0.138823\n",
      "Epoch:2519 \t loss: 0.095010\n",
      "Epoch:2520 \t loss: 0.045308\n",
      "Epoch:2521 \t loss: 0.033396\n",
      "Epoch:2522 \t loss: 0.065148\n",
      "Epoch:2523 \t loss: 0.000003\n",
      "Epoch:2524 \t loss: 0.096004\n",
      "Epoch:2525 \t loss: 0.000029\n",
      "Epoch:2526 \t loss: 0.000013\n",
      "Epoch:2527 \t loss: 0.000139\n",
      "Epoch:2528 \t loss: 0.114830\n",
      "Epoch:2529 \t loss: 0.030215\n",
      "Epoch:2530 \t loss: 0.071660\n",
      "Epoch:2531 \t loss: 0.000023\n",
      "Epoch:2532 \t loss: 0.023499\n",
      "Epoch:2533 \t loss: 0.039107\n",
      "Epoch:2534 \t loss: 0.148848\n",
      "Epoch:2535 \t loss: 0.020229\n",
      "Epoch:2536 \t loss: 0.000001\n",
      "Epoch:2537 \t loss: 0.000005\n",
      "Epoch:2538 \t loss: 0.062182\n",
      "Epoch:2539 \t loss: 0.092985\n",
      "Epoch:2540 \t loss: 0.038683\n",
      "Epoch:2541 \t loss: 0.114584\n",
      "Epoch:2542 \t loss: 0.143837\n",
      "Epoch:2543 \t loss: 0.090499\n",
      "Epoch:2544 \t loss: 0.108979\n",
      "Epoch:2545 \t loss: 0.000004\n",
      "Epoch:2546 \t loss: 0.029417\n",
      "Epoch:2547 \t loss: 0.019375\n",
      "Epoch:2548 \t loss: 0.121295\n",
      "Epoch:2549 \t loss: 0.025747\n",
      "Epoch:2550 \t loss: 0.000000\n",
      "Epoch:2551 \t loss: 0.051285\n",
      "Epoch:2552 \t loss: 0.000008\n",
      "Epoch:2553 \t loss: 0.066221\n",
      "Epoch:2554 \t loss: 0.000002\n",
      "Epoch:2555 \t loss: 0.112797\n",
      "Epoch:2556 \t loss: 0.184664\n",
      "Epoch:2557 \t loss: 0.000007\n",
      "Epoch:2558 \t loss: 0.000001\n",
      "Epoch:2559 \t loss: 0.022181\n",
      "Epoch:2560 \t loss: 0.087676\n",
      "Epoch:2561 \t loss: 0.105644\n",
      "Epoch:2562 \t loss: 0.040547\n",
      "Epoch:2563 \t loss: 0.071006\n",
      "Epoch:2564 \t loss: 0.022980\n",
      "Epoch:2565 \t loss: 0.040763\n",
      "Epoch:2566 \t loss: 0.082036\n",
      "Epoch:2567 \t loss: 0.106243\n",
      "Epoch:2568 \t loss: 0.000004\n",
      "Epoch:2569 \t loss: 0.088216\n",
      "Epoch:2570 \t loss: 0.000037\n",
      "Epoch:2571 \t loss: 0.053085\n",
      "Epoch:2572 \t loss: 0.100384\n",
      "Epoch:2573 \t loss: 0.076100\n",
      "Epoch:2574 \t loss: 0.000013\n",
      "Epoch:2575 \t loss: 0.050299\n",
      "Epoch:2576 \t loss: 0.068373\n",
      "Epoch:2577 \t loss: 0.062433\n",
      "Epoch:2578 \t loss: 0.175360\n",
      "Epoch:2579 \t loss: 0.083339\n",
      "Epoch:2580 \t loss: 0.097025\n",
      "Epoch:2581 \t loss: 0.050150\n",
      "Epoch:2582 \t loss: 0.000003\n",
      "Epoch:2583 \t loss: 0.044034\n",
      "Epoch:2584 \t loss: 0.087317\n",
      "Epoch:2585 \t loss: 0.065041\n",
      "Epoch:2586 \t loss: 0.000008\n",
      "Epoch:2587 \t loss: 0.071041\n",
      "Epoch:2588 \t loss: 0.089829\n",
      "Epoch:2589 \t loss: 0.084169\n",
      "Epoch:2590 \t loss: 0.000122\n",
      "Epoch:2591 \t loss: 0.049253\n",
      "Epoch:2592 \t loss: 0.066616\n",
      "Epoch:2593 \t loss: 0.000010\n",
      "Epoch:2594 \t loss: 0.108973\n",
      "Epoch:2595 \t loss: 0.059389\n",
      "Epoch:2596 \t loss: 0.066478\n",
      "Epoch:2597 \t loss: 0.000017\n",
      "Epoch:2598 \t loss: 0.134933\n",
      "Epoch:2599 \t loss: 0.092664\n",
      "Epoch:2600 \t loss: 0.072247\n",
      "Epoch:2601 \t loss: 0.022722\n",
      "Epoch:2602 \t loss: 0.031953\n",
      "Epoch:2603 \t loss: 0.095543\n",
      "Epoch:2604 \t loss: 0.028937\n",
      "Epoch:2605 \t loss: 0.000001\n",
      "Epoch:2606 \t loss: 0.063817\n",
      "Epoch:2607 \t loss: 0.117442\n",
      "Epoch:2608 \t loss: 0.131627\n",
      "Epoch:2609 \t loss: 0.104615\n",
      "Epoch:2610 \t loss: 0.043916\n",
      "Epoch:2611 \t loss: 0.074700\n",
      "Epoch:2612 \t loss: 0.000114\n",
      "Epoch:2613 \t loss: 0.000082\n",
      "Epoch:2614 \t loss: 0.000138\n",
      "Epoch:2615 \t loss: 0.034898\n",
      "Epoch:2616 \t loss: 0.065885\n",
      "Epoch:2617 \t loss: 0.026739\n",
      "Epoch:2618 \t loss: 0.000818\n",
      "Epoch:2619 \t loss: 0.000025\n",
      "Epoch:2620 \t loss: 0.000006\n",
      "Epoch:2621 \t loss: 0.000024\n",
      "Epoch:2622 \t loss: 0.089122\n",
      "Epoch:2623 \t loss: 0.031767\n",
      "Epoch:2624 \t loss: 0.077350\n",
      "Epoch:2625 \t loss: 0.087530\n",
      "Epoch:2626 \t loss: 0.000005\n",
      "Epoch:2627 \t loss: 0.048209\n",
      "Epoch:2628 \t loss: 0.000033\n",
      "Epoch:2629 \t loss: 0.000008\n",
      "Epoch:2630 \t loss: 0.144658\n",
      "Epoch:2631 \t loss: 0.000008\n",
      "Epoch:2632 \t loss: 0.108443\n",
      "Epoch:2633 \t loss: 0.025547\n",
      "Epoch:2634 \t loss: 0.000007\n",
      "Epoch:2635 \t loss: 0.092879\n",
      "Epoch:2636 \t loss: 0.000010\n",
      "Epoch:2637 \t loss: 0.036731\n",
      "Epoch:2638 \t loss: 0.078179\n",
      "Epoch:2639 \t loss: 0.073982\n",
      "Epoch:2640 \t loss: 0.042210\n",
      "Epoch:2641 \t loss: 0.068164\n",
      "Epoch:2642 \t loss: 0.149912\n",
      "Epoch:2643 \t loss: 0.126322\n",
      "Epoch:2644 \t loss: 0.000003\n",
      "Epoch:2645 \t loss: 0.055584\n",
      "Epoch:2646 \t loss: 0.000006\n",
      "Epoch:2647 \t loss: 0.049247\n",
      "Epoch:2648 \t loss: 0.022242\n",
      "Epoch:2649 \t loss: 0.024996\n",
      "Epoch:2650 \t loss: 0.000012\n",
      "Epoch:2651 \t loss: 0.000003\n",
      "Epoch:2652 \t loss: 0.154336\n",
      "Epoch:2653 \t loss: 0.137772\n",
      "Epoch:2654 \t loss: 0.000003\n",
      "Epoch:2655 \t loss: 0.035588\n",
      "Epoch:2656 \t loss: 0.133566\n",
      "Epoch:2657 \t loss: 0.092004\n",
      "Epoch:2658 \t loss: 0.056598\n",
      "Epoch:2659 \t loss: 0.061496\n",
      "Epoch:2660 \t loss: 0.100506\n",
      "Epoch:2661 \t loss: 0.081684\n",
      "Epoch:2662 \t loss: 0.028777\n",
      "Epoch:2663 \t loss: 0.063447\n",
      "Epoch:2664 \t loss: 0.042656\n",
      "Epoch:2665 \t loss: 0.034937\n",
      "Epoch:2666 \t loss: 0.042270\n",
      "Epoch:2667 \t loss: 0.000010\n",
      "Epoch:2668 \t loss: 0.026716\n",
      "Epoch:2669 \t loss: 0.000002\n",
      "Epoch:2670 \t loss: 0.023630\n",
      "Epoch:2671 \t loss: 0.065734\n",
      "Epoch:2672 \t loss: 0.077323\n",
      "Epoch:2673 \t loss: 0.061974\n",
      "Epoch:2674 \t loss: 0.000002\n",
      "Epoch:2675 \t loss: 0.037251\n",
      "Epoch:2676 \t loss: 0.000001\n",
      "Epoch:2677 \t loss: 0.107245\n",
      "Epoch:2678 \t loss: 0.059847\n",
      "Epoch:2679 \t loss: 0.040922\n",
      "Epoch:2680 \t loss: 0.082168\n",
      "Epoch:2681 \t loss: 0.031285\n",
      "Epoch:2682 \t loss: 0.026974\n",
      "Epoch:2683 \t loss: 0.024968\n",
      "Epoch:2684 \t loss: 0.043549\n",
      "Epoch:2685 \t loss: 0.046135\n",
      "Epoch:2686 \t loss: 0.000008\n",
      "Epoch:2687 \t loss: 0.065221\n",
      "Epoch:2688 \t loss: 0.064838\n",
      "Epoch:2689 \t loss: 0.023778\n",
      "Epoch:2690 \t loss: 0.000002\n",
      "Epoch:2691 \t loss: 0.042264\n",
      "Epoch:2692 \t loss: 0.026570\n",
      "Epoch:2693 \t loss: 0.000002\n",
      "Epoch:2694 \t loss: 0.023728\n",
      "Epoch:2695 \t loss: 0.022543\n",
      "Epoch:2696 \t loss: 0.000012\n",
      "Epoch:2697 \t loss: 0.000001\n",
      "Epoch:2698 \t loss: 0.000007\n",
      "Epoch:2699 \t loss: 0.114994\n",
      "Epoch:2700 \t loss: 0.023861\n",
      "Epoch:2701 \t loss: 0.029129\n",
      "Epoch:2702 \t loss: 0.044660\n",
      "Epoch:2703 \t loss: 0.052642\n",
      "Epoch:2704 \t loss: 0.134951\n",
      "Epoch:2705 \t loss: 0.069042\n",
      "Epoch:2706 \t loss: 0.096927\n",
      "Epoch:2707 \t loss: 0.000002\n",
      "Epoch:2708 \t loss: 0.053409\n",
      "Epoch:2709 \t loss: 0.075214\n",
      "Epoch:2710 \t loss: 0.000004\n",
      "Epoch:2711 \t loss: 0.042109\n",
      "Epoch:2712 \t loss: 0.023513\n",
      "Epoch:2713 \t loss: 0.150496\n",
      "Epoch:2714 \t loss: 0.255902\n",
      "Epoch:2715 \t loss: 0.025294\n",
      "Epoch:2716 \t loss: 0.000014\n",
      "Epoch:2717 \t loss: 0.052411\n",
      "Epoch:2718 \t loss: 0.045642\n",
      "Epoch:2719 \t loss: 0.048122\n",
      "Epoch:2720 \t loss: 0.030032\n",
      "Epoch:2721 \t loss: 0.018623\n",
      "Epoch:2722 \t loss: 0.071777\n",
      "Epoch:2723 \t loss: 0.047124\n",
      "Epoch:2724 \t loss: 0.112756\n",
      "Epoch:2725 \t loss: 0.022402\n",
      "Epoch:2726 \t loss: 0.045808\n",
      "Epoch:2727 \t loss: 0.063833\n",
      "Epoch:2728 \t loss: 0.041976\n",
      "Epoch:2729 \t loss: 0.196844\n",
      "Epoch:2730 \t loss: 0.023240\n",
      "Epoch:2731 \t loss: 0.208002\n",
      "Epoch:2732 \t loss: 0.021689\n",
      "Epoch:2733 \t loss: 0.122387\n",
      "Epoch:2734 \t loss: 0.042538\n",
      "Epoch:2735 \t loss: 0.000001\n",
      "Epoch:2736 \t loss: 0.027916\n",
      "Epoch:2737 \t loss: 0.083980\n",
      "Epoch:2738 \t loss: 0.000001\n",
      "Epoch:2739 \t loss: 0.000001\n",
      "Epoch:2740 \t loss: 0.112822\n",
      "Epoch:2741 \t loss: 0.049164\n",
      "Epoch:2742 \t loss: 0.000001\n",
      "Epoch:2743 \t loss: 0.078269\n",
      "Epoch:2744 \t loss: 0.000001\n",
      "Epoch:2745 \t loss: 0.084261\n",
      "Epoch:2746 \t loss: 0.000001\n",
      "Epoch:2747 \t loss: 0.064596\n",
      "Epoch:2748 \t loss: 0.024768\n",
      "Epoch:2749 \t loss: 0.116792\n",
      "Epoch:2750 \t loss: 0.035469\n",
      "Epoch:2751 \t loss: 0.121629\n",
      "Epoch:2752 \t loss: 0.027633\n",
      "Epoch:2753 \t loss: 0.000026\n",
      "Epoch:2754 \t loss: 0.000012\n",
      "Epoch:2755 \t loss: 0.021728\n",
      "Epoch:2756 \t loss: 0.082793\n",
      "Epoch:2757 \t loss: 0.105010\n",
      "Epoch:2758 \t loss: 0.079651\n",
      "Epoch:2759 \t loss: 0.000001\n",
      "Epoch:2760 \t loss: 0.058034\n",
      "Epoch:2761 \t loss: 0.000045\n",
      "Epoch:2762 \t loss: 0.148729\n",
      "Epoch:2763 \t loss: 0.024441\n",
      "Epoch:2764 \t loss: 0.027378\n",
      "Epoch:2765 \t loss: 0.100405\n",
      "Epoch:2766 \t loss: 0.103388\n",
      "Epoch:2767 \t loss: 0.179309\n",
      "Epoch:2768 \t loss: 0.080731\n",
      "Epoch:2769 \t loss: 0.046670\n",
      "Epoch:2770 \t loss: 0.046553\n",
      "Epoch:2771 \t loss: 0.025697\n",
      "Epoch:2772 \t loss: 0.025764\n",
      "Epoch:2773 \t loss: 0.000004\n",
      "Epoch:2774 \t loss: 0.037057\n",
      "Epoch:2775 \t loss: 0.111040\n",
      "Epoch:2776 \t loss: 0.033375\n",
      "Epoch:2777 \t loss: 0.053842\n",
      "Epoch:2778 \t loss: 0.021284\n",
      "Epoch:2779 \t loss: 0.000010\n",
      "Epoch:2780 \t loss: 0.000004\n",
      "Epoch:2781 \t loss: 0.064037\n",
      "Epoch:2782 \t loss: 0.000010\n",
      "Epoch:2783 \t loss: 0.100737\n",
      "Epoch:2784 \t loss: 0.001445\n",
      "Epoch:2785 \t loss: 0.109034\n",
      "Epoch:2786 \t loss: 0.123287\n",
      "Epoch:2787 \t loss: 0.000003\n",
      "Epoch:2788 \t loss: 0.074843\n",
      "Epoch:2789 \t loss: 0.024916\n",
      "Epoch:2790 \t loss: 0.020980\n",
      "Epoch:2791 \t loss: 0.000002\n",
      "Epoch:2792 \t loss: 0.023533\n",
      "Epoch:2793 \t loss: 0.060320\n",
      "Epoch:2794 \t loss: 0.039910\n",
      "Epoch:2795 \t loss: 0.000004\n",
      "Epoch:2796 \t loss: 0.030241\n",
      "Epoch:2797 \t loss: 0.036677\n",
      "Epoch:2798 \t loss: 0.106283\n",
      "Epoch:2799 \t loss: 0.037145\n",
      "Epoch:2800 \t loss: 0.171244\n",
      "Epoch:2801 \t loss: 0.022732\n",
      "Epoch:2802 \t loss: 0.076367\n",
      "Epoch:2803 \t loss: 0.000002\n",
      "Epoch:2804 \t loss: 0.088414\n",
      "Epoch:2805 \t loss: 0.032304\n",
      "Epoch:2806 \t loss: 0.000002\n",
      "Epoch:2807 \t loss: 0.105626\n",
      "Epoch:2808 \t loss: 0.077093\n",
      "Epoch:2809 \t loss: 0.043334\n",
      "Epoch:2810 \t loss: 0.041828\n",
      "Epoch:2811 \t loss: 0.000000\n",
      "Epoch:2812 \t loss: 0.060046\n",
      "Epoch:2813 \t loss: 0.000002\n",
      "Epoch:2814 \t loss: 0.043800\n",
      "Epoch:2815 \t loss: 0.078926\n",
      "Epoch:2816 \t loss: 0.035964\n",
      "Epoch:2817 \t loss: 0.024161\n",
      "Epoch:2818 \t loss: 0.000000\n",
      "Epoch:2819 \t loss: 0.041596\n",
      "Epoch:2820 \t loss: 0.000013\n",
      "Epoch:2821 \t loss: 0.031977\n",
      "Epoch:2822 \t loss: 0.032402\n",
      "Epoch:2823 \t loss: 0.088769\n",
      "Epoch:2824 \t loss: 0.024629\n",
      "Epoch:2825 \t loss: 0.052441\n",
      "Epoch:2826 \t loss: 0.042105\n",
      "Epoch:2827 \t loss: 0.071127\n",
      "Epoch:2828 \t loss: 0.025752\n",
      "Epoch:2829 \t loss: 0.000000\n",
      "Epoch:2830 \t loss: 0.075684\n",
      "Epoch:2831 \t loss: 0.000001\n",
      "Epoch:2832 \t loss: 0.123129\n",
      "Epoch:2833 \t loss: 1.395991\n",
      "Epoch:2834 \t loss: 0.127085\n",
      "Epoch:2835 \t loss: 0.000003\n",
      "Epoch:2836 \t loss: 0.000004\n",
      "Epoch:2837 \t loss: 0.116834\n",
      "Epoch:2838 \t loss: 0.073597\n",
      "Epoch:2839 \t loss: 0.061905\n",
      "Epoch:2840 \t loss: 0.000058\n",
      "Epoch:2841 \t loss: 0.042074\n",
      "Epoch:2842 \t loss: 0.044125\n",
      "Epoch:2843 \t loss: 0.000003\n",
      "Epoch:2844 \t loss: 0.048088\n",
      "Epoch:2845 \t loss: 0.065656\n",
      "Epoch:2846 \t loss: 0.031066\n",
      "Epoch:2847 \t loss: 0.000007\n",
      "Epoch:2848 \t loss: 0.027319\n",
      "Epoch:2849 \t loss: 0.043540\n",
      "Epoch:2850 \t loss: 0.085192\n",
      "Epoch:2851 \t loss: 0.124579\n",
      "Epoch:2852 \t loss: 0.057406\n",
      "Epoch:2853 \t loss: 0.080660\n",
      "Epoch:2854 \t loss: 0.043304\n",
      "Epoch:2855 \t loss: 0.166614\n",
      "Epoch:2856 \t loss: 0.000003\n",
      "Epoch:2857 \t loss: 0.000002\n",
      "Epoch:2858 \t loss: 0.022061\n",
      "Epoch:2859 \t loss: 0.000000\n",
      "Epoch:2860 \t loss: 0.057883\n",
      "Epoch:2861 \t loss: 0.054835\n",
      "Epoch:2862 \t loss: 0.038003\n",
      "Epoch:2863 \t loss: 0.113954\n",
      "Epoch:2864 \t loss: 0.000009\n",
      "Epoch:2865 \t loss: 0.000011\n",
      "Epoch:2866 \t loss: 0.130559\n",
      "Epoch:2867 \t loss: 0.120542\n",
      "Epoch:2868 \t loss: 0.108160\n",
      "Epoch:2869 \t loss: 0.079744\n",
      "Epoch:2870 \t loss: 0.000002\n",
      "Epoch:2871 \t loss: 0.076111\n",
      "Epoch:2872 \t loss: 0.027390\n",
      "Epoch:2873 \t loss: 0.036851\n",
      "Epoch:2874 \t loss: 0.045235\n",
      "Epoch:2875 \t loss: 0.000018\n",
      "Epoch:2876 \t loss: 0.029640\n",
      "Epoch:2877 \t loss: 0.000004\n",
      "Epoch:2878 \t loss: 0.028826\n",
      "Epoch:2879 \t loss: 0.000002\n",
      "Epoch:2880 \t loss: 0.000002\n",
      "Epoch:2881 \t loss: 0.000001\n",
      "Epoch:2882 \t loss: 0.058614\n",
      "Epoch:2883 \t loss: 0.041249\n",
      "Epoch:2884 \t loss: 0.111807\n",
      "Epoch:2885 \t loss: 0.000011\n",
      "Epoch:2886 \t loss: 0.000003\n",
      "Epoch:2887 \t loss: 0.045537\n",
      "Epoch:2888 \t loss: 0.036521\n",
      "Epoch:2889 \t loss: 0.000001\n",
      "Epoch:2890 \t loss: 0.000004\n",
      "Epoch:2891 \t loss: 0.000002\n",
      "Epoch:2892 \t loss: 0.063261\n",
      "Epoch:2893 \t loss: 0.073679\n",
      "Epoch:2894 \t loss: 0.028308\n",
      "Epoch:2895 \t loss: 0.032174\n",
      "Epoch:2896 \t loss: 0.000005\n",
      "Epoch:2897 \t loss: 0.000002\n",
      "Epoch:2898 \t loss: 0.097201\n",
      "Epoch:2899 \t loss: 0.056240\n",
      "Epoch:2900 \t loss: 0.075204\n",
      "Epoch:2901 \t loss: 0.026181\n",
      "Epoch:2902 \t loss: 0.052374\n",
      "Epoch:2903 \t loss: 0.000004\n",
      "Epoch:2904 \t loss: 0.000000\n",
      "Epoch:2905 \t loss: 0.030068\n",
      "Epoch:2906 \t loss: 0.000000\n",
      "Epoch:2907 \t loss: 0.067423\n",
      "Epoch:2908 \t loss: 0.049399\n",
      "Epoch:2909 \t loss: 0.000016\n",
      "Epoch:2910 \t loss: 0.021515\n",
      "Epoch:2911 \t loss: 0.000001\n",
      "Epoch:2912 \t loss: 0.000001\n",
      "Epoch:2913 \t loss: 0.063400\n",
      "Epoch:2914 \t loss: 0.050123\n",
      "Epoch:2915 \t loss: 0.026233\n",
      "Epoch:2916 \t loss: 0.051400\n",
      "Epoch:2917 \t loss: 0.029916\n",
      "Epoch:2918 \t loss: 0.116127\n",
      "Epoch:2919 \t loss: 0.000001\n",
      "Epoch:2920 \t loss: 0.086949\n",
      "Epoch:2921 \t loss: 0.085199\n",
      "Epoch:2922 \t loss: 0.106657\n",
      "Epoch:2923 \t loss: 0.082601\n",
      "Epoch:2924 \t loss: 0.023111\n",
      "Epoch:2925 \t loss: 0.025486\n",
      "Epoch:2926 \t loss: 0.054433\n",
      "Epoch:2927 \t loss: 0.083537\n",
      "Epoch:2928 \t loss: 0.025642\n",
      "Epoch:2929 \t loss: 0.075831\n",
      "Epoch:2930 \t loss: 0.000057\n",
      "Epoch:2931 \t loss: 0.000041\n",
      "Epoch:2932 \t loss: 0.029097\n",
      "Epoch:2933 \t loss: 0.060566\n",
      "Epoch:2934 \t loss: 0.135049\n",
      "Epoch:2935 \t loss: 0.012173\n",
      "Epoch:2936 \t loss: 0.106497\n",
      "Epoch:2937 \t loss: 0.000022\n",
      "Epoch:2938 \t loss: 0.044787\n",
      "Epoch:2939 \t loss: 0.000011\n",
      "Epoch:2940 \t loss: 0.083973\n",
      "Epoch:2941 \t loss: 0.000003\n",
      "Epoch:2942 \t loss: 0.000004\n",
      "Epoch:2943 \t loss: 0.080978\n",
      "Epoch:2944 \t loss: 0.058329\n",
      "Epoch:2945 \t loss: 0.036189\n",
      "Epoch:2946 \t loss: 0.000152\n",
      "Epoch:2947 \t loss: 0.043583\n",
      "Epoch:2948 \t loss: 0.079038\n",
      "Epoch:2949 \t loss: 0.064062\n",
      "Epoch:2950 \t loss: 0.069795\n",
      "Epoch:2951 \t loss: 0.000029\n",
      "Epoch:2952 \t loss: 0.000004\n",
      "Epoch:2953 \t loss: 0.000035\n",
      "Epoch:2954 \t loss: 0.043077\n",
      "Epoch:2955 \t loss: 0.000004\n",
      "Epoch:2956 \t loss: 0.022051\n",
      "Epoch:2957 \t loss: 0.031225\n",
      "Epoch:2958 \t loss: 0.064414\n",
      "Epoch:2959 \t loss: 0.026530\n",
      "Epoch:2960 \t loss: 0.025978\n",
      "Epoch:2961 \t loss: 0.008748\n",
      "Epoch:2962 \t loss: 0.059623\n",
      "Epoch:2963 \t loss: 0.029524\n",
      "Epoch:2964 \t loss: 0.000000\n",
      "Epoch:2965 \t loss: 0.058088\n",
      "Epoch:2966 \t loss: 0.037970\n",
      "Epoch:2967 \t loss: 0.058877\n",
      "Epoch:2968 \t loss: 0.070964\n",
      "Epoch:2969 \t loss: 0.058429\n",
      "Epoch:2970 \t loss: 0.028465\n",
      "Epoch:2971 \t loss: 0.149451\n",
      "Epoch:2972 \t loss: 0.000005\n",
      "Epoch:2973 \t loss: 0.088420\n",
      "Epoch:2974 \t loss: 0.054977\n",
      "Epoch:2975 \t loss: 0.140783\n",
      "Epoch:2976 \t loss: 0.114757\n",
      "Epoch:2977 \t loss: 0.031442\n",
      "Epoch:2978 \t loss: 0.079776\n",
      "Epoch:2979 \t loss: 0.108853\n",
      "Epoch:2980 \t loss: 0.095379\n",
      "Epoch:2981 \t loss: 0.090009\n",
      "Epoch:2982 \t loss: 0.062406\n",
      "Epoch:2983 \t loss: 0.041415\n",
      "Epoch:2984 \t loss: 0.048461\n",
      "Epoch:2985 \t loss: 0.000003\n",
      "Epoch:2986 \t loss: 0.000013\n",
      "Epoch:2987 \t loss: 0.054571\n",
      "Epoch:2988 \t loss: 0.051611\n",
      "Epoch:2989 \t loss: 0.000003\n",
      "Epoch:2990 \t loss: 0.083906\n",
      "Epoch:2991 \t loss: 0.059982\n",
      "Epoch:2992 \t loss: 0.050641\n",
      "Epoch:2993 \t loss: 0.022017\n",
      "Epoch:2994 \t loss: 0.082316\n",
      "Epoch:2995 \t loss: 0.000040\n",
      "Epoch:2996 \t loss: 0.000008\n",
      "Epoch:2997 \t loss: 0.091734\n",
      "Epoch:2998 \t loss: 0.071163\n",
      "Epoch:2999 \t loss: 0.070558\n",
      "Epoch:3000 \t loss: 0.000090\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = [], []\n",
    "\n",
    "if DO_NSP_TEST :\n",
    "    train_samples, test_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "else :\n",
    "    train_samples = all_samples\n",
    "\n",
    "    # the maximum of length of extents\n",
    "object_max_len = 19 # longest extents is 8\n",
    "# the number of tokens objects\n",
    "object_max_vocab = 356    \n",
    "\n",
    "batch_data = make_data(extent_token_train, train_samples, object2idx, object_max_vocab )\n",
    "\n",
    "batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)]\n",
    "dataset = BERTDataset(*batch_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model = BERT(n_layers,object_max_vocab,object_max_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for one_batch in dataloader:\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch]\n",
    "\n",
    "        logits_cls, logits_lm, _ = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        loss_cls = criterion(logits_cls, is_next)\n",
    "        loss_lm = criterion(logits_lm.view(-1, object_max_vocab), masked_tokens.view(-1))\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n",
    "    \n",
    "    # 每30个epoch保存一次模型\n",
    "    if epoch % 30 == 0 :\n",
    "        torch.save(model.state_dict(), 'no_pos_object_pretrained.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f732c-cf95-47c4-bed2-3d143fd17037",
   "metadata": {},
   "source": [
    "# Saving the pre-trained model\n",
    "现在要训练一轮需要花费巨量的时间，所以先保存到文件以免刷新以后需要重新训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3072fc2-4142-48f4-a340-c75bca49b15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'no_pos_object_pretrained.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d56dd2-3618-450c-9225-04674cfc2eb7",
   "metadata": {},
   "source": [
    "# Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc99ff-c7e1-456e-a3f9-922e86485bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [sample[2] for sample in test_samples]\n",
    "# print(labels)\n",
    "\n",
    "num_true = labels.count(True)\n",
    "num_false = labels.count(False)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of True:\", num_true)\n",
    "print(\"Number of False:\", num_false)\n",
    "\n",
    "labels_mapping = {\"True\": 1, \"False\": 0}\n",
    "labels_01 = [labels_mapping[str(sample[2])] for sample in test_samples]\n",
    "# print(labels_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abb838-fa9d-4e0f-915a-2c0db846b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_NSP_TEST :\n",
    "    pretrained_model = BERT(n_layers)\n",
    "    pretrained_model.eval()\n",
    "    pretrained_model.load_state_dict(torch.load('pretrained.dat'))\n",
    "    pretrained_model.to(device)\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
    "    #     input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    #     segment_ids = torch.LongTensor(segment_ids).unsqueeze(0).to(device)\n",
    "    #     masked_pos = torch.LongTensor(masked_pos).unsqueeze(0).to(device)\n",
    "    #     masked_tokens = torch.LongTensor(masked_tokens).unsqueeze(0).to(device)\n",
    "    #     logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
    "    #     input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
    "\n",
    "    for sample in test_samples:\n",
    "        index_a = sample[0]\n",
    "        index_b = sample[1]\n",
    "        tokens_a = extent_token_list[index_a]\n",
    "        tokens_b = extent_token_list[index_b]\n",
    "\n",
    "        input_ids = torch.tensor([object2idx['[CLS]']] + tokens_a + [object2idx['[SEP]']] + tokens_b + [object2idx['[SEP]']])\n",
    "        segment_ids = torch.tensor([0 for i in range(\n",
    "                        1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))])\n",
    "        masked_pos = torch.tensor([0 for i in range(\n",
    "                        1 + len(tokens_a) + 1)] + [0 for i in range(1 + len(tokens_b))])\n",
    "        input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "        segment_ids = torch.LongTensor(segment_ids).unsqueeze(0).to(device)\n",
    "        masked_pos = torch.LongTensor(masked_pos).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "        logits_cls, _, _ = pretrained_model(input_ids, segment_ids, masked_pos)\n",
    "        cpu = torch.device('cpu')\n",
    "        pred_next = logits_cls.data.max(1)[1].data.to(cpu).numpy()[0]\n",
    "        predictions.append(pred_next) \n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels_01, predictions)\n",
    "    precision = precision_score(labels_01, predictions)\n",
    "    recall = recall_score(labels_01, predictions)\n",
    "    f1 = f1_score(labels_01, predictions)\n",
    "    roc_auc = roc_auc_score(labels_01, predictions)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "else :\n",
    "    print('NSP TEST is disabled since DO_NSP_TEST is set to False.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256c64-5c39-4e0d-8c3f-7e9e294f2bcc",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7ec4a-ee80-4db1-8e40-e57e1e780117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06b24e-1cd3-4a63-879c-eec2a1fc44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_true_permutes(extent_token_list, tup_len = 3) :\n",
    "    true_permutes = []\n",
    "    dist = [0 for i in range(tup_len + 1)]\n",
    "    \n",
    "    for extent in extent_token_list :\n",
    "        extent_len = len(extent)\n",
    "        \n",
    "        for now_len in range(2, tup_len + 1) :\n",
    "            if extent_len >= now_len :\n",
    "                now_pmt = [' '.join([str(ele) for ele in list(p)] + ['0' for _ in range(tup_len - now_len)]) for p in itertools.permutations(extent, now_len)]\n",
    "            else :\n",
    "                now_pmt = []\n",
    "\n",
    "            true_permutes.extend(now_pmt)\n",
    "            dist[now_len] += len(now_pmt)\n",
    "\n",
    "    true_permutes = set(true_permutes)\n",
    "    \n",
    "    return true_permutes, np.array(dist, dtype = np.float32)\n",
    "\n",
    "def pad_negative_samples(object2idx, true_permutes, length_distribution, number) :\n",
    "    lengths = np.arange(0, len(length_distribution))\n",
    "    tup_len = len(length_distribution) - 1 \n",
    "\n",
    "    print(lengths)\n",
    "    print(length_distribution)\n",
    "    \n",
    "    object_list = []\n",
    "    for obj in object2idx :\n",
    "        if not '[' in obj :\n",
    "            object_list.append(object2idx[obj])\n",
    "    \n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < number :\n",
    "        length = np.random.choice(lengths, p=length_distribution)\n",
    "\n",
    "        tmp_list = random.sample(object_list, length)\n",
    "        if length < tup_len :\n",
    "            tmp_list.extend([0 for _ in range(tup_len - length)])\n",
    "        \n",
    "        tmp_str = ' '.join([str(x) for x in tmp_list])\n",
    "        if tmp_str in true_permutes :\n",
    "            continue\n",
    "\n",
    "        negative_samples.append((tmp_list, False))\n",
    "    return negative_samples\n",
    "\n",
    "def prepare_object_list_data(object2idx, extent_token_list, extent_token_list_new, tup_len = 3) :\n",
    "    old_true_permutes, old_distribution = get_true_permutes(extent_token_list, tup_len)\n",
    "    new_true_permutes, new_distribution = get_true_permutes(extent_token_list_new, tup_len)\n",
    "    added_true_permutes = new_true_permutes - old_true_permutes\n",
    "    added_distribution = new_distribution - old_distribution\n",
    "    added_distribution /= np.sum(added_distribution)\n",
    "    \n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "    \n",
    "    for perm_str in old_true_permutes :\n",
    "        lst = [int(x) for x in perm_str.split(' ')]\n",
    "        train_samples.append((lst, True))\n",
    "    for perm_str in added_true_permutes :\n",
    "        lst = [int(x) for x in perm_str.split(' ')]\n",
    "        test_samples.append((lst, True))\n",
    "    \n",
    "    train_len = len(train_samples)\n",
    "    test_len = len(test_samples)\n",
    "    \n",
    "    negative_samples = pad_negative_samples(object2idx, new_true_permutes, list(added_distribution), train_len + test_len)\n",
    "    train_negative_samples, test_negative_samples = train_test_split(negative_samples, test_size=test_len / (train_len + test_len), random_state=42)\n",
    "\n",
    "    train_samples.extend(train_negative_samples)\n",
    "    test_samples.extend(test_negative_samples)\n",
    "    \n",
    "    random.shuffle(train_samples)\n",
    "    random.shuffle(test_samples)\n",
    "    \n",
    "    return train_samples, test_samples\n",
    "\n",
    "train_labeled_lists, test_labeled_lists = prepare_object_list_data(object2idx, extent_token_list, extent_token_list_new, tup_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444363be-08e7-43c5-9c40-b38c8e7a39b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the ratio of 1 and 0\n",
    "df = pd.DataFrame(test_labeled_lists, columns=['Pair', 'Label'])\n",
    "\n",
    "# Calculate the ratios\n",
    "ratio_zeros = (df['Label'] == 0).mean()\n",
    "ratio_ones = (df['Label'] == 1).mean()\n",
    "\n",
    "print(f\"in test set ratio of 0s: {ratio_zeros:.3f}\")\n",
    "print(f\"in test set ratio of 1s: {ratio_ones:.3f}\")\n",
    "\n",
    "df2 = pd.DataFrame(train_labeled_lists, columns=['Pair', 'Label'])\n",
    "\n",
    "# Calculate the ratios\n",
    "ratio_zero = (df2['Label'] == 0).mean()\n",
    "ratio_one = (df2['Label'] == 1).mean()\n",
    "\n",
    "print(f\"in train set ratio of 0s: {ratio_zero:.3f}\")\n",
    "print(f\"in train set ratio of 1s: {ratio_one:.3f}\")\n",
    "\n",
    "print('train set size ' + str(len(train_labeled_lists)))\n",
    "print('test set size ' + str(len(test_labeled_lists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44006f7b-b58f-43b5-ba7b-32dc2ce5e780",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef9943-2c2e-4e63-9d0e-4d98973caddb",
   "metadata": {},
   "source": [
    "##  MLP for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb7f61-7896-43af-bcab-e39ed0914367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design a MLP for classification task\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, object_pretrained_model, attribute_pretrained_model, embedding_size, hidden_size, output_size, dropout_rate = .1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.bert_object = object_pretrained_model\n",
    "        self.bert_attribute = attribute_pretrained_model\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs_object, segments_object, masked_poses_object,inputs_attribute, segments_attribute, masked_poses_attribute):\n",
    "        _, __, x1 = self.bert_object(inputs_object, segments_object, masked_poses_object)\n",
    "        _, __, x2 = self.bert_attribute(inputs_attribute, segments_attribute, masked_poses_attribute)\n",
    "        x = self.fc1(torch.cat((x1,x2), dim=0))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49621787-09c0-42bb-a2a5-49168298ac72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(pair_set):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for lst, label in pair_set:\n",
    "        inputs.append(lst)\n",
    "        labels.append(label)\n",
    "    return torch.tensor(inputs), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50d109-a6e5-41c6-a451-43391b326a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# input_size = 2 * 768\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 9\n",
    "batch_size = 36\n",
    "\n",
    "# 加载旧的模型\n",
    "# the maximum of length of extents\n",
    "object_max_len = 19 # longest extents is 8\n",
    "# the number of tokens objects\n",
    "object_max_vocab = 356    \n",
    "object_pretrained_model = BERT(n_layers, object_max_vocab, object_max_len)\n",
    "\n",
    "# the maximum of length of sequences\n",
    "attribute_max_len = 1500\n",
    "# the number of tokens (objects or attributes)\n",
    "attribute_max_vocab = 12973\n",
    "attribute_pretrained_model = BERT(n_layers, attribute_max_vocab, attribute_max_len)\n",
    "\n",
    "object_pretrained_model.load_state_dict(torch.load('object_pretrained.dat'))\n",
    "attribute_pretrained_model.load_state_dict(torch.load('attribute_pretrained.dat'))\n",
    "\n",
    "object_pretrained_model.eval()\n",
    "attribute_pretrained_model.eval()\n",
    "object_pretrained_model.to(device)\n",
    "attribute_pretrained_model.to(device)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MLP_model = MLP(object_pretrained_model,attribute_pretrained_model, d_model, hidden_size, output_size, dropout_rate=0.1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to device\n",
    "MLP_model = MLP_model.to(device)\n",
    "\n",
    "# Prepare the data\n",
    "train_inputs, train_labels = prepare_data(train_labeled_lists)\n",
    "test_inputs, test_labels = prepare_data(test_labeled_lists)\n",
    "\n",
    "train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fa74-69ff-4214-b7da-94c646012c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n",
    "\n",
    "    for inputs, labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        segments = torch.tensor([[0 for _ in i] for i in inputs])\n",
    "        masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in inputs])\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        segments, masked_poses = segments.to(device), masked_poses.to(device)\n",
    "        \n",
    "        outputs = MLP_model(inputs_object, segments_object, masked_poses_object,inputs_attribute, segments_attribute, masked_poses_attribute)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm with the current loss\n",
    "        pbar.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3956915-e016-4c94-8b83-552bf63ad6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segments = torch.tensor([[0 for _ in i] for i in train_inputs])\n",
    "masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in train_inputs])\n",
    "\n",
    "segments = segments.to(device)\n",
    "masked_poses = masked_poses.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = MLP_model(train_inputs, segments, masked_poses)\n",
    "    predictions = (train_outputs > 0.5).float().cpu().numpy()\n",
    "    train_labels_numpy = train_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(train_labels_numpy, predictions_binary)\n",
    "precision = precision_score(train_labels_numpy, predictions_binary)\n",
    "recall = recall_score(train_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(train_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(train_labels_numpy, train_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576624d-bdf2-4f5a-bf23-d3a1e5d82564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "segments = torch.tensor([[0 for _ in i] for i in test_inputs])\n",
    "masked_poses = torch.tensor([[0 for _ in range(max_pred)] for i in test_inputs])\n",
    "\n",
    "segments = segments.to(device)\n",
    "masked_poses = masked_poses.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = MLP_model(test_inputs, segments, masked_poses)\n",
    "    predictions = (test_outputs > 0.5).float().cpu().numpy()\n",
    "    test_labels_numpy = test_labels.cpu().numpy()\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(test_labels_numpy, predictions_binary)\n",
    "precision = precision_score(test_labels_numpy, predictions_binary)\n",
    "recall = recall_score(test_labels_numpy, predictions_binary)\n",
    "f1 = f1_score(test_labels_numpy, predictions_binary)\n",
    "auc = roc_auc_score(test_labels_numpy, test_outputs.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b73de-c828-4e9b-b5e3-2811c70ac034",
   "metadata": {},
   "source": [
    "## Logistic Regression for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9c91b-68d8-44f8-9284-aa2ee036eefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare the data\n",
    "train_inputs, train_labels = prepare_data(train_labeled_pairs, embeddings)\n",
    "test_inputs, test_labels = prepare_data(test_labeled_pairs, embeddings)\n",
    "\n",
    "# train_inputs and train_labels are PyTorch tensors\n",
    "train_inputs = train_inputs.to(\"cpu\")\n",
    "train_labels = train_labels.to(\"cpu\")\n",
    "\n",
    "test_inputs = test_inputs.to(\"cpu\")\n",
    "test_labels = test_labels.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4354271-b0d7-4796-ba67-49cc8629d70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ae538-6e90-4323-9d3d-7352699005d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "pred_labels = logistic_regression_model.predict(test_inputs)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, pred_labels)\n",
    "# precision = precision_score(test_labels, pred_labels)\n",
    "# recall = recall_score(test_labels, pred_labels)\n",
    "# f1 = f1_score(test_labels, pred_labels)\n",
    "# auc = roc_auc_score(test_labels, pred_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Print classification report for more detailed evaluation\n",
    "print(\"Classification Report:\")\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "# print(f'AUC: {auc:.4f}')\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a64b9-c4a6-4fa9-a9f3-09744a07232d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
